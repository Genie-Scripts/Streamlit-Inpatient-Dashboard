# data_processing_tab.py (ä¿®æ­£æ¡ˆ)

import warnings
warnings.filterwarnings('ignore', category=FutureWarning) # FutureWarning ã‚’ç„¡è¦–
import streamlit as st
import pandas as pd
import numpy as np # NaNãƒã‚§ãƒƒã‚¯ãªã©ã§ä½¿ç”¨ã•ã‚Œã‚‹å¯èƒ½æ€§
import time
import os
import tempfile
import gc
import psutil # ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡å–å¾—ã®ãŸã‚
import logging # logging ã‚’è¿½åŠ 

# integrated_preprocessing ã¨ loader ã‹ã‚‰å¿…è¦ãªé–¢æ•°ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from integrated_preprocessing import (
    integrated_preprocess_data, calculate_file_hash, efficient_duplicate_check
)
from loader import load_files # loader.py ã® load_files ã‚’ä½¿ç”¨ (read_excel_cached ã¯ load_fileså†…éƒ¨ã§ä½¿ç”¨)
from forecast import generate_filtered_summaries
from utils import initialize_all_mappings, create_dept_mapping_table

logger = logging.getLogger(__name__) # logger ã‚’è¨­å®š

# --- å®šæ•° ---
# EXCEL_USE_COLUMNS ã¯ã€loader.py ã® read_excel_cached ã«æ¸¡ã™éš›ã®ã€ŒæœŸå¾…ã™ã‚‹æ¨™æº–åˆ—åã€
EXCEL_USE_COLUMNS = [
    "ç—…æ£Ÿã‚³ãƒ¼ãƒ‰", "è¨ºç™‚ç§‘å", "æ—¥ä»˜", "åœ¨é™¢æ‚£è€…æ•°",
    "å…¥é™¢æ‚£è€…æ•°", "ç·Šæ€¥å…¥é™¢æ‚£è€…æ•°", "é€€é™¢æ‚£è€…æ•°", "æ­»äº¡æ‚£è€…æ•°"
    # "å»¶ã¹åœ¨é™¢æ—¥æ•°ï¼ˆäººæ—¥ï¼‰" # ã“ã‚Œã¯ integrated_preprocessing ã§è¨ˆç®—ã•ã‚Œã‚‹ã®ã§ã€èª­ã¿è¾¼ã¿å¯¾è±¡ã‹ã‚‰ã¯å¤–ã™
]
EXCEL_DTYPES = { # dtypeæŒ‡å®šã‚‚æ¨™æº–åˆ—åã§è¡Œã†
    "ç—…æ£Ÿã‚³ãƒ¼ãƒ‰": str,
    "è¨ºç™‚ç§‘å": str,
    # "æ—¥ä»˜": # æ—¥ä»˜å‹ã¯ read_excel å¾Œã« pd.to_datetime ã§å‡¦ç†ã™ã‚‹æ–¹ãŒå …ç‰¢
    "åœ¨é™¢æ‚£è€…æ•°": float, # Excelå†…ãŒæ•°å€¤ã§ã‚ã‚‹ã“ã¨ã‚’æœŸå¾…
    "å…¥é™¢æ‚£è€…æ•°": float,
    "ç·Šæ€¥å…¥é™¢æ‚£è€…æ•°": float,
    "é€€é™¢æ‚£è€…æ•°": float,
    "æ­»äº¡æ‚£è€…æ•°": float
}

# --- ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›£è¦–é–¢æ•° (å¤‰æ›´ãªã—) ---
def log_memory_usage():
    # ... (æ—¢å­˜ã®ã‚³ãƒ¼ãƒ‰)
    try:
        process = psutil.Process()
        mem_info = process.memory_info()
        mem_usage_mb = mem_info.rss / (1024 * 1024)
        mem_percent = process.memory_percent()
        system_mem = psutil.virtual_memory()
        system_mem_percent = system_mem.percent
        available_mb = system_mem.available / (1024 * 1024)
        return {
            'process_mb': mem_usage_mb, 'process_percent': mem_percent,
            'system_percent': system_mem_percent, 'available_mb': available_mb
        }
    except Exception as e:
        logger.error(f"ãƒ¡ãƒ¢ãƒªæƒ…å ±å–å¾—ã‚¨ãƒ©ãƒ¼: {e}", exc_info=True)
        return None

def perform_cleanup(deep=False):
    # ... (æ—¢å­˜ã®ã‚³ãƒ¼ãƒ‰)
    if deep and 'df' in st.session_state and st.session_state.df is not None:
        if 'filtered_results' in st.session_state and st.session_state.get('filtered_results') != st.session_state.get('all_results'): # å­˜åœ¨ç¢ºèª
            st.session_state.filtered_results = None
        if 'forecast_model_results' in st.session_state:
            st.session_state.forecast_model_results = None
    try:
        temp_dir_root = tempfile.gettempdir()
        app_temp_files_pattern = os.path.join(temp_dir_root, "integrated_dashboard_temp_*")
        import glob
        for temp_file_path in glob.glob(app_temp_files_pattern):
            try:
                if os.path.isfile(temp_file_path): os.unlink(temp_file_path)
                elif os.path.isdir(temp_file_path): import shutil; shutil.rmtree(temp_file_path, ignore_errors=True)
            except Exception as e_file_del: logger.warning(f"ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã®å‰Šé™¤ä¸­ã«ã‚¨ãƒ©ãƒ¼: {e_file_del}")
    except Exception as e_temp_clean: logger.warning(f"ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼: {e_temp_clean}")
    gc.collect()
    time.sleep(0.1)
    gc.collect()


# --- ãƒ‡ãƒ¼ã‚¿å‡¦ç†ã®é–¢æ•° (å¤‰æ›´ãªã—) ---
def get_app_data_dir():
    # ... (æ—¢å­˜ã®ã‚³ãƒ¼ãƒ‰)
    base_temp_dir = tempfile.gettempdir()
    app_data_dir = os.path.join(base_temp_dir, "integrated_dashboard_data")
    if not os.path.exists(app_data_dir):
        try: os.makedirs(app_data_dir, exist_ok=True)
        except OSError as e: st.error(f"ãƒ‡ãƒ¼ã‚¿ä¿å­˜ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ä½œæˆã«å¤±æ•—: {app_data_dir}\n{e}"); return None
    return app_data_dir

def get_base_file_info(app_data_dir):
    if app_data_dir is None: return None
    info_path = os.path.join(app_data_dir, "base_file_info.json")
    if os.path.exists(info_path):
        try: # ã“ã®tryãƒ–ãƒ­ãƒƒã‚¯ã«å¯¾å¿œã™ã‚‹exceptã‚’è¿½åŠ 
            import json; # jsonãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã¯ã“ã®ã‚¹ã‚³ãƒ¼ãƒ—ã§ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
            with open(info_path, 'r', encoding='utf-8') as f:
                return json.load(f)
        except Exception as e: # â˜…â˜…â˜… exceptãƒ–ãƒ­ãƒƒã‚¯ã‚’è¿½åŠ  â˜…â˜…â˜…
            logger.error(f"ãƒ™ãƒ¼ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«æƒ…å ±ã®èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}", exc_info=True)
            return None
    return None

def save_base_file_info(app_data_dir, file_name, file_size, file_hash):
    # ... (æ—¢å­˜ã®ã‚³ãƒ¼ãƒ‰)
    if app_data_dir is None: return
    info_path = os.path.join(app_data_dir, "base_file_info.json")
    info = {"file_name": file_name, "file_size": file_size, "file_hash": file_hash}
    try: import json;
        with open(info_path, 'w', encoding='utf-8') as f: json.dump(info, f, ensure_ascii=False, indent=2) # encodingæŒ‡å®š
    except Exception as e: logger.error(f"ãƒ™ãƒ¼ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«æƒ…å ±ã®ä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}", exc_info=True)


def debug_target_file_processing(target_data, search_keywords=['å…¨ä½“', 'ç—…é™¢å…¨ä½“', 'ç—…é™¢']):
    # ... (æ—¢å­˜ã®ã‚³ãƒ¼ãƒ‰ã€å¤‰æ›´ãªã—)
    debug_info = {'file_loaded': target_data is not None, 'columns': [], 'shape': (0,0), 'search_results': {}, 'sample_data': None}
    if target_data is not None:
        debug_info['columns'] = list(target_data.columns); debug_info['shape'] = target_data.shape
        debug_info['sample_data'] = target_data.head(3).to_dict('records') if len(target_data) > 0 else []
        for keyword in search_keywords:
            results = []
            for col in target_data.columns:
                if target_data[col].dtype == 'object':
                    try: # æ–‡å­—åˆ—ä»¥å¤–ãŒå«ã¾ã‚Œã‚‹å¯èƒ½æ€§ã‚’è€ƒæ…®
                        matches = target_data[target_data[col].astype(str).str.contains(keyword, na=False, case=False)]
                        if len(matches) > 0:
                            results.append({'column': col, 'matches': len(matches), 'sample_values': matches[col].unique()[:3].tolist()})
                    except Exception as e_search: logger.debug(f"ç›®æ¨™å€¤ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‡ãƒãƒƒã‚°æ¤œç´¢ä¸­ã‚¨ãƒ©ãƒ¼ ({col}, {keyword}): {e_search}")
            debug_info['search_results'][keyword] = results
    return debug_info


def extract_targets_from_file(target_data):
    # ... (æ—¢å­˜ã®ã‚³ãƒ¼ãƒ‰ã€ãƒ­ã‚°å‡ºåŠ›ã‚’loggerã«å¤‰æ›´)
    if target_data is None or target_data.empty: return None, None
    debug_info = debug_target_file_processing(target_data)
    search_patterns = [('éƒ¨é–€ã‚³ãƒ¼ãƒ‰', ['å…¨ä½“', 'ç—…é™¢', 'ç·åˆ']), ('éƒ¨é–€å', ['ç—…é™¢å…¨ä½“', 'å…¨ä½“', 'ç—…é™¢', 'ç·åˆ']),
                       ('è¨ºç™‚ç§‘å', ['ç—…é™¢å…¨ä½“', 'å…¨ä½“', 'ç—…é™¢', 'ç·åˆ']), ('ç§‘å', ['ç—…é™¢å…¨ä½“', 'å…¨ä½“', 'ç—…é™¢', 'ç·åˆ'])]
    target_row = None; used_pattern = None
    for col_name, keywords in search_patterns:
        if col_name in target_data.columns:
            for keyword in keywords:
                try: # ã‚¢ã‚µãƒ¼ã‚·ãƒ§ãƒ³ã‚¨ãƒ©ãƒ¼å¯¾ç­–
                    mask = target_data[col_name].astype(str).str.contains(keyword, na=False, case=False)
                    matches = target_data[mask]
                    if len(matches) > 0: target_row = matches.iloc[0]; used_pattern = f"{col_name}='{keyword}'"; logger.info(f"ç›®æ¨™å€¤ãƒ‡ãƒ¼ã‚¿æ¤œç´¢æˆåŠŸ: {used_pattern}"); break
                except Exception as e_pat: logger.debug(f"ç›®æ¨™å€¤æ¤œç´¢ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚¨ãƒ©ãƒ¼ ({col_name}, {keyword}): {e_pat}")
            if target_row is not None: break
    if target_row is None:
        logger.warning("ç›®æ¨™å€¤ãƒ‡ãƒ¼ã‚¿ã§ã€Œå…¨ä½“ã€ã«ç›¸å½“ã™ã‚‹è¡ŒãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
        logger.warning(f"åˆ©ç”¨å¯èƒ½ãªåˆ—: {list(target_data.columns)}")
        logger.warning(f"ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿:\n{target_data.head()}")
        return None, debug_info
    target_days = None; target_admissions = None
    days_columns = ['å»¶ã¹åœ¨é™¢æ—¥æ•°ç›®æ¨™', 'åœ¨é™¢æ—¥æ•°ç›®æ¨™', 'ç›®æ¨™åœ¨é™¢æ—¥æ•°', 'å»¶ã¹åœ¨é™¢æ—¥æ•°', 'åœ¨é™¢æ—¥æ•°']; admission_columns = ['æ–°å…¥é™¢æ‚£è€…æ•°ç›®æ¨™', 'å…¥é™¢æ‚£è€…æ•°ç›®æ¨™', 'ç›®æ¨™å…¥é™¢æ‚£è€…æ•°', 'æ–°å…¥é™¢æ‚£è€…æ•°', 'å…¥é™¢æ‚£è€…æ•°']
    for col in days_columns:
        if col in target_data.columns:
            try: value = target_row[col];
                if pd.notna(value) and str(value).strip() != '': target_days = float(str(value).replace(',', '').replace('äººæ—¥', '').strip()); logger.info(f"å»¶ã¹åœ¨é™¢æ—¥æ•°ç›®æ¨™ã‚’å–å¾—: {target_days} (åˆ—: {col})"); break
            except (ValueError, TypeError) as e: logger.warning(f"å»¶ã¹åœ¨é™¢æ—¥æ•°ç›®æ¨™ã®å¤‰æ›ã‚¨ãƒ©ãƒ¼ (åˆ—: {col}): {e}")
    for col in admission_columns:
        if col in target_data.columns:
            try: value = target_row[col];
                if pd.notna(value) and str(value).strip() != '': target_admissions = float(str(value).replace(',', '').replace('äºº', '').strip()); logger.info(f"æ–°å…¥é™¢æ‚£è€…æ•°ç›®æ¨™ã‚’å–å¾—: {target_admissions} (åˆ—: {col})"); break
            except (ValueError, TypeError) as e: logger.warning(f"æ–°å…¥é™¢æ‚£è€…æ•°ç›®æ¨™ã®å¤‰æ›ã‚¨ãƒ©ãƒ¼ (åˆ—: {col}): {e}")
    if (target_days is None or target_admissions is None) and 'ç›®æ¨™å€¤' in target_data.columns:
        try: general_target = float(str(target_row['ç›®æ¨™å€¤']).replace(',', '').strip());
            if target_days is None: target_days = general_target; logger.info(f"ä¸€èˆ¬ç›®æ¨™å€¤ã‹ã‚‰å»¶ã¹åœ¨é™¢æ—¥æ•°ç›®æ¨™ã‚’è¨­å®š: {target_days}")
            elif target_admissions is None: target_admissions = general_target; logger.info(f"ä¸€èˆ¬ç›®æ¨™å€¤ã‹ã‚‰æ–°å…¥é™¢æ‚£è€…æ•°ç›®æ¨™ã‚’è¨­å®š: {target_admissions}")
        except (ValueError, TypeError) as e: logger.warning(f"ä¸€èˆ¬ç›®æ¨™å€¤ã®å¤‰æ›ã‚¨ãƒ©ãƒ¼: {e}")
    extracted_targets = {'target_days': target_days, 'target_admissions': target_admissions, 'used_pattern': used_pattern, 'source_row': target_row.to_dict() if target_row is not None else None}
    return extracted_targets, debug_info


def process_data_with_progress(base_file_uploaded, new_files_uploaded, target_file_uploaded, progress_bar):
    """ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ã—ã€å‰å‡¦ç†ã‚’å®Ÿè¡Œã™ã‚‹ï¼ˆé€²æ—è¡¨ç¤ºä»˜ããƒ»ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°å¼·åŒ–ç‰ˆï¼‰"""
    try:
        start_time_total = time.time()
        st.session_state.performance_metrics = st.session_state.get('performance_metrics', {})
        st.session_state.performance_metrics['data_conversion_time'] = 0 # åˆæœŸåŒ–

        # --- 1. ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ ---
        progress_bar.progress(5, text="1. ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿æº–å‚™ä¸­...")
        load_start_time = time.time()

        # loader.load_files ã‚’å‘¼ã³å‡ºã— (usecols ã¨ dtype ã‚’æ¸¡ã™)
        # df_raw ã¨ processed_files_info ã‚’å—ã‘å–ã‚‹
        df_raw, processed_files_info = load_files(
            base_file_uploader, # base_file_uploaded ã‹ã‚‰å¤‰æ›´
            new_files_uploader, # new_files_uploaded ã‹ã‚‰å¤‰æ›´
            usecols_excel=EXCEL_USE_COLUMNS,
            dtype_excel=EXCEL_DTYPES
        )
        load_end_time = time.time()
        st.session_state.performance_metrics['data_load_time'] = load_end_time - load_start_time

        # èª­ã¿è¾¼ã¿çµæœã®ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯
        successful_reads = 0
        failed_files = []
        if processed_files_info: # å‡¦ç†æƒ…å ±ãŒã‚ã‚‹å ´åˆ
            for info in processed_files_info:
                if info['status'] == 'success':
                    successful_reads += 1
                else:
                    failed_files.append(f"{info['name']} ({info['message']})")
            if successful_reads > 0:
                st.success(f"{successful_reads} ä»¶ã®ãƒ•ã‚¡ã‚¤ãƒ«ãŒæ­£å¸¸ã«èª­ã¿è¾¼ã¾ã‚Œã¾ã—ãŸã€‚")
            if failed_files:
                st.warning(f"{len(failed_files)} ä»¶ã®ãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã¾ãŸã¯ã‚¹ã‚­ãƒƒãƒ—ã•ã‚Œã¾ã—ãŸ:")
                for f_info in failed_files:
                    st.caption(f"- {f_info}")
        elif df_raw.empty: # å‡¦ç†æƒ…å ±ã‚‚ãªãã€df_rawã‚‚ç©ºã®å ´åˆ
            st.error("èª­ã¿è¾¼ã‚€ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚å›ºå®šãƒ•ã‚¡ã‚¤ãƒ«ã¾ãŸã¯è¿½åŠ ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ã€‚")
            progress_bar.progress(100, text="ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿å¤±æ•—ã€‚")
            return False, None, None, None, None # æ—©æœŸãƒªã‚¿ãƒ¼ãƒ³

        if df_raw.empty: # çµåˆå¾Œã‚‚ç©ºãªã‚‰ã‚¨ãƒ©ãƒ¼
            st.error("èª­ã¿è¾¼ã¾ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ãŒç©ºã§ã™ã€‚ãƒ•ã‚¡ã‚¤ãƒ«å†…å®¹ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
            progress_bar.progress(100, text="ãƒ‡ãƒ¼ã‚¿å†…å®¹ãŒç©ºã§ã™ã€‚")
            return False, None, None, None, None # æ—©æœŸãƒªã‚¿ãƒ¼ãƒ³

        progress_bar.progress(20, text="1. ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿å®Œäº†ã€‚ãƒ‡ãƒ¼ã‚¿çµåˆä¸­...")

        # èª­ã¿è¾¼ã¿æ™‚ã«è¿½åŠ ã•ã‚ŒãŸã‚½ãƒ¼ã‚¹æƒ…å ±åˆ—ã‚’å‰Šé™¤ã—ã¦ã‹ã‚‰é‡è¤‡ãƒã‚§ãƒƒã‚¯
        source_info_cols = ['_source_file_', '_source_type_']
        cols_to_drop_before_dup_check = [col for col in source_info_cols if col in df_raw.columns]
        if cols_to_drop_before_dup_check:
            df_raw_for_dup_check = df_raw.drop(columns=cols_to_drop_before_dup_check)
        else:
            df_raw_for_dup_check = df_raw

        progress_bar.progress(22, text="2. é‡è¤‡ãƒã‚§ãƒƒã‚¯ä¸­...")
        df_processed_duplicates = efficient_duplicate_check(df_raw_for_dup_check)
        del df_raw_for_dup_check, df_raw # ãƒ¡ãƒ¢ãƒªè§£æ”¾
        gc.collect()

        # --- 2. ç›®æ¨™å€¤ãƒ•ã‚¡ã‚¤ãƒ«ã®å‡¦ç† ---
        target_data = None
        target_file_debug_info = None
        extracted_targets = None
        if target_file_uploader:
            progress_bar.progress(25, text="ç›®æ¨™å€¤ãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿ä¸­...")
            try:
                target_file_uploader.seek(0) # ãƒã‚¤ãƒ³ã‚¿ã‚’ãƒªã‚»ãƒƒãƒˆ
                # CSVã®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ãƒˆãƒ©ã‚¤
                encodings_to_try = ['utf-8', 'shift_jis', 'cp932', 'utf-8-sig']
                target_df_temp = None
                for enc in encodings_to_try:
                    try:
                        target_df_temp = pd.read_csv(target_file_uploader, encoding=enc)
                        logger.info(f"ç›®æ¨™å€¤ãƒ•ã‚¡ã‚¤ãƒ«ã‚’{enc}ã§èª­ã¿è¾¼ã¿æˆåŠŸ")
                        target_file_uploader.seek(0) # æ¬¡ã®è©¦è¡Œã®ãŸã‚ã«ãƒªã‚»ãƒƒãƒˆ
                        break
                    except UnicodeDecodeError:
                        logger.debug(f"ç›®æ¨™å€¤ãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰è©¦è¡Œå¤±æ•—: {enc}")
                        target_file_uploader.seek(0)
                        continue
                if target_df_temp is None or target_df_temp.empty:
                    st.warning("ç›®æ¨™å€¤ãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸï¼ˆé©åˆ‡ãªã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ãŒè¦‹ã¤ã‹ã‚‰ãªã„ã‹ã€ãƒ•ã‚¡ã‚¤ãƒ«ãŒç©ºã§ã™ï¼‰ã€‚")
                else:
                    target_data = target_df_temp
                    extracted_targets, target_file_debug_info = extract_targets_from_file(target_data)
                    st.session_state.target_file_debug_info = target_file_debug_info
                    st.session_state.extracted_targets = extracted_targets
                    # create_dept_mapping_table ã¯ initialize_all_mappings ã§å‘¼ã³å‡ºã•ã‚Œã‚‹
                    st.success("ç›®æ¨™å€¤ãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿ã¨è§£æãŒå®Œäº†ã—ã¾ã—ãŸã€‚")
            except Exception as e_target:
                st.warning(f"ç›®æ¨™å€¤ãƒ•ã‚¡ã‚¤ãƒ«ã®å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e_target)}")
                logger.error(f"ç›®æ¨™å€¤ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e_target}", exc_info=True)
                target_data = None # ã‚¨ãƒ©ãƒ¼æ™‚ã¯Noneã«
        else:
            logger.info("ç›®æ¨™å€¤ãƒ•ã‚¡ã‚¤ãƒ«ã¯ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
        progress_bar.progress(28, text="ç›®æ¨™å€¤ãƒ•ã‚¡ã‚¤ãƒ«ã®å‡¦ç†å®Œäº†ã€‚")


        # --- 3. ãƒ‡ãƒ¼ã‚¿ã®å‰å‡¦ç† ---
        progress_bar.progress(30, text="3. ãƒ‡ãƒ¼ã‚¿ã®å‰å‡¦ç†ä¸­...")
        preprocess_start_time = time.time()
        df_final, validation_results = integrated_preprocess_data(df_processed_duplicates, target_data_df=target_data)
        preprocess_end_time = time.time()
        st.session_state.performance_metrics['processing_time'] = preprocess_end_time - preprocess_start_time
        del df_processed_duplicates # ãƒ¡ãƒ¢ãƒªè§£æ”¾
        gc.collect()

        if df_final is None or df_final.empty:
            progress_bar.progress(100, text="ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†ã«å¤±æ•—ã—ã¾ã—ãŸã€‚")
            st.error("ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†ã®çµæœã€æœ‰åŠ¹ãªãƒ‡ãƒ¼ã‚¿ãŒæ®‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
            if validation_results and validation_results.get('errors'):
                for err_msg in validation_results.get('errors', []): st.error(err_msg)
            return False, None, None, None, validation_results

        # --- 4. æ¤œè¨¼çµæœã®è¡¨ç¤ºã¨æœ€çµ‚é›†è¨ˆ ---
        progress_bar.progress(50, text="4. ãƒ‡ãƒ¼ã‚¿ã®æ¤œè¨¼ä¸­...")
        st.session_state.validation_results = validation_results
        if validation_results:
            if validation_results.get("warnings"):
                with st.expander("ãƒ‡ãƒ¼ã‚¿æ¤œè¨¼ã®è­¦å‘Š", expanded=True):
                    for warn_msg in validation_results["warnings"]: st.warning(warn_msg)
            if validation_results.get("errors"): # ã‚¨ãƒ©ãƒ¼ãŒã‚ã‚Œã°å‡¦ç†ä¸­æ–­ã‚‚æ¤œè¨
                st.error("ãƒ‡ãƒ¼ã‚¿æ¤œè¨¼ã§ä»¥ä¸‹ã®ã‚¨ãƒ©ãƒ¼ãŒæ¤œå‡ºã•ã‚Œã¾ã—ãŸã€‚å‡¦ç†ã‚’ç¶™ç¶šã§ãã¾ã›ã‚“ã€‚")
                for err_msg in validation_results["errors"]: st.error(err_msg)
                return False, None, None, None, validation_results
        
        progress_bar.progress(85, text="5. å…¨ä½“ãƒ‡ãƒ¼ã‚¿ã®é›†è¨ˆä¸­...")
        all_results = None
        try:
            all_results = generate_filtered_summaries(df_final, None, None)
        except Exception as e_summary:
            st.warning(f"å…¨ä½“ãƒ‡ãƒ¼ã‚¿ã®é›†è¨ˆä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e_summary}")
            logger.error(f"å…¨ä½“ãƒ‡ãƒ¼ã‚¿é›†è¨ˆã‚¨ãƒ©ãƒ¼: {e_summary}", exc_info=True)
            # all_results ã¯ None ã®ã¾ã¾
        
        if all_results is None or not all_results.get("summary", pd.DataFrame()).empty is False: # summaryãŒç©ºã§ãªã„ã‹ã‚‚ãƒã‚§ãƒƒã‚¯
            # all_resultsãŒNoneã‹ã€summaryãŒç©ºã®å ´åˆã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
            default_latest_date = df_final["æ—¥ä»˜"].max() if not df_final.empty and "æ—¥ä»˜" in df_final.columns else pd.Timestamp.now().normalize()
            all_results = {
                "latest_date": default_latest_date,
                "summary": pd.DataFrame(), "weekday": pd.DataFrame(), "holiday": pd.DataFrame(),
                "monthly_all": pd.DataFrame(), "monthly_weekday": pd.DataFrame(), "monthly_holiday": pd.DataFrame(),
            }
            if df_final is not None and not df_final.empty : # df_final ãŒã‚ã‚Œã°è­¦å‘Šå‡ºã™
                 st.warning("å…¨ä½“çµæœã®é›†è¨ˆã«ä¸€éƒ¨å¤±æ•—ã—ãŸãŸã‚ã€é™å®šçš„ãªçµæœã«ãªã‚Šã¾ã™ã€‚")


        latest_data_date_obj = all_results.get("latest_date", pd.Timestamp.now().normalize()) # latest_date -> latest_data_date_obj

        # --- 5. ãƒãƒƒãƒ”ãƒ³ã‚°åˆæœŸåŒ–ã¨æœ€çµ‚å‡¦ç† ---
        progress_bar.progress(95, text="6. ãƒãƒƒãƒ”ãƒ³ã‚°æƒ…å ±ã®åˆæœŸåŒ–ä¸­...")
        if df_final is not None and not df_final.empty:
            initialize_all_mappings(df_final, target_data)
            logger.info("è¨ºç™‚ç§‘ãŠã‚ˆã³ç—…æ£Ÿã®ãƒãƒƒãƒ”ãƒ³ã‚°æƒ…å ±ã‚’åˆæœŸåŒ–ãƒ»æ›´æ–°ã—ã¾ã—ãŸã€‚")
        
        total_time_taken = time.time() - start_time_total
        logger.info(f"ãƒ‡ãƒ¼ã‚¿å‡¦ç†å…¨ä½“å®Œäº†ã€‚å‡¦ç†æ™‚é–“: {total_time_taken:.1f}ç§’, ãƒ¬ã‚³ãƒ¼ãƒ‰æ•°: {len(df_final) if df_final is not None else 0}")
        
        # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ­ã‚°ã®è¨˜éŒ² (æ—¢å­˜ã®ã‚‚ã®ã‚’æ´»ç”¨)
        if 'performance_logs' not in st.session_state: st.session_state.performance_logs = []
        st.session_state.performance_logs.append({
            'timestamp': time.strftime("%Y-%m-%d %H:%M:%S"), 'operation': 'ãƒ‡ãƒ¼ã‚¿å‡¦ç†å…¨ä½“', 'duration': total_time_taken,
            'details': {
                'rows': len(df_final) if df_final is not None else 0,
                'columns': len(df_final.columns) if df_final is not None and hasattr(df_final, 'columns') else 0,
                'files_new': len(new_files_uploader) if new_files_uploader else 0, # new_files_uploadedã‹ã‚‰å¤‰æ›´
                # 'base_file_processed_as_parquet': base_data_loaded_from_parquet # ã“ã®å¤‰æ•°ã¯ã‚¹ã‚³ãƒ¼ãƒ—å¤–
            }
        })

        progress_bar.progress(100, text=f"ãƒ‡ãƒ¼ã‚¿ã®å‡¦ç†ãŒå®Œäº†ã—ã¾ã—ãŸã€‚å‡¦ç†æ™‚é–“: {total_time_taken:.1f}ç§’")
        return True, df_final, target_data, all_results, latest_data_date_obj # validation_results ã®ä»£ã‚ã‚Šã« latest_data_date_obj

    except Exception as e_main:
        logger.error(f"ãƒ‡ãƒ¼ã‚¿å‡¦ç†ã®ãƒ¡ã‚¤ãƒ³ãƒ—ãƒ­ã‚»ã‚¹ã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e_main}", exc_info=True)
        progress_bar.progress(100, text=f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e_main)}")
        st.error(f"ãƒ‡ãƒ¼ã‚¿å‡¦ç†ä¸­ã«äºˆæœŸã›ã¬ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e_main)}")
        st.error(traceback.format_exc()) # è©³ç´°ãªãƒˆãƒ¬ãƒ¼ã‚¹ãƒãƒƒã‚¯ã‚’è¡¨ç¤º
        return False, None, None, None, None # 5ã¤ã®å€¤ã‚’è¿”ã™ã‚ˆã†ã«çµ±ä¸€


def create_data_processing_tab():
    """ãƒ‡ãƒ¼ã‚¿å‡¦ç†ã‚¿ãƒ–ã®UIå®Ÿè£…"""
    st.header("ğŸ“Š ãƒ‡ãƒ¼ã‚¿å…¥åŠ›") # åç§°å¤‰æ›´: ãƒ‡ãƒ¼ã‚¿å‡¦ç† -> ãƒ‡ãƒ¼ã‚¿å…¥åŠ›

    # ... (æ—¢å­˜ã® expander ã®èª¬æ˜æ–‡)
    with st.expander("â„¹ï¸ ãƒ‡ãƒ¼ã‚¿å…¥åŠ›ã«ã¤ã„ã¦", expanded=False):
        st.markdown("""
        **ãƒ‡ãƒ¼ã‚¿å…¥åŠ›ã®æµã‚Œ:**
        1. **å›ºå®šãƒ•ã‚¡ã‚¤ãƒ«**: ãƒ¡ã‚¤ãƒ³ã¨ãªã‚‹å…¥é™¢æ‚£è€…ãƒ‡ãƒ¼ã‚¿ï¼ˆå¿…é ˆã¾ãŸã¯ã‚­ãƒ£ãƒƒã‚·ãƒ¥åˆ©ç”¨ï¼‰
        2. **è¿½åŠ ãƒ•ã‚¡ã‚¤ãƒ«**: è£œå®Œãƒ‡ãƒ¼ã‚¿ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ã€è¤‡æ•°å¯ï¼‰
        3. **ç›®æ¨™å€¤ãƒ•ã‚¡ã‚¤ãƒ«**: éƒ¨é–€åˆ¥ç›®æ¨™è¨­å®šï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ã€CSVå½¢å¼ï¼‰

        **å¯¾å¿œãƒ•ã‚¡ã‚¤ãƒ«å½¢å¼ (å…¥é™¢ãƒ‡ãƒ¼ã‚¿):** Excel (.xlsx, .xls)
        **å¿…è¦ãªåˆ—å (æŸ”è»Ÿã«å¯¾å¿œè©¦è¡Œ):**
        ç—…æ£Ÿã‚³ãƒ¼ãƒ‰, è¨ºç™‚ç§‘å, æ—¥ä»˜, åœ¨é™¢æ‚£è€…æ•°, å…¥é™¢æ‚£è€…æ•°, ç·Šæ€¥å…¥é™¢æ‚£è€…æ•°, é€€é™¢æ‚£è€…æ•°, æ­»äº¡æ‚£è€…æ•°
        """)


    if 'data_processing_initialized' not in st.session_state: # ã“ã®ã‚­ãƒ¼ã¯æ®‹ã™
        st.session_state.data_processing_initialized = True
        st.session_state.data_processed = False
        st.session_state.df = None
        st.session_state.target_data = None
        st.session_state.all_results = None
        st.session_state.validation_results = None
        st.session_state.latest_data_date_str = "ãƒ‡ãƒ¼ã‚¿èª­è¾¼å‰"
        st.session_state.target_file_debug_info = None
        st.session_state.extracted_targets = None
        if 'performance_metrics' not in st.session_state:
            st.session_state.performance_metrics = {'data_load_time': 0, 'data_conversion_time': 0, 'processing_time': 0}

    st.subheader("ğŸ“ ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰")
    col_f1, col_f2, col_f3 = st.columns(3) # å¤‰æ•°åå¤‰æ›´
    with col_f1:
        base_file_uploader_widget = st.file_uploader( # å¤‰æ•°åå¤‰æ›´
            "å›ºå®šãƒ•ã‚¡ã‚¤ãƒ« (Excel)", type=["xlsx", "xls"], key="base_file_dp_tab_v2", # ã‚­ãƒ¼å¤‰æ›´
            help="ãƒ¡ã‚¤ãƒ³ã®Excelãƒ•ã‚¡ã‚¤ãƒ«ã€‚éå»å‡¦ç†æ¸ˆã¿ã®åŒä¸€ãƒ•ã‚¡ã‚¤ãƒ«ã¯ã‚­ãƒ£ãƒƒã‚·ãƒ¥åˆ©ç”¨å¯ï¼ˆã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ä¸è¦ï¼‰ã€‚"
        )
    with col_f2:
        new_files_uploader_widget = st.file_uploader( # å¤‰æ•°åå¤‰æ›´
            "è¿½åŠ ãƒ•ã‚¡ã‚¤ãƒ« (Excel)", type=["xlsx", "xls"], accept_multiple_files=True,
            key="new_files_dp_tab_v2", help="è£œå®Œãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆè¤‡æ•°å¯ï¼‰ã€‚" # ã‚­ãƒ¼å¤‰æ›´
        )
    with col_f3:
        target_file_uploader_widget = st.file_uploader( # å¤‰æ•°åå¤‰æ›´
            "ç›®æ¨™å€¤ãƒ•ã‚¡ã‚¤ãƒ« (CSV)", type=["csv"], key="target_file_dp_tab_v2", # ã‚­ãƒ¼å¤‰æ›´
            help="éƒ¨é–€åˆ¥ã®ç›®æ¨™å€¤ãƒ‡ãƒ¼ã‚¿ï¼ˆCSVå½¢å¼ï¼‰ã€‚"
        )

    # ç›®æ¨™å€¤ãƒ•ã‚¡ã‚¤ãƒ«ã®çŠ¶æ³è¡¨ç¤ºã¯ã‚µã‚¤ãƒ‰ãƒãƒ¼ã«ç§»å‹•æ¸ˆã¿ãªã®ã§ã€ã“ã“ã®ãƒ­ã‚¸ãƒƒã‚¯ã¯å‰Šé™¤
    # if st.session_state.get('target_data') is not None:
    #    ...

    # ãƒ•ã‚¡ã‚¤ãƒ«åˆ—åç¢ºèª (è¡¨ç¤ºãŒå†—é•·ã«ãªã‚‹ãŸã‚ã€ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã«ã™ã‚‹ã‹ã€ãƒ­ã‚°å‡ºåŠ›ã‚’ä¸»ã¨ã™ã‚‹)
    # if base_file_uploader_widget:
    #     with st.expander("ğŸ“‹ å›ºå®šãƒ•ã‚¡ã‚¤ãƒ«åˆ—åç¢ºèª (å…ˆé ­3è¡Œ)", expanded=False):
    #         show_excel_column_info(base_file_uploader_widget)
    # if new_files_uploader_widget:
    #     st.markdown("---") # è¦‹ãŸç›®ã®åŒºåˆ‡ã‚Š
    #     for i, file_obj_new in enumerate(new_files_uploader_widget):
    #         with st.expander(f"ğŸ“‹ è¿½åŠ ãƒ•ã‚¡ã‚¤ãƒ«: {file_obj_new.name} åˆ—åç¢ºèª (å…ˆé ­3è¡Œ)", expanded=False):
    #             show_excel_column_info(file_obj_new)


    # å‡¦ç†å®Ÿè¡Œãƒœã‚¿ãƒ³ã®æ´»æ€§åŒ–æ¡ä»¶
    # Parquetã‚­ãƒ£ãƒƒã‚·ãƒ¥ã®ãƒã‚§ãƒƒã‚¯ï¼ˆã“ã‚Œã¯ get_app_data_dir ãªã©ã‚’ä½¿ã£ã¦è¡Œã†ï¼‰
    app_data_dir_val = get_app_data_dir()
    parquet_base_path_val = os.path.join(app_data_dir_val, "processed_base_data.parquet") if app_data_dir_val else None
    can_process_now = False
    if base_file_uploader_widget:
        can_process_now = True
    elif parquet_base_path_val and os.path.exists(parquet_base_path_val):
        can_process_now = True
        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥åˆ©ç”¨ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã¯ process_data_with_progress å†…ã§åˆ¤æ–­ãƒ»è¡¨ç¤ºã™ã‚‹æ–¹ãŒé©åˆ‡
        # st.info("éå»ã«å‡¦ç†ã—ãŸãƒ™ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿ï¼ˆParquetã‚­ãƒ£ãƒƒã‚·ãƒ¥ï¼‰ãŒå­˜åœ¨ã—ã¾ã™...")
    elif new_files_uploader_widget: # è¿½åŠ ãƒ•ã‚¡ã‚¤ãƒ«ã®ã¿ã§ã‚‚å‡¦ç†è©¦è¡Œ
        can_process_now = True


    if can_process_now:
        if not st.session_state.get('data_processed', False):
            process_button_key = "process_data_button_dp_tab_v2" # ã‚­ãƒ¼å¤‰æ›´
            if st.button("ãƒ‡ãƒ¼ã‚¿å‡¦ç†ã‚’å®Ÿè¡Œ", key=process_button_key, use_container_width=True):
                # å‡¦ç†å¯¾è±¡ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’æ¸¡ã™
                base_file_to_process = base_file_uploader_widget
                new_files_to_process = new_files_uploader_widget if new_files_uploader_widget else []
                target_file_to_process = target_file_uploader_widget

                progress_bar_ui_main = st.progress(0, text="ãƒ‡ãƒ¼ã‚¿å‡¦ç†ã‚’é–‹å§‹ã—ã¾ã™...")
                success_flag, df_result_main, target_data_result_main, all_results_main, latest_date_obj_main = process_data_with_progress(
                    base_file_to_process, new_files_to_process, target_file_to_process, progress_bar_ui_main
                )
                if success_flag and df_result_main is not None and not df_result_main.empty:
                    st.session_state.df = df_result_main
                    st.session_state.target_data = target_data_result_main
                    st.session_state.all_results = all_results_main
                    st.session_state.data_processed = True
                    if isinstance(latest_date_obj_main, pd.Timestamp):
                        st.session_state.latest_data_date_str = latest_date_obj_main.strftime("%Yå¹´%mæœˆ%dæ—¥")
                    else: # validation_results ãŒè¿”ã£ã¦ããŸå ´åˆ (ã‚¨ãƒ©ãƒ¼ã‚±ãƒ¼ã‚¹)
                        st.session_state.latest_data_date_str = "ãƒ‡ãƒ¼ã‚¿å‡¦ç†å®Œäº† (æ—¥ä»˜ä¸æ˜)"
                        # validation_results ã®ã‚¨ãƒ©ãƒ¼è¡¨ç¤ºã¯ process_data_with_progress å†…ã§è¡Œã‚ã‚Œã‚‹ã‚ˆã†ã«ãªã£ãŸ
                    st.success(f"ãƒ‡ãƒ¼ã‚¿ã®å‡¦ç†ãŒå®Œäº†ã—ã¾ã—ãŸã€‚æœ€æ–°ãƒ‡ãƒ¼ã‚¿æ—¥ä»˜: {st.session_state.latest_data_date_str}")
                    st.session_state.mappings_initialized_after_processing = True # ãƒãƒƒãƒ”ãƒ³ã‚°ã‚‚å®Œäº†
                    perform_cleanup(deep=True)
                    st.rerun()
                else:
                    # ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã¯ process_data_with_progress å†…ã§è¡¨ç¤ºã•ã‚Œã‚‹ã¯ãš
                    if latest_date_obj_main is None and not success_flag: # è‡´å‘½çš„ãªã‚¨ãƒ©ãƒ¼ã§çµæœãŒNoneã®å ´åˆ
                        st.error("ãƒ‡ãƒ¼ã‚¿å‡¦ç†ä¸­ã«è‡´å‘½çš„ãªã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚è©³ç´°ã¯ãƒ­ã‚°ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
                    elif latest_date_obj_main and isinstance(latest_date_obj_main, dict) and latest_date_obj_main.get('errors'): # validation_results ãŒè¿”ã£ã¦ããŸå ´åˆ
                        st.error("ãƒ‡ãƒ¼ã‚¿æ¤œè¨¼ã§ã‚¨ãƒ©ãƒ¼ãŒæ¤œå‡ºã•ã‚Œã¾ã—ãŸã€‚è©³ç´°ã¯ä¸Šè¨˜ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")

        else: # ãƒ‡ãƒ¼ã‚¿å‡¦ç†æ¸ˆã¿ã®å ´åˆ
            st.success(f"ãƒ‡ãƒ¼ã‚¿å‡¦ç†æ¸ˆã¿ï¼ˆæœ€æ–°ãƒ‡ãƒ¼ã‚¿æ—¥ä»˜: {st.session_state.latest_data_date_str}ï¼‰")
            if st.session_state.get('target_data') is not None: st.success("ç›®æ¨™å€¤ãƒ‡ãƒ¼ã‚¿ã‚‚èª­ã¿è¾¼ã¿æ¸ˆã¿ã§ã™ã€‚")
            else: st.info("ç›®æ¨™å€¤ãƒ‡ãƒ¼ã‚¿ã¯èª­ã¿è¾¼ã¾ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")

            if st.session_state.get('df') is not None:
                df_display_main = st.session_state.df # å¤‰æ•°åå¤‰æ›´
                with st.expander("ãƒ‡ãƒ¼ã‚¿æ¦‚è¦", expanded=True):
                    # ... (æ—¢å­˜ã®ãƒ‡ãƒ¼ã‚¿æ¦‚è¦è¡¨ç¤ºãƒ­ã‚¸ãƒƒã‚¯ - å¤‰æ›´ãªã—) ...
                    col1_sum, col2_sum, col3_sum = st.columns(3)
                    with col1_sum:
                        if not df_display_main.empty and 'æ—¥ä»˜' in df_display_main.columns:
                            min_dt = df_display_main['æ—¥ä»˜'].min()
                            max_dt = df_display_main['æ—¥ä»˜'].max()
                            if pd.notna(min_dt) and pd.notna(max_dt): # NaTã§ãªã„ã“ã¨ã‚’ç¢ºèª
                                st.metric("ãƒ‡ãƒ¼ã‚¿æœŸé–“", f"{min_dt.strftime('%Y/%m/%d')} - {max_dt.strftime('%Y/%m/%d')}")
                            else: st.metric("ãƒ‡ãƒ¼ã‚¿æœŸé–“", "N/A (ç„¡åŠ¹ãªæ—¥ä»˜)")
                        else: st.metric("ãƒ‡ãƒ¼ã‚¿æœŸé–“", "N/A")
                    with col2_sum: st.metric("ç·ãƒ¬ã‚³ãƒ¼ãƒ‰æ•°", f"{len(df_display_main):,}")
                    with col3_sum: st.metric("ç—…æ£Ÿæ•°", f"{df_display_main['ç—…æ£Ÿã‚³ãƒ¼ãƒ‰'].nunique() if 'ç—…æ£Ÿã‚³ãƒ¼ãƒ‰' in df_display_main.columns else 'N/A'}")
                    col1_sum2, col2_sum2, col3_sum2 = st.columns(3)
                    with col1_sum2: st.metric("è¨ºç™‚ç§‘æ•°", f"{df_display_main['è¨ºç™‚ç§‘å'].nunique() if 'è¨ºç™‚ç§‘å' in df_display_main.columns else 'N/A'}")
                    with col2_sum2: st.metric("å¹³æ—¥æ•°", f"{(df_display_main['å¹³æ—¥åˆ¤å®š'] == 'å¹³æ—¥').sum()}" if "å¹³æ—¥åˆ¤å®š" in df_display_main.columns else "N/A")
                    with col3_sum2: st.metric("ä¼‘æ—¥æ•°", f"{(df_display_main['å¹³æ—¥åˆ¤å®š'] == 'ä¼‘æ—¥').sum()}" if "å¹³æ—¥åˆ¤å®š" in df_display_main.columns else "N/A")

                    perf_metrics_disp = st.session_state.get('performance_metrics', {}) # å¤‰æ•°åå¤‰æ›´
                    if perf_metrics_disp:
                        st.subheader("å‡¦ç†ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹")
                        pcol1, pcol2, pcol3, pcol4 = st.columns(4)
                        with pcol1: st.metric("ãƒ‡ãƒ¼ã‚¿èª­è¾¼æ™‚é–“", f"{perf_metrics_disp.get('data_load_time', 0):.1f}ç§’")
                        # with pcol2: st.metric("Parquetå¤‰æ›æ™‚é–“", f"{perf_metrics_disp.get('data_conversion_time', 0):.1f}ç§’") # ã“ã®ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã¯å‰Šé™¤ã¾ãŸã¯è¦‹ç›´ã—
                        with pcol2: pass # ç©ºç™½åˆ—
                        with pcol3: st.metric("ãƒ‡ãƒ¼ã‚¿å‡¦ç†æ™‚é–“", f"{perf_metrics_disp.get('processing_time', 0):.1f}ç§’")
                        with pcol4:
                            try: mem_info_disp = log_memory_usage(); # å¤‰æ•°åå¤‰æ›´
                                if mem_info_disp: st.metric("ç¾åœ¨ã®ãƒ¡ãƒ¢ãƒªä½¿ç”¨", f"{mem_info_disp.get('process_mb', 0):.1f} MB ({mem_info_disp.get('process_percent', 0):.1f}%)")
                                else: st.metric("ãƒ¡ãƒ¢ãƒªæƒ…å ±", "å–å¾—ä¸å¯")
                            except Exception: st.metric("ãƒ¡ãƒ¢ãƒªæƒ…å ±", "å–å¾—ä¸å¯")

                validation_res_main = st.session_state.get('validation_results') # å¤‰æ•°åå¤‰æ›´
                if validation_res_main:
                    if validation_res_main.get("warnings") or validation_res_main.get("info") or validation_res_main.get("errors"):
                        with st.expander("ãƒ‡ãƒ¼ã‚¿æ¤œè¨¼çµæœ", expanded=False):
                            for err_msg_disp in validation_res_main.get("errors", []): st.error(err_msg_disp) # å¤‰æ•°åå¤‰æ›´
                            for info_msg_disp in validation_res_main.get("info", []): st.info(info_msg_disp) # å¤‰æ•°åå¤‰æ›´
                            for warn_msg_disp_main in validation_res_main.get("warnings", []): st.warning(warn_msg_disp_main) # å¤‰æ•°åå¤‰æ›´
            
            # ãƒãƒƒãƒ”ãƒ³ã‚°æƒ…å ±ã®è¡¨ç¤ºã¯ã‚µã‚¤ãƒ‰ãƒãƒ¼ã«é›†ç´„ã—ãŸã®ã§ã€ã“ã“ã§ã¯å‰Šé™¤
            # if st.session_state.get('data_processed', False) and st.session_state.get('target_data') is not None:
            #    with st.expander("è¨ºç™‚ç§‘ãƒãƒƒãƒ”ãƒ³ã‚°è¨­å®š", expanded=False): ...

            if st.button("ãƒ‡ãƒ¼ã‚¿ã‚’ãƒªã‚»ãƒƒãƒˆ (Parquetã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚‚å‰Šé™¤)", key="reset_data_button_dp_tab_v2", use_container_width=True): # ã‚­ãƒ¼å¤‰æ›´
                st.session_state.data_processed = False
                st.session_state.df = None; st.session_state.all_results = None; st.session_state.target_data = None
                st.session_state.validation_results = None; st.session_state.latest_data_date_str = "ãƒ‡ãƒ¼ã‚¿èª­è¾¼å‰"
                st.session_state.target_file_debug_info = None; st.session_state.extracted_targets = None
                st.session_state.performance_metrics = {'data_load_time': 0, 'data_conversion_time': 0, 'processing_time': 0}
                st.session_state.dept_mapping = {}; st.session_state.dept_mapping_initialized = False
                st.session_state.ward_mapping = {}; st.session_state.ward_mapping_initialized = False
                st.session_state.mappings_initialized_after_processing = False # ãƒãƒƒãƒ”ãƒ³ã‚°åˆæœŸåŒ–ãƒ•ãƒ©ã‚°ã‚‚ãƒªã‚»ãƒƒãƒˆ

                if app_data_dir_val: # å¤‰æ•°åå¤‰æ›´
                    parquet_to_delete_main = os.path.join(app_data_dir_val, "processed_base_data.parquet") # å¤‰æ•°åå¤‰æ›´
                    info_to_delete_main = os.path.join(app_data_dir_val, "base_file_info.json") # å¤‰æ•°åå¤‰æ›´
                    if os.path.exists(parquet_to_delete_main):
                        try: os.remove(parquet_to_delete_main); st.info("ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã•ã‚ŒãŸãƒ™ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿ã‚’å‰Šé™¤ã—ã¾ã—ãŸã€‚")
                        except Exception as e_del_pq: logger.warning(f"Parquetå‰Šé™¤ã‚¨ãƒ©ãƒ¼: {e_del_pq}") # å¤‰æ•°åå¤‰æ›´
                    if os.path.exists(info_to_delete_main):
                        try: os.remove(info_to_delete_main)
                        except Exception as e_del_info: logger.warning(f"Infoãƒ•ã‚¡ã‚¤ãƒ«å‰Šé™¤ã‚¨ãƒ©ãƒ¼: {e_del_info}") # å¤‰æ•°åå¤‰æ›´
                perform_cleanup(deep=True)
                st.rerun()
    else: # can_process_now ãŒ False ã®å ´åˆ
        st.info("ã€Œå›ºå®šãƒ•ã‚¡ã‚¤ãƒ«ã€ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã™ã‚‹ã‹ã€ä»¥å‰å‡¦ç†ã—ãŸãƒ™ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’åˆ©ç”¨ã§ãã‚‹çŠ¶æ…‹ã«ã—ã¦ãã ã•ã„ã€‚ã¾ãŸã¯ã€Œè¿½åŠ ãƒ•ã‚¡ã‚¤ãƒ«ã€ã®ã¿ã§ã‚‚å‡¦ç†ã‚’é–‹å§‹ã§ãã¾ã™ã€‚")
        # ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ãƒœã‚¿ãƒ³ã¯å‰Šé™¤ã¾ãŸã¯åˆ¥é€”æ¤œè¨
        # if st.button("ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨", key="sample_data_button_dp_tab"):
        #     st.info("ã“ã®æ©Ÿèƒ½ã¯é–‹ç™ºä¸­ã§ã™ã€‚ç¾åœ¨ã¯è‡ªåˆ†ã®ãƒ‡ãƒ¼ã‚¿ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ã€‚")


# show_excel_column_info é–¢æ•°ã¯ loader.py ã«ã¯å­˜åœ¨ã›ãšã€data_processing_tab.py ã®ãƒ­ãƒ¼ã‚«ãƒ«é–¢æ•°ã ã£ãŸã€‚
# åˆ—åç¢ºèªUIã¯å†—é•·ã«ãªã‚‹ãŸã‚ã€åŸºæœ¬çš„ã«ã¯ãƒ­ã‚°å‡ºåŠ›ã¨ã€é‡è¦ãªåˆ—ãŒãªã„å ´åˆã®ã‚¨ãƒ©ãƒ¼/è­¦å‘Šã§å¯¾å¿œã™ã‚‹æ–¹é‡ã€‚
# ã‚‚ã—UIã§ç¢ºèªã—ãŸã„å ´åˆã¯ã€ã“ã®é–¢æ•°ã‚’å†åº¦ data_processing_tab.py ã«å®šç¾©ã—ã€é™å®šçš„ã«ä½¿ç”¨ã™ã‚‹ã€‚
# def show_excel_column_info(uploaded_file):
#    ... (å…ƒã®ãƒ­ã‚¸ãƒƒã‚¯)