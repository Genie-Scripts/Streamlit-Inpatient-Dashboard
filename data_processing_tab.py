# data_processing_tab.py
import streamlit as st
import pandas as pd
import numpy as np
from datetime import datetime, timedelta
import logging
import traceback

# utilsã‹ã‚‰å¿…è¦ãªé–¢æ•°ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from utils import initialize_all_mappings

# unified_filtersã‹ã‚‰å¿…è¦ãªé–¢æ•°ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
try:
    from unified_filters import initialize_unified_filters
except ImportError:
    initialize_unified_filters = None

# data_persistenceã‹ã‚‰å¿…è¦ãªé–¢æ•°ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
try:
    from data_persistence import save_data_to_file
except ImportError:
    save_data_to_file = None

logger = logging.getLogger(__name__)

def validate_uploaded_data(df):
    """
    ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ã®å¦¥å½“æ€§ã‚’ãƒã‚§ãƒƒã‚¯
    
    Args:
        df (pd.DataFrame): ãƒã‚§ãƒƒã‚¯å¯¾è±¡ã®ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ 
        
    Returns:
        dict: ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³çµæœ
    """
    validation_results = {
        'is_valid': True,
        'warnings': [],
        'errors': [],
        'info': []
    }
    
    try:
        # åŸºæœ¬çš„ãªãƒ‡ãƒ¼ã‚¿æ§‹é€ ãƒã‚§ãƒƒã‚¯
        if df is None or df.empty:
            validation_results['errors'].append("ãƒ‡ãƒ¼ã‚¿ãŒç©ºã§ã™")
            validation_results['is_valid'] = False
            return validation_results
        
        # å¿…é ˆåˆ—ã®ãƒã‚§ãƒƒã‚¯
        required_columns = ['æ—¥ä»˜']
        missing_columns = [col for col in required_columns if col not in df.columns]
        if missing_columns:
            validation_results['errors'].append(f"å¿…é ˆåˆ—ãŒä¸è¶³ã—ã¦ã„ã¾ã™: {', '.join(missing_columns)}")
            validation_results['is_valid'] = False
        
        # æ—¥ä»˜åˆ—ã®ãƒã‚§ãƒƒã‚¯
        if 'æ—¥ä»˜' in df.columns:
            # æ—¥ä»˜ã¨ã—ã¦å¤‰æ›å¯èƒ½ã‹ãƒã‚§ãƒƒã‚¯
            try:
                date_series = pd.to_datetime(df['æ—¥ä»˜'], errors='coerce')
                invalid_dates = date_series.isna().sum()
                if invalid_dates > 0:
                    validation_results['warnings'].append(f"ç„¡åŠ¹ãªæ—¥ä»˜ãƒ‡ãƒ¼ã‚¿ãŒ{invalid_dates}ä»¶ã‚ã‚Šã¾ã™ï¼ˆé™¤å¤–ã•ã‚Œã¾ã™ï¼‰")
                
                # æ—¥ä»˜ç¯„å›²ã®ãƒã‚§ãƒƒã‚¯
                valid_dates = date_series.dropna()
                if not valid_dates.empty:
                    min_date = valid_dates.min()
                    max_date = valid_dates.max()
                    date_range = (max_date - min_date).days
                    
                    if date_range > 3650:  # 10å¹´ä»¥ä¸Š
                        validation_results['warnings'].append(f"ãƒ‡ãƒ¼ã‚¿æœŸé–“ãŒé•·ã™ãã¾ã™ï¼ˆ{date_range}æ—¥é–“ï¼‰")
                    
                    validation_results['info'].append(f"ãƒ‡ãƒ¼ã‚¿æœŸé–“: {min_date.strftime('%Y/%m/%d')} ï½ {max_date.strftime('%Y/%m/%d')} ({date_range + 1}æ—¥é–“)")
                
            except Exception as e:
                validation_results['errors'].append(f"æ—¥ä»˜åˆ—ã®å‡¦ç†ã§ã‚¨ãƒ©ãƒ¼: {str(e)}")
                validation_results['is_valid'] = False
        
        # æ¨å¥¨åˆ—ã®ãƒã‚§ãƒƒã‚¯
        recommended_columns = ['ç—…æ£Ÿã‚³ãƒ¼ãƒ‰', 'è¨ºç™‚ç§‘å', 'åœ¨é™¢æ‚£è€…æ•°', 'å…¥é™¢æ‚£è€…æ•°', 'é€€é™¢æ‚£è€…æ•°']
        missing_recommended = [col for col in recommended_columns if col not in df.columns]
        if missing_recommended:
            validation_results['warnings'].append(f"æ¨å¥¨åˆ—ãŒä¸è¶³ã—ã¦ã„ã¾ã™: {', '.join(missing_recommended)}")
        
        # ãƒ‡ãƒ¼ã‚¿å‹ã®ãƒã‚§ãƒƒã‚¯
        numeric_columns = ['åœ¨é™¢æ‚£è€…æ•°', 'å…¥é™¢æ‚£è€…æ•°', 'é€€é™¢æ‚£è€…æ•°']
        for col in numeric_columns:
            if col in df.columns:
                try:
                    numeric_data = pd.to_numeric(df[col], errors='coerce')
                    invalid_numeric = numeric_data.isna().sum()
                    if invalid_numeric > 0:
                        validation_results['warnings'].append(f"{col}åˆ—ã«æ•°å€¤ã§ãªã„ãƒ‡ãƒ¼ã‚¿ãŒ{invalid_numeric}ä»¶ã‚ã‚Šã¾ã™")
                except Exception as e:
                    validation_results['warnings'].append(f"{col}åˆ—ã®æ•°å€¤ãƒã‚§ãƒƒã‚¯ã§ã‚¨ãƒ©ãƒ¼: {str(e)}")
        
        # åŸºæœ¬çµ±è¨ˆæƒ…å ±
        validation_results['info'].append(f"ç·ãƒ¬ã‚³ãƒ¼ãƒ‰æ•°: {len(df):,}ä»¶")
        validation_results['info'].append(f"åˆ—æ•°: {len(df.columns)}åˆ—")
        
        # é‡è¤‡ãƒã‚§ãƒƒã‚¯
        if len(df.columns) >= 3:
            key_columns = ['æ—¥ä»˜']
            if 'ç—…æ£Ÿã‚³ãƒ¼ãƒ‰' in df.columns:
                key_columns.append('ç—…æ£Ÿã‚³ãƒ¼ãƒ‰')
            if 'è¨ºç™‚ç§‘å' in df.columns:
                key_columns.append('è¨ºç™‚ç§‘å')
            
            duplicates = df.duplicated(subset=key_columns).sum()
            if duplicates > 0:
                validation_results['warnings'].append(f"é‡è¤‡ãƒ¬ã‚³ãƒ¼ãƒ‰ãŒ{duplicates}ä»¶ã‚ã‚Šã¾ã™")
        
    except Exception as e:
        validation_results['errors'].append(f"ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³å‡¦ç†ã§ã‚¨ãƒ©ãƒ¼: {str(e)}")
        validation_results['is_valid'] = False
        logger.error(f"ãƒ‡ãƒ¼ã‚¿ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ã‚¨ãƒ©ãƒ¼: {e}", exc_info=True)
    
    return validation_results

def preprocess_data(df):
    """
    ãƒ‡ãƒ¼ã‚¿ã®å‰å‡¦ç†ã‚’å®Ÿè¡Œ
    
    Args:
        df (pd.DataFrame): å‰å‡¦ç†å¯¾è±¡ã®ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ 
        
    Returns:
        pd.DataFrame: å‰å‡¦ç†æ¸ˆã¿ã®ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ 
    """
    try:
        if df is None or df.empty:
            return df
        
        processed_df = df.copy()
        
        # æ—¥ä»˜åˆ—ã®æ­£è¦åŒ–
        if 'æ—¥ä»˜' in processed_df.columns:
            processed_df['æ—¥ä»˜'] = pd.to_datetime(processed_df['æ—¥ä»˜'], errors='coerce')
            processed_df = processed_df.dropna(subset=['æ—¥ä»˜'])
            processed_df['æ—¥ä»˜'] = processed_df['æ—¥ä»˜'].dt.normalize()
        
        # æ•°å€¤åˆ—ã®å¤‰æ›
        numeric_columns = ['åœ¨é™¢æ‚£è€…æ•°', 'å…¥é™¢æ‚£è€…æ•°', 'é€€é™¢æ‚£è€…æ•°']
        for col in numeric_columns:
            if col in processed_df.columns:
                processed_df[col] = pd.to_numeric(processed_df[col], errors='coerce').fillna(0)
        
        # æ–‡å­—åˆ—åˆ—ã®ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°
        string_columns = ['ç—…æ£Ÿã‚³ãƒ¼ãƒ‰', 'è¨ºç™‚ç§‘å']
        for col in string_columns:
            if col in processed_df.columns:
                processed_df[col] = processed_df[col].astype(str).str.strip()
        
        # é‡è¤‡ã®é™¤å»
        if len(processed_df.columns) >= 3:
            key_columns = ['æ—¥ä»˜']
            if 'ç—…æ£Ÿã‚³ãƒ¼ãƒ‰' in processed_df.columns:
                key_columns.append('ç—…æ£Ÿã‚³ãƒ¼ãƒ‰')
            if 'è¨ºç™‚ç§‘å' in processed_df.columns:
                key_columns.append('è¨ºç™‚ç§‘å')
            
            processed_df = processed_df.drop_duplicates(subset=key_columns, keep='last')
        
        # æ—¥ä»˜ã§ã‚½ãƒ¼ãƒˆ
        if 'æ—¥ä»˜' in processed_df.columns:
            processed_df = processed_df.sort_values('æ—¥ä»˜').reset_index(drop=True)
        
        return processed_df
        
    except Exception as e:
        logger.error(f"ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}", exc_info=True)
        return df

def create_data_processing_tab():
    """
    ãƒ‡ãƒ¼ã‚¿å…¥åŠ›ãƒ»å‡¦ç†ã‚¿ãƒ–ã®ãƒ¡ã‚¤ãƒ³é–¢æ•°
    """
    st.header("ğŸ“¥ ãƒ‡ãƒ¼ã‚¿å…¥åŠ›ãƒ»å‡¦ç†")
    
    # ãƒ‡ãƒ¼ã‚¿ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã‚»ã‚¯ã‚·ãƒ§ãƒ³
    st.markdown("### ğŸ“‚ ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰")
    
    col1, col2 = st.columns([2, 1])
    
    with col1:
        uploaded_file = st.file_uploader(
            "å…¥é™¢ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ã‚’é¸æŠã—ã¦ãã ã•ã„",
            type=['xlsx', 'xls', 'csv'],
            help="Excelå½¢å¼ã¾ãŸã¯CSVå½¢å¼ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„"
        )
    
    with col2:
        st.markdown("**ğŸ“‹ å¿…è¦ãªåˆ—:**")
        st.markdown("â€¢ æ—¥ä»˜ï¼ˆå¿…é ˆï¼‰")
        st.markdown("â€¢ ç—…æ£Ÿã‚³ãƒ¼ãƒ‰ï¼ˆæ¨å¥¨ï¼‰")
        st.markdown("â€¢ è¨ºç™‚ç§‘åï¼ˆæ¨å¥¨ï¼‰")
        st.markdown("â€¢ åœ¨é™¢æ‚£è€…æ•°ï¼ˆæ¨å¥¨ï¼‰")
        st.markdown("â€¢ å…¥é™¢æ‚£è€…æ•°ï¼ˆæ¨å¥¨ï¼‰")
        st.markdown("â€¢ é€€é™¢æ‚£è€…æ•°ï¼ˆæ¨å¥¨ï¼‰")
    
    # ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†ã‚ªãƒ—ã‚·ãƒ§ãƒ³
    if uploaded_file is not None:
        st.markdown("### âš™ï¸ å‡¦ç†ã‚ªãƒ—ã‚·ãƒ§ãƒ³")
        
        col_opt1, col_opt2, col_opt3 = st.columns(3)
        
        with col_opt1:
            file_encoding = st.selectbox(
                "ãƒ•ã‚¡ã‚¤ãƒ«ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°",
                ["utf-8", "shift_jis", "cp932"],
                index=0,
                help="CSVãƒ•ã‚¡ã‚¤ãƒ«ã®æ–‡å­—ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°"
            )
        
        with col_opt2:
            data_validation = st.checkbox(
                "ãƒ‡ãƒ¼ã‚¿æ¤œè¨¼ã‚’å®Ÿè¡Œ",
                value=True,
                help="ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰æ™‚ã«ãƒ‡ãƒ¼ã‚¿ã®å¦¥å½“æ€§ã‚’ãƒã‚§ãƒƒã‚¯"
            )
        
        with col_opt3:
            auto_preprocess = st.checkbox(
                "è‡ªå‹•å‰å‡¦ç†ã‚’å®Ÿè¡Œ",
                value=True,
                help="ãƒ‡ãƒ¼ã‚¿å‹å¤‰æ›ã€é‡è¤‡é™¤å»ç­‰ã‚’è‡ªå‹•å®Ÿè¡Œ"
            )
        
        # ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†å®Ÿè¡Œ
        if st.button("ğŸš€ ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‡¦ç†", type="primary", use_container_width=True):
            try:
                with st.spinner("ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿ä¸­..."):
                    # ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿
                    if uploaded_file.name.endswith('.csv'):
                        try:
                            df = pd.read_csv(uploaded_file, encoding=file_encoding)
                        except UnicodeDecodeError:
                            # ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚¨ãƒ©ãƒ¼ã®å ´åˆã€ä»–ã®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚’è©¦è¡Œ
                            st.warning(f"æŒ‡å®šã•ã‚ŒãŸã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ï¼ˆ{file_encoding}ï¼‰ã§èª­ã¿è¾¼ã‚ã¾ã›ã‚“ã§ã—ãŸã€‚ä»–ã®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚’è©¦è¡Œã—ã¾ã™ã€‚")
                            for alt_encoding in ['utf-8', 'shift_jis', 'cp932']:
                                if alt_encoding != file_encoding:
                                    try:
                                        uploaded_file.seek(0)  # ãƒ•ã‚¡ã‚¤ãƒ«ãƒã‚¤ãƒ³ã‚¿ã‚’ãƒªã‚»ãƒƒãƒˆ
                                        df = pd.read_csv(uploaded_file, encoding=alt_encoding)
                                        st.success(f"ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚° {alt_encoding} ã§èª­ã¿è¾¼ã¿æˆåŠŸ")
                                        break
                                    except UnicodeDecodeError:
                                        continue
                            else:
                                raise UnicodeDecodeError("ã™ã¹ã¦ã®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã§èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸ")
                    else:
                        df = pd.read_excel(uploaded_file)
                
                if df.empty:
                    st.error("âŒ ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ã«ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")
                    return
                
                st.success(f"âœ… ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿å®Œäº†: {len(df):,}è¡Œ Ã— {len(df.columns)}åˆ—")
                
                # ãƒ‡ãƒ¼ã‚¿ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼
                st.markdown("### ğŸ‘€ ãƒ‡ãƒ¼ã‚¿ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼")
                st.dataframe(df.head(10), use_container_width=True)
                
                # ãƒ‡ãƒ¼ã‚¿æ¤œè¨¼
                if data_validation:
                    st.markdown("### ğŸ” ãƒ‡ãƒ¼ã‚¿æ¤œè¨¼çµæœ")
                    validation_results = validate_uploaded_data(df)
                    
                    # ã‚¨ãƒ©ãƒ¼è¡¨ç¤º
                    if validation_results['errors']:
                        for error in validation_results['errors']:
                            st.error(f"âŒ {error}")
                    
                    # è­¦å‘Šè¡¨ç¤º
                    if validation_results['warnings']:
                        for warning in validation_results['warnings']:
                            st.warning(f"âš ï¸ {warning}")
                    
                    # æƒ…å ±è¡¨ç¤º
                    if validation_results['info']:
                        for info in validation_results['info']:
                            st.info(f"â„¹ï¸ {info}")
                    
                    if not validation_results['is_valid']:
                        st.error("ãƒ‡ãƒ¼ã‚¿æ¤œè¨¼ã«å¤±æ•—ã—ã¾ã—ãŸã€‚ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
                        return
                
                # è‡ªå‹•å‰å‡¦ç†
                if auto_preprocess:
                    st.markdown("### ğŸ”§ ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†")
                    with st.spinner("ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†ä¸­..."):
                        processed_df = preprocess_data(df)
                        
                        if len(processed_df) != len(df):
                            st.info(f"å‰å‡¦ç†ã«ã‚ˆã‚Š {len(df) - len(processed_df)} è¡ŒãŒé™¤å»ã•ã‚Œã¾ã—ãŸï¼ˆé‡è¤‡ã€ç„¡åŠ¹ãƒ‡ãƒ¼ã‚¿ç­‰ï¼‰")
                        
                        df = processed_df
                
                # ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã«ä¿å­˜
                st.session_state['df'] = df
                st.session_state['target_data'] = None  # ç›®æ¨™å€¤ãƒ‡ãƒ¼ã‚¿ã¯ãƒªã‚»ãƒƒãƒˆ
                st.session_state['data_processed'] = True
                st.session_state['data_source'] = 'data_processing_tab'
                
                # æœ€æ–°æ—¥ä»˜ã®è¨­å®š
                if 'æ—¥ä»˜' in df.columns and not df['æ—¥ä»˜'].empty:
                    latest_date = df['æ—¥ä»˜'].max()
                    st.session_state.latest_data_date_str = latest_date.strftime('%Yå¹´%mæœˆ%dæ—¥')
                else:
                    st.session_state.latest_data_date_str = "æ—¥ä»˜ä¸æ˜"
                
                # ãƒãƒƒãƒ”ãƒ³ã‚°ã¨ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ã®åˆæœŸåŒ–
                initialize_all_mappings(df, None)
                if initialize_unified_filters:
                    initialize_unified_filters(df)
                st.session_state.mappings_initialized_after_processing = True
                
                st.success("âœ… ãƒ‡ãƒ¼ã‚¿å‡¦ç†ãŒå®Œäº†ã—ã¾ã—ãŸï¼")
                
                # è‡ªå‹•ä¿å­˜ã‚ªãƒ—ã‚·ãƒ§ãƒ³
                if st.checkbox("å‡¦ç†æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ã‚’è‡ªå‹•ä¿å­˜", value=True):
                    if save_data_to_file:
                        metadata = {
                            'processing_timestamp': datetime.now().isoformat(),
                            'source_file': uploaded_file.name,
                            'validation_performed': data_validation,
                            'preprocessing_performed': auto_preprocess
                        }
                        
                        if save_data_to_file(df, None, metadata):
                            st.success("ğŸ’¾ ãƒ‡ãƒ¼ã‚¿ã‚’è‡ªå‹•ä¿å­˜ã—ã¾ã—ãŸ")
                        else:
                            st.warning("âš ï¸ è‡ªå‹•ä¿å­˜ã«å¤±æ•—ã—ã¾ã—ãŸ")
                
                # å‡¦ç†å®Œäº†å¾Œã®æƒ…å ±è¡¨ç¤º
                st.markdown("### ğŸ“Š å‡¦ç†å®Œäº†ãƒ‡ãƒ¼ã‚¿æƒ…å ±")
                col_info1, col_info2, col_info3 = st.columns(3)
                
                with col_info1:
                    st.metric("ç·ãƒ¬ã‚³ãƒ¼ãƒ‰æ•°", f"{len(df):,}ä»¶")
                
                with col_info2:
                    if 'æ—¥ä»˜' in df.columns:
                        min_date = df['æ—¥ä»˜'].min()
                        max_date = df['æ—¥ä»˜'].max()
                        period_days = (max_date - min_date).days + 1
                        st.metric("ãƒ‡ãƒ¼ã‚¿æœŸé–“", f"{period_days}æ—¥é–“")
                    else:
                        st.metric("ãƒ‡ãƒ¼ã‚¿æœŸé–“", "ä¸æ˜")
                
                with col_info3:
                    unique_depts = df['è¨ºç™‚ç§‘å'].nunique() if 'è¨ºç™‚ç§‘å' in df.columns else 0
                    unique_wards = df['ç—…æ£Ÿã‚³ãƒ¼ãƒ‰'].nunique() if 'ç—…æ£Ÿã‚³ãƒ¼ãƒ‰' in df.columns else 0
                    st.metric("éƒ¨é–€æ•°", f"è¨ºç™‚ç§‘{unique_depts}ã€ç—…æ£Ÿ{unique_wards}")
                
                # ä»–ã®ã‚¿ãƒ–ã¸ã®èª˜å°
                st.markdown("---")
                st.info("ğŸ‰ ãƒ‡ãƒ¼ã‚¿å‡¦ç†ãŒå®Œäº†ã—ã¾ã—ãŸï¼ä»–ã®ã‚¿ãƒ–ã§åˆ†æã‚’é–‹å§‹ã§ãã¾ã™ã€‚")
                
            except Exception as e:
                st.error(f"âŒ ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}")
                logger.error(f"ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}", exc_info=True)
                st.error(traceback.format_exc())
    
    else:
        # ãƒ•ã‚¡ã‚¤ãƒ«ãŒé¸æŠã•ã‚Œã¦ã„ãªã„å ´åˆã®è¡¨ç¤º
        st.markdown("### ğŸ’¡ ä½¿ç”¨æ–¹æ³•")
        st.markdown("""
        1. **ãƒ•ã‚¡ã‚¤ãƒ«é¸æŠ**: Excel (.xlsx, .xls) ã¾ãŸã¯ CSV ãƒ•ã‚¡ã‚¤ãƒ«ã‚’é¸æŠ
        2. **å‡¦ç†ã‚ªãƒ—ã‚·ãƒ§ãƒ³è¨­å®š**: ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚„å‡¦ç†æ–¹æ³•ã‚’æŒ‡å®š
        3. **å‡¦ç†å®Ÿè¡Œ**: ã€Œãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‡¦ç†ã€ãƒœã‚¿ãƒ³ã‚’ã‚¯ãƒªãƒƒã‚¯
        4. **æ¤œè¨¼ãƒ»å‰å‡¦ç†**: ãƒ‡ãƒ¼ã‚¿ã®å¦¥å½“æ€§ãƒã‚§ãƒƒã‚¯ã¨è‡ªå‹•å‰å‡¦ç†
        5. **å®Œäº†**: ä»–ã®ã‚¿ãƒ–ã§åˆ†æé–‹å§‹
        """)
        
        # ç¾åœ¨ã®ãƒ‡ãƒ¼ã‚¿çŠ¶æ³è¡¨ç¤º
        if st.session_state.get('data_processed', False):
            st.markdown("### ğŸ“Š ç¾åœ¨ã®ãƒ‡ãƒ¼ã‚¿çŠ¶æ³")
            df_current = st.session_state.get('df')
            if df_current is not None and not df_current.empty:
                st.success(f"âœ… ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿æ¸ˆã¿: {len(df_current):,}ä»¶")
                
                col_current1, col_current2 = st.columns(2)
                with col_current1:
                    st.write(f"ğŸ“… æœ€æ–°æ—¥ä»˜: {st.session_state.get('latest_data_date_str', 'ä¸æ˜')}")
                    st.write(f"ğŸ”„ èª­ã¿è¾¼ã¿å…ƒ: {st.session_state.get('data_source', 'ä¸æ˜')}")
                
                with col_current2:
                    if 'æ—¥ä»˜' in df_current.columns:
                        min_date = df_current['æ—¥ä»˜'].min()
                        max_date = df_current['æ—¥ä»˜'].max()
                        period_days = (max_date - min_date).days + 1
                        st.write(f"ğŸ“Š ãƒ‡ãƒ¼ã‚¿æœŸé–“: {period_days}æ—¥é–“")
                    
                    st.write(f"ğŸ“‹ åˆ—æ•°: {len(df_current.columns)}åˆ—")
            else:
                st.warning("âš ï¸ ãƒ‡ãƒ¼ã‚¿å‡¦ç†ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¦ã„ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™")
        else:
            st.info("ğŸ“ ãƒ‡ãƒ¼ã‚¿ãŒèª­ã¿è¾¼ã¾ã‚Œã¦ã„ã¾ã›ã‚“")
    
    # ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
    st.markdown("---")
    st.markdown("### ğŸ“‹ ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿")
    
    # ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã®ä½œæˆ
    sample_dates = pd.date_range(start='2024-01-01', end='2024-01-31', freq='D')
    sample_data = []
    
    for date in sample_dates:
        for ward in ['A1ç—…æ£Ÿ', 'B2ç—…æ£Ÿ', 'ICU']:
            for dept in ['å†…ç§‘', 'å¤–ç§‘', 'å°å…ç§‘']:
                sample_data.append({
                    'æ—¥ä»˜': date.strftime('%Y-%m-%d'),
                    'ç—…æ£Ÿã‚³ãƒ¼ãƒ‰': ward,
                    'è¨ºç™‚ç§‘å': dept,
                    'åœ¨é™¢æ‚£è€…æ•°': np.random.randint(20, 50),
                    'å…¥é™¢æ‚£è€…æ•°': np.random.randint(1, 5),
                    'é€€é™¢æ‚£è€…æ•°': np.random.randint(1, 5)
                })
    
    sample_df = pd.DataFrame(sample_data)
    sample_csv = sample_df.to_csv(index=False, encoding='utf-8')
    
    st.download_button(
        label="ğŸ“„ ã‚µãƒ³ãƒ—ãƒ«CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
        data=sample_csv,
        file_name="sample_inpatient_data.csv",
        mime="text/csv",
        help="ãƒ‡ãƒ¼ã‚¿å½¢å¼ã®å‚è€ƒã¨ã—ã¦ä½¿ç”¨ã—ã¦ãã ã•ã„"
    )
    
    # ãƒ‡ãƒ¼ã‚¿å½¢å¼èª¬æ˜
    with st.expander("ğŸ“– ãƒ‡ãƒ¼ã‚¿å½¢å¼ã®è©³ç´°èª¬æ˜", expanded=False):
        st.markdown("""
        **ğŸ“… æ—¥ä»˜åˆ—**
        - å½¢å¼: YYYY-MM-DDï¼ˆä¾‹: 2024-01-15ï¼‰
        - å¿…é ˆé …ç›®ã§ã™
        
        **ğŸ¥ ç—…æ£Ÿã‚³ãƒ¼ãƒ‰åˆ—**
        - å„ç—…æ£Ÿã®è­˜åˆ¥ã‚³ãƒ¼ãƒ‰
        - ä¾‹: A1ç—…æ£Ÿ, B2ç—…æ£Ÿ, ICU
        
        **âš•ï¸ è¨ºç™‚ç§‘ååˆ—**
        - è¨ºç™‚ç§‘ã®åç§°
        - ä¾‹: å†…ç§‘, å¤–ç§‘, å°å…ç§‘
        
        **ğŸ‘¥ åœ¨é™¢æ‚£è€…æ•°åˆ—**
        - ãã®æ—¥ã®åœ¨é™¢æ‚£è€…æ•°
        - æ•°å€¤ï¼ˆæ•´æ•°ï¼‰
        
        **ğŸ“ˆ å…¥é™¢æ‚£è€…æ•°åˆ—**
        - ãã®æ—¥ã®æ–°å…¥é™¢æ‚£è€…æ•°
        - æ•°å€¤ï¼ˆæ•´æ•°ï¼‰
        
        **ğŸ“‰ é€€é™¢æ‚£è€…æ•°åˆ—**
        - ãã®æ—¥ã®é€€é™¢æ‚£è€…æ•°
        - æ•°å€¤ï¼ˆæ•´æ•°ï¼‰
        """)