import warnings
warnings.filterwarnings('ignore', category=FutureWarning)
import streamlit as st
import pandas as pd
import numpy as np
import time
import os
import tempfile
import gc
import psutil
from integrated_preprocessing import (
    integrated_preprocess_data, calculate_file_hash, efficient_duplicate_check
)
from loader import read_excel_cached, process_uploaded_file
from forecast import generate_filtered_summaries


# --- å®šæ•° ---
EXCEL_USE_COLUMNS = [
    "ç—…æ£Ÿã‚³ãƒ¼ãƒ‰", "è¨ºç™‚ç§‘å", "æ—¥ä»˜", "åœ¨é™¢æ‚£è€…æ•°",
    "å…¥é™¢æ‚£è€…æ•°", "ç·Šæ€¥å…¥é™¢æ‚£è€…æ•°", "é€€é™¢æ‚£è€…æ•°", "æ­»äº¡æ‚£è€…æ•°"
]
EXCEL_DTYPES = {
    "ç—…æ£Ÿã‚³ãƒ¼ãƒ‰": str,
    "è¨ºç™‚ç§‘å": str,
    "åœ¨é™¢æ‚£è€…æ•°": float,
    "å…¥é™¢æ‚£è€…æ•°": float,
    "ç·Šæ€¥å…¥é™¢æ‚£è€…æ•°": float,
    "é€€é™¢æ‚£è€…æ•°": float,
    "æ­»äº¡æ‚£è€…æ•°": float
}

# --- ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›£è¦–é–¢æ•° ---
def log_memory_usage():
    """ç¾åœ¨ã®ãƒ¡ãƒ¢ãƒªä½¿ç”¨çŠ¶æ³ã‚’ãƒ­ã‚°ã«å‡ºåŠ›ã™ã‚‹ï¼ˆæ”¹å–„ç‰ˆï¼‰"""
    try:
        process = psutil.Process()
        mem_info = process.memory_info()
        mem_usage_mb = mem_info.rss / (1024 * 1024)
        mem_percent = process.memory_percent()
        
        # ã‚·ã‚¹ãƒ†ãƒ å…¨ä½“ã®ãƒ¡ãƒ¢ãƒªæƒ…å ±ã‚‚å–å¾—
        system_mem = psutil.virtual_memory()
        system_mem_percent = system_mem.percent
        available_mb = system_mem.available / (1024 * 1024)
        
        return {
            'process_mb': mem_usage_mb,
            'process_percent': mem_percent,
            'system_percent': system_mem_percent,
            'available_mb': available_mb
        }
    except Exception as e:
        print(f"ãƒ¡ãƒ¢ãƒªæƒ…å ±å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
        return None
        
def perform_cleanup(deep=False):
    """ãƒ¡ãƒ¢ãƒªã‚’è§£æ”¾ã™ã‚‹"""
    if deep and 'df' in st.session_state and st.session_state.df is not None:
        if 'filtered_results' in st.session_state and st.session_state.filtered_results != st.session_state.all_results:
            st.session_state.filtered_results = None
        if 'forecast_model_results' in st.session_state:
            st.session_state.forecast_model_results = None
    try:
        temp_dir_root = tempfile.gettempdir()
        # ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³å›ºæœ‰ã®ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æ¨å¥¨
        app_temp_files_pattern = os.path.join(temp_dir_root, "integrated_dashboard_temp_*") 
        import glob
        for temp_file_path in glob.glob(app_temp_files_pattern):
            try:
                if os.path.isfile(temp_file_path):
                    os.unlink(temp_file_path)
                elif os.path.isdir(temp_file_path):
                    import shutil
                    shutil.rmtree(temp_file_path, ignore_errors=True)
            except Exception as e:
                print(f"ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã®å‰Šé™¤ä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")
    except Exception as e:
        print(f"ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")
    gc.collect()
    time.sleep(0.1) # çŸ­ã„å¾…æ©Ÿæ™‚é–“
    gc.collect()

# --- ãƒ‡ãƒ¼ã‚¿å‡¦ç†ã®é–¢æ•° ---
def get_app_data_dir():
    """ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã®ãƒ‡ãƒ¼ã‚¿ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’å–å¾—ã™ã‚‹"""
    base_temp_dir = tempfile.gettempdir()
    app_data_dir = os.path.join(base_temp_dir, "integrated_dashboard_data") 
    if not os.path.exists(app_data_dir):
        try:
            os.makedirs(app_data_dir, exist_ok=True)
        except OSError as e:
            st.error(f"ãƒ‡ãƒ¼ã‚¿ä¿å­˜ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ä½œæˆã«å¤±æ•—ã—ã¾ã—ãŸ: {app_data_dir}\n{e}")
            return None
    return app_data_dir

def get_base_file_info(app_data_dir):
    """ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã•ã‚ŒãŸãƒ™ãƒ¼ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«æƒ…å ±ã‚’å–å¾—ã™ã‚‹"""
    if app_data_dir is None: return None
    info_path = os.path.join(app_data_dir, "base_file_info.json")
    if os.path.exists(info_path):
        try:
            import json
            with open(info_path, 'r') as f:
                return json.load(f)
        except Exception as e:
            print(f"ãƒ™ãƒ¼ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«æƒ…å ±ã®èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
            return None
    return None

def save_base_file_info(app_data_dir, file_name, file_size, file_hash):
    """ãƒ™ãƒ¼ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«æƒ…å ±ã‚’ä¿å­˜ã™ã‚‹"""
    if app_data_dir is None: return
    info_path = os.path.join(app_data_dir, "base_file_info.json")
    info = {
        "file_name": file_name,
        "file_size": file_size,
        "file_hash": file_hash
    }
    try:
        import json
        with open(info_path, 'w') as f:
            json.dump(info, f)
    except Exception as e:
        print(f"ãƒ™ãƒ¼ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«æƒ…å ±ã®ä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")

def debug_target_file_processing(target_data, search_keywords=['å…¨ä½“', 'ç—…é™¢å…¨ä½“', 'ç—…é™¢']):
    """ç›®æ¨™å€¤ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‡ãƒãƒƒã‚°æƒ…å ±ã‚’è¡¨ç¤º"""
    debug_info = {
        'file_loaded': target_data is not None,
        'columns': [],
        'shape': (0, 0),
        'search_results': {},
        'sample_data': None
    }
    
    if target_data is not None:
        debug_info['columns'] = list(target_data.columns)
        debug_info['shape'] = target_data.shape
        debug_info['sample_data'] = target_data.head(3).to_dict('records') if len(target_data) > 0 else []
        
        # å„æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã§ã®çµæœã‚’ç¢ºèª
        for keyword in search_keywords:
            results = []
            for col in target_data.columns:
                if target_data[col].dtype == 'object':  # æ–‡å­—åˆ—åˆ—ã®ã¿ãƒã‚§ãƒƒã‚¯
                    matches = target_data[target_data[col].astype(str).str.contains(keyword, na=False, case=False)]
                    if len(matches) > 0:
                        results.append({
                            'column': col,
                            'matches': len(matches),
                            'sample_values': matches[col].unique()[:3].tolist()
                        })
            debug_info['search_results'][keyword] = results
    
    return debug_info

def extract_targets_from_file(target_data):
    """ç›®æ¨™å€¤ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ç›®æ¨™å€¤ã‚’æŠ½å‡ºã™ã‚‹ï¼ˆæ”¹å–„ç‰ˆï¼‰"""
    if target_data is None or target_data.empty:
        return None, None
    
    debug_info = debug_target_file_processing(target_data)
    
    # ã€Œå…¨ä½“ã€ã€Œç—…é™¢å…¨ä½“ã€ã€Œç—…é™¢ã€ã®ã„ãšã‚Œã‹ã‚’å«ã‚€è¡Œã‚’æ¤œç´¢
    search_conditions = []
    
    # è¤‡æ•°ã®æ¤œç´¢ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’è©¦è¡Œ
    search_patterns = [
        ('éƒ¨é–€ã‚³ãƒ¼ãƒ‰', ['å…¨ä½“', 'ç—…é™¢', 'ç·åˆ']),
        ('éƒ¨é–€å', ['ç—…é™¢å…¨ä½“', 'å…¨ä½“', 'ç—…é™¢', 'ç·åˆ']),
        ('è¨ºç™‚ç§‘å', ['ç—…é™¢å…¨ä½“', 'å…¨ä½“', 'ç—…é™¢', 'ç·åˆ']),
        ('ç§‘å', ['ç—…é™¢å…¨ä½“', 'å…¨ä½“', 'ç—…é™¢', 'ç·åˆ'])
    ]
    
    target_row = None
    used_pattern = None
    
    for col_name, keywords in search_patterns:
        if col_name in target_data.columns:
            for keyword in keywords:
                mask = target_data[col_name].astype(str).str.contains(keyword, na=False, case=False)
                matches = target_data[mask]
                if len(matches) > 0:
                    target_row = matches.iloc[0]  # æœ€åˆã®ä¸€è‡´ã‚’ä½¿ç”¨
                    used_pattern = f"{col_name}='{keyword}'"
                    print(f"ç›®æ¨™å€¤ãƒ‡ãƒ¼ã‚¿æ¤œç´¢æˆåŠŸ: {used_pattern}")
                    break
            if target_row is not None:
                break
    
    if target_row is None:
        print("ç›®æ¨™å€¤ãƒ‡ãƒ¼ã‚¿ã§ã€Œå…¨ä½“ã€ã«ç›¸å½“ã™ã‚‹è¡ŒãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
        print(f"åˆ©ç”¨å¯èƒ½ãªåˆ—: {list(target_data.columns)}")
        print(f"ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿:\n{target_data.head()}")
        return None, debug_info
    
    # ç›®æ¨™å€¤ã‚’æŠ½å‡º
    target_days = None
    target_admissions = None
    
    # å»¶ã¹åœ¨é™¢æ—¥æ•°ç›®æ¨™ã®æŠ½å‡º
    days_columns = ['å»¶ã¹åœ¨é™¢æ—¥æ•°ç›®æ¨™', 'åœ¨é™¢æ—¥æ•°ç›®æ¨™', 'ç›®æ¨™åœ¨é™¢æ—¥æ•°', 'å»¶ã¹åœ¨é™¢æ—¥æ•°', 'åœ¨é™¢æ—¥æ•°']
    for col in days_columns:
        if col in target_data.columns:
            try:
                value = target_row[col]
                if pd.notna(value) and value != '' and str(value).strip() != '':
                    target_days = float(str(value).replace(',', '').replace('äººæ—¥', '').strip())
                    print(f"å»¶ã¹åœ¨é™¢æ—¥æ•°ç›®æ¨™ã‚’å–å¾—: {target_days} (åˆ—: {col})")
                    break
            except (ValueError, TypeError) as e:
                print(f"å»¶ã¹åœ¨é™¢æ—¥æ•°ç›®æ¨™ã®å¤‰æ›ã‚¨ãƒ©ãƒ¼ (åˆ—: {col}): {e}")
                continue
    
    # æ–°å…¥é™¢æ‚£è€…æ•°ç›®æ¨™ã®æŠ½å‡º
    admission_columns = ['æ–°å…¥é™¢æ‚£è€…æ•°ç›®æ¨™', 'å…¥é™¢æ‚£è€…æ•°ç›®æ¨™', 'ç›®æ¨™å…¥é™¢æ‚£è€…æ•°', 'æ–°å…¥é™¢æ‚£è€…æ•°', 'å…¥é™¢æ‚£è€…æ•°']
    for col in admission_columns:
        if col in target_data.columns:
            try:
                value = target_row[col]
                if pd.notna(value) and value != '' and str(value).strip() != '':
                    target_admissions = float(str(value).replace(',', '').replace('äºº', '').strip())
                    print(f"æ–°å…¥é™¢æ‚£è€…æ•°ç›®æ¨™ã‚’å–å¾—: {target_admissions} (åˆ—: {col})")
                    break
            except (ValueError, TypeError) as e:
                print(f"æ–°å…¥é™¢æ‚£è€…æ•°ç›®æ¨™ã®å¤‰æ›ã‚¨ãƒ©ãƒ¼ (åˆ—: {col}): {e}")
                continue
    
    # ä¸€èˆ¬çš„ãªã€Œç›®æ¨™å€¤ã€åˆ—ã‹ã‚‰ã®æŠ½å‡ºã‚‚è©¦è¡Œ
    if (target_days is None or target_admissions is None) and 'ç›®æ¨™å€¤' in target_data.columns:
        try:
            general_target = float(str(target_row['ç›®æ¨™å€¤']).replace(',', '').strip())
            if target_days is None:
                target_days = general_target
                print(f"ä¸€èˆ¬ç›®æ¨™å€¤ã‹ã‚‰å»¶ã¹åœ¨é™¢æ—¥æ•°ç›®æ¨™ã‚’è¨­å®š: {target_days}")
            elif target_admissions is None:
                target_admissions = general_target
                print(f"ä¸€èˆ¬ç›®æ¨™å€¤ã‹ã‚‰æ–°å…¥é™¢æ‚£è€…æ•°ç›®æ¨™ã‚’è¨­å®š: {target_admissions}")
        except (ValueError, TypeError) as e:
            print(f"ä¸€èˆ¬ç›®æ¨™å€¤ã®å¤‰æ›ã‚¨ãƒ©ãƒ¼: {e}")
    
    extracted_targets = {
        'target_days': target_days,
        'target_admissions': target_admissions,
        'used_pattern': used_pattern,
        'source_row': target_row.to_dict() if target_row is not None else None
    }
    
    return extracted_targets, debug_info

def process_data_with_progress(base_file_uploaded, new_files_uploaded, target_file_uploaded, progress_bar):
    """ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ã—ã€å‰å‡¦ç†ã‚’å®Ÿè¡Œã™ã‚‹ï¼ˆé€²æ—è¡¨ç¤ºä»˜ãï¼‰"""
    try: # é–¢æ•°å…¨ä½“ã®ãƒ¡ã‚¤ãƒ³ try ãƒ–ãƒ­ãƒƒã‚¯
        start_time_total = time.time()
        df_list_for_concat = []
        app_data_dir = get_app_data_dir()
        parquet_base_path = os.path.join(app_data_dir, "processed_base_data.parquet") if app_data_dir else None
        st.session_state.performance_metrics = st.session_state.get('performance_metrics', {})
        st.session_state.performance_metrics['data_conversion_time'] = 0

        progress_bar.progress(5, text="1. ãƒ™ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿ã®æº–å‚™ä¸­...")
        load_start_time = time.time()

        current_base_file_hash = None
        if base_file_uploaded:
            base_file_uploaded.seek(0)
            current_base_file_hash = calculate_file_hash(base_file_uploaded.read())
            base_file_uploaded.seek(0)

        saved_base_file_info = get_base_file_info(app_data_dir)
        base_data_loaded_from_parquet = False

        # --- ãƒ™ãƒ¼ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«ã®å‡¦ç† ---
        if parquet_base_path and os.path.exists(parquet_base_path):
            should_load_parquet = False
            if base_file_uploaded: # æ–°ã—ã„ãƒ™ãƒ¼ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«ãŒã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸå ´åˆ
                if saved_base_file_info and saved_base_file_info.get("file_hash") == current_base_file_hash:
                    should_load_parquet = True
                else:
                    print("ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸãƒ™ãƒ¼ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«ãŒå‰å›ã¨ç•°ãªã‚‹ï¼ˆã¾ãŸã¯æƒ…å ±ãªã—ï¼‰ãŸã‚ã€Parquetã‚’å†ä½œæˆã—ã¾ã™ã€‚")
                    if os.path.exists(parquet_base_path): os.remove(parquet_base_path)
                    if os.path.exists(os.path.join(app_data_dir, "base_file_info.json")):
                        os.remove(os.path.join(app_data_dir, "base_file_info.json"))
            else: # ãƒ™ãƒ¼ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«ãŒã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚Œãšã€ParquetãŒå­˜åœ¨ã™ã‚‹å ´åˆ
                should_load_parquet = True

            if should_load_parquet:
                try:
                    print(f"Parquetãƒ™ãƒ¼ã‚¹ãƒ•ã‚¡ã‚¤ãƒ« '{parquet_base_path}' ã‚’èª­ã¿è¾¼ã¿ã¾ã™ã€‚")
                    df_base = pd.read_parquet(parquet_base_path)
                    # ã‚½ãƒ¼ã‚¹è­˜åˆ¥å­ã‚’è¿½åŠ 
                    df_base['ãƒ•ã‚¡ã‚¤ãƒ«ã‚½ãƒ¼ã‚¹'] = 'ãƒ™ãƒ¼ã‚¹'
                    df_list_for_concat.append(df_base)
                    base_data_loaded_from_parquet = True
                    progress_bar.progress(10, text="1. ãƒ™ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿ã‚’Parquetã‹ã‚‰èª­ã¿è¾¼ã¿å®Œäº†ã€‚")
                except Exception as e:
                    st.warning(f"Parquetãƒ™ãƒ¼ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}ã€‚Excelã‹ã‚‰èª­ã¿è¾¼ã¿ã¾ã™ã€‚")
                    if os.path.exists(parquet_base_path): os.remove(parquet_base_path)
                    if os.path.exists(os.path.join(app_data_dir, "base_file_info.json")):
                        os.remove(os.path.join(app_data_dir, "base_file_info.json"))
                    base_data_loaded_from_parquet = False # å¿µã®ãŸã‚ãƒ•ãƒ©ã‚°ã‚’ãƒªã‚»ãƒƒãƒˆ

        if not base_data_loaded_from_parquet and base_file_uploaded:
            print("ãƒ™ãƒ¼ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«ã‚’Excelã‹ã‚‰èª­ã¿è¾¼ã¿ã¾ã™ã€‚")
            base_file_bytes = base_file_uploaded.getvalue()
            df_base_excel = read_excel_cached(base_file_bytes, usecols=EXCEL_USE_COLUMNS, dtype=EXCEL_DTYPES)
            if df_base_excel is not None and not df_base_excel.empty:
                # ã‚½ãƒ¼ã‚¹è­˜åˆ¥å­ã‚’è¿½åŠ 
                df_base_excel['ãƒ•ã‚¡ã‚¤ãƒ«ã‚½ãƒ¼ã‚¹'] = 'ãƒ™ãƒ¼ã‚¹'
                df_list_for_concat.append(df_base_excel)
                progress_bar.progress(10, text="1. ãƒ™ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿ã‚’Excelã‹ã‚‰èª­ã¿è¾¼ã¿å®Œäº†ã€‚")
                if parquet_base_path: # Parquetãƒ‘ã‚¹ãŒæœ‰åŠ¹ãªå ´åˆã®ã¿ä¿å­˜è©¦è¡Œ
                    try:
                        conversion_start_time = time.time()
                        # ã‚½ãƒ¼ã‚¹è­˜åˆ¥å­ã‚’ä¸€æ™‚çš„ã«å‰Šé™¤ã—ã¦ã‹ã‚‰ä¿å­˜ (äº’æ›æ€§ã®ãŸã‚)
                        df_to_save = df_base_excel.drop(columns=['ãƒ•ã‚¡ã‚¤ãƒ«ã‚½ãƒ¼ã‚¹'], errors='ignore')
                        df_to_save.to_parquet(parquet_base_path, index=False)
                        save_base_file_info(app_data_dir,
                                            base_file_uploaded.name,
                                            base_file_uploaded.size,
                                            current_base_file_hash)
                        conversion_end_time = time.time()
                        st.session_state.performance_metrics['data_conversion_time'] = conversion_end_time - conversion_start_time
                        print(f"ãƒ™ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿ã‚’Parquetã¨ã—ã¦ '{parquet_base_path}' ã«ä¿å­˜ã—ã¾ã—ãŸã€‚")
                    except Exception as e:
                        st.warning(f"ãƒ™ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿ã®Parquetä¿å­˜ã«å¤±æ•—: {e}")
            else:
                st.warning("ãƒ™ãƒ¼ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸã€‚")
        
        # --- è¿½åŠ ãƒ•ã‚¡ã‚¤ãƒ«ã®å‡¦ç†ï¼ˆæ”¹å–„ç‰ˆï¼‰ ---
        successful_additional_files = 0
        if new_files_uploaded:
            progress_bar.progress(15, text="1. è¿½åŠ ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿ä¸­...")
            
            for i, file in enumerate(new_files_uploaded):
                try:
                    file_bytes = file.getvalue()
                    df_file = read_excel_cached(file_bytes, usecols=EXCEL_USE_COLUMNS, dtype=EXCEL_DTYPES)
                    
                    if df_file is not None and not df_file.empty:
                        df_file['ãƒ•ã‚¡ã‚¤ãƒ«ã‚½ãƒ¼ã‚¹'] = f'è¿½åŠ _{i}'
                        df_list_for_concat.append(df_file)
                        successful_additional_files += 1
                        print(f"è¿½åŠ ãƒ•ã‚¡ã‚¤ãƒ« '{file.name}' ã®èª­ã¿è¾¼ã¿æˆåŠŸ: {len(df_file)}è¡Œ")
                    else:
                        print(f"è¿½åŠ ãƒ•ã‚¡ã‚¤ãƒ« '{file.name}' ã¯æœŸå¾…ã•ã‚Œã‚‹å½¢å¼ã§ã¯ãªã„ãŸã‚ã€ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã—ãŸã€‚")
                        
                except Exception as e:
                    print(f"è¿½åŠ ãƒ•ã‚¡ã‚¤ãƒ« '{file.name}' ã®èª­ã¿è¾¼ã¿ä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")
                    # ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¦ã‚‚ã‚¹ã‚­ãƒƒãƒ—ã—ã¦ç¶šè¡Œ
                    continue
            
            if successful_additional_files > 0:
                progress_bar.progress(20, text=f"1. è¿½åŠ ãƒ‡ãƒ¼ã‚¿ {successful_additional_files}/{len(new_files_uploaded)} ãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿å®Œäº†ã€‚")
                st.info(f"è¿½åŠ ãƒ•ã‚¡ã‚¤ãƒ«: {successful_additional_files}/{len(new_files_uploaded)} ä»¶ãŒæ­£å¸¸ã«èª­ã¿è¾¼ã¾ã‚Œã¾ã—ãŸã€‚")
            else:
                progress_bar.progress(20, text="1. è¿½åŠ ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿å®Œäº†ï¼ˆåˆ©ç”¨å¯èƒ½ãªãƒ•ã‚¡ã‚¤ãƒ«ãªã—ï¼‰ã€‚")
                if new_files_uploaded:
                    st.warning("è¿½åŠ ãƒ•ã‚¡ã‚¤ãƒ«ã¯ã™ã¹ã¦æœŸå¾…ã•ã‚Œã‚‹å½¢å¼ã¨ç•°ãªã‚‹ãŸã‚ã€ã‚¹ã‚­ãƒƒãƒ—ã•ã‚Œã¾ã—ãŸã€‚")
                    
        load_end_time = time.time()
        st.session_state.performance_metrics['data_load_time'] = load_end_time - load_start_time

        if not df_list_for_concat:
            st.error("èª­ã¿è¾¼ã‚€ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚å›ºå®šãƒ•ã‚¡ã‚¤ãƒ«ã¾ãŸã¯è¿½åŠ ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ã€‚")
            progress_bar.progress(100, text="ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿å¤±æ•—ã€‚")
            return False, None, None, None, None # æˆ»ã‚Šå€¤ã®æ•°ã‚’åˆã‚ã›ã‚‹

        progress_bar.progress(25, text="2. ãƒ‡ãƒ¼ã‚¿ã®çµåˆä¸­...")
        df_raw = pd.concat(df_list_for_concat, ignore_index=True)
        st.write(f"çµåˆå¾Œã®è¡Œæ•°: {len(df_raw)}") # â˜…è¿½åŠ ï¼šçµåˆç›´å¾Œã®è¡Œæ•°
        if not df_raw.empty:
            # ä¾‹: ä¸»è¦ãªã‚­ãƒ¼åˆ— (æ—¥ä»˜, ç—…æ£Ÿã‚³ãƒ¼ãƒ‰, è¨ºç™‚ç§‘åãªã©) ã§é‡è¤‡ã‚’ç¢ºèª
            # key_columns = ['æ—¥ä»˜', 'ç—…æ£Ÿã‚³ãƒ¼ãƒ‰', 'è¨ºç™‚ç§‘å'] # å®Ÿéš›ã®ã‚­ãƒ¼åˆ—ã«åˆã‚ã›ã¦ãã ã•ã„
            # st.write(f"ä¸»è¦ã‚­ãƒ¼ã§ã®é‡è¤‡è¡Œæ•° (çµåˆå¾Œ): {df_raw.duplicated(subset=key_columns).sum()}")
            st.write(f"å®Œå…¨ä¸€è‡´ã§ã®é‡è¤‡è¡Œæ•° (çµåˆå¾Œ): {df_raw.duplicated().sum()}")

        # é€²æ—ãƒãƒ¼ã®æ›´æ–°
        progress_bar.progress(26, text="2. é‡è¤‡ãƒã‚§ãƒƒã‚¯ä¸­...")
        
        # åŠ¹ç‡çš„ãªé‡è¤‡ãƒã‚§ãƒƒã‚¯é–¢æ•°ã‚’å‘¼ã³å‡ºã™
        df_raw = efficient_duplicate_check(df_raw)
        st.write(f"efficient_duplicate_checkå¾Œã®è¡Œæ•°: {len(df_raw)}") # â˜…è¿½åŠ ï¼šé‡è¤‡å‰Šé™¤ç›´å¾Œã®è¡Œæ•°
        if not df_raw.empty:
            # st.write(f"ä¸»è¦ã‚­ãƒ¼ã§ã®é‡è¤‡è¡Œæ•° (efficient_duplicate_checkå¾Œ): {df_raw.duplicated(subset=key_columns).sum()}")
            st.write(f"å®Œå…¨ä¸€è‡´ã§ã®é‡è¤‡è¡Œæ•° (efficient_duplicate_checkå¾Œ): {df_raw.duplicated().sum()}")
        
        # ä¸è¦ãªåˆ—ã®å‰Šé™¤
        if 'ãƒ•ã‚¡ã‚¤ãƒ«ã‚½ãƒ¼ã‚¹' in df_raw.columns:
            df_raw = df_raw.drop(columns=['ãƒ•ã‚¡ã‚¤ãƒ«ã‚½ãƒ¼ã‚¹'], errors='ignore')
        
        del df_list_for_concat # ãƒ¡ãƒ¢ãƒªè§£æ”¾
        gc.collect()

        # --- â˜…ç›®æ¨™å€¤ãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿å‡¦ç†ï¼ˆå¤§å¹…æ”¹å–„ç‰ˆï¼‰--- 
        target_data = None
        target_file_debug_info = None
        extracted_targets = None
        
        if target_file_uploaded:
            progress_bar.progress(28, text="ç›®æ¨™å€¤ãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿ä¸­...") 
            try:
                # CSVãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿ï¼ˆã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°è‡ªå‹•åˆ¤å®šï¼‰
                target_file_uploaded.seek(0)
                file_content = target_file_uploaded.read()
                target_file_uploaded.seek(0)
                
                # ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚’è©¦è¡Œ
                encodings = ['utf-8', 'shift_jis', 'cp932', 'utf-8-sig']
                target_df_temp = None
                
                for encoding in encodings:
                    try:
                        target_df_temp = pd.read_csv(target_file_uploaded, encoding=encoding)
                        print(f"ç›®æ¨™å€¤ãƒ•ã‚¡ã‚¤ãƒ«ã‚’{encoding}ã§èª­ã¿è¾¼ã¿æˆåŠŸ")
                        break
                    except UnicodeDecodeError:
                        target_file_uploaded.seek(0)
                        continue
                    except Exception as e:
                        print(f"ç›®æ¨™å€¤ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼ ({encoding}): {e}")
                        target_file_uploaded.seek(0)
                        continue
                
                if target_df_temp is None or target_df_temp.empty:
                    st.warning("ç›®æ¨™å€¤ãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸã€‚")
                    target_data = None
                else:
                    target_data = target_df_temp
                    print(f"ç›®æ¨™å€¤ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿æˆåŠŸ: {target_data.shape}")
                    
                    # ãƒ‡ãƒãƒƒã‚°æƒ…å ±ã®ç”Ÿæˆã¨ç›®æ¨™å€¤ã®æŠ½å‡º
                    extracted_targets, target_file_debug_info = extract_targets_from_file(target_data)
                    
                    # ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã«ä¿å­˜
                    st.session_state.target_file_debug_info = target_file_debug_info
                    st.session_state.extracted_targets = extracted_targets
                    
                    # è¨ºç™‚ç§‘ãƒãƒƒãƒ”ãƒ³ã‚°ãƒ†ãƒ¼ãƒ–ãƒ«ã‚’ä½œæˆ
                    try:
                        from utils import create_dept_mapping_table
                        create_dept_mapping_table(target_data)
                        print("è¨ºç™‚ç§‘ãƒãƒƒãƒ”ãƒ³ã‚°ãƒ†ãƒ¼ãƒ–ãƒ«ã‚’ä½œæˆã—ã¾ã—ãŸã€‚")
                    except ImportError:
                        print("utils.pyãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚è¨ºç™‚ç§‘ãƒãƒƒãƒ”ãƒ³ã‚°ã¯ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
                    except Exception as e_map:
                        print(f"è¨ºç™‚ç§‘ãƒãƒƒãƒ”ãƒ³ã‚°ãƒ†ãƒ¼ãƒ–ãƒ«ä½œæˆä¸­ã«ã‚¨ãƒ©ãƒ¼: {e_map}")
                    
                    st.success("ç›®æ¨™å€¤ãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿ãŒå®Œäº†ã—ã¾ã—ãŸã€‚")
                    
            except Exception as e:
                st.warning(f"ç›®æ¨™å€¤ãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸ: {str(e)}")
                target_data = None
                print(f"ç›®æ¨™å€¤ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")

        # --- ãƒ‡ãƒ¼ã‚¿ã®å‰å‡¦ç† ---
        progress_bar.progress(30, text="2. ãƒ‡ãƒ¼ã‚¿ã®å‰å‡¦ç†ä¸­...")
        preprocess_start = time.time()
        
        # integrated_preprocess_data ã®å‘¼ã³å‡ºã— (ã“ã“ã§ target_data ã‚’æ¸¡ã™)
        df, validation_results = integrated_preprocess_data(df_raw, target_data_df=target_data)
        del df_raw 
        gc.collect()
        
        if df is None or df.empty:
            progress_bar.progress(100, text="ãƒ‡ãƒ¼ã‚¿å‡¦ç†ã«å¤±æ•—ã—ã¾ã—ãŸã€‚")
            st.error("ãƒ‡ãƒ¼ã‚¿å‡¦ç†ã«å¤±æ•—ã—ã¾ã—ãŸã€‚")
            if validation_results and validation_results.get('errors'): 
                for err in validation_results.get('errors', []): 
                    st.error(err)
            return False, None, None, None, validation_results

        progress_bar.progress(50, text="3. ãƒ‡ãƒ¼ã‚¿ã®æ¤œè¨¼ä¸­...")
        st.session_state.validation_results = validation_results # ã‚»ãƒƒã‚·ãƒ§ãƒ³ã«æ¤œè¨¼çµæœã‚’ä¿å­˜
        if validation_results.get("warnings", []):
            with st.expander("ãƒ‡ãƒ¼ã‚¿æ¤œè¨¼ã®è­¦å‘Š", expanded=True):
                for warning in validation_results["warnings"]:
                    st.warning(warning)
        if not validation_results.get("is_valid", True) and validation_results.get("errors"):
             st.error("ãƒ‡ãƒ¼ã‚¿æ¤œè¨¼ã§è‡´å‘½çš„ãªã‚¨ãƒ©ãƒ¼ãŒæ¤œå‡ºã•ã‚Œã¾ã—ãŸã€‚å‡¦ç†ã‚’ä¸­æ–­ã—ã¾ã™ã€‚")
             return False, None, None, None, validation_results

        progress_bar.progress(90, text="5. å…¨ä½“ãƒ‡ãƒ¼ã‚¿ã®é›†è¨ˆä¸­...")
        all_results = None # åˆæœŸåŒ–
        try:
            # generate_filtered_summaries ãŒã‚¤ãƒ³ãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ã‚Œã°ç›´æ¥å‘¼ã³å‡ºã™
            all_results = generate_filtered_summaries(df, None, None) # 'å…¨ä½“' ã®é›†è¨ˆ
        except NameError: # ã‚¤ãƒ³ãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ãªã‹ã£ãŸå ´åˆ
            st.warning("é›†è¨ˆé–¢æ•° 'generate_filtered_summaries' ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚forecast.pyã‹ã‚‰ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
            all_results = None
        except Exception as e_summary:
            st.warning(f"å…¨ä½“ãƒ‡ãƒ¼ã‚¿ã®é›†è¨ˆä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e_summary}")
            all_results = None

        if all_results is None: # é›†è¨ˆå¤±æ•—ã¾ãŸã¯é–¢æ•°ãŒãªã„å ´åˆ
            st.warning("å…¨ä½“çµæœã¯é™å®šçš„ã«ãªã‚Šã¾ã™ã€‚") 
            all_results = {
                "latest_date": df["æ—¥ä»˜"].max() if not df.empty else pd.Timestamp.now().normalize(),
                "summary": pd.DataFrame(),
                "weekday": pd.DataFrame(),
                "holiday": pd.DataFrame(),
                "monthly_all": pd.DataFrame(),
            }

        preprocess_end = time.time()
        st.session_state.performance_metrics['processing_time'] = preprocess_end - preprocess_start

        latest_date = all_results.get("latest_date", pd.Timestamp.now().normalize())
        end_time_total = time.time()
        total_time = end_time_total - start_time_total

        st.session_state.performance_logs = st.session_state.get('performance_logs', [])
        log_entry = {
            'timestamp': time.strftime("%Y-%m-%d %H:%M:%S"),
            'operation': 'ãƒ‡ãƒ¼ã‚¿å‡¦ç†å…¨ä½“',
            'duration': total_time,
            'details': {
                'rows': len(df) if df is not None else 0,
                'columns': len(df.columns) if df is not None else 0,
                'files_new': len(new_files_uploaded) if new_files_uploaded else 0,
                'base_file_processed_as_parquet': base_data_loaded_from_parquet
            }
        }
        st.session_state.performance_logs.append(log_entry)
        
        progress_bar.progress(100, text=f"ãƒ‡ãƒ¼ã‚¿ã®å‡¦ç†ãŒå®Œäº†ã—ã¾ã—ãŸã€‚å‡¦ç†æ™‚é–“: {total_time:.1f}ç§’")
        return True, df, target_data, all_results, latest_date

    except Exception as e: # ãƒ¡ã‚¤ãƒ³ã®tryãƒ–ãƒ­ãƒƒã‚¯ã«å¯¾ã™ã‚‹except
        progress_bar.progress(100, text=f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}")
        import traceback
        st.error(f"ãƒ‡ãƒ¼ã‚¿å‡¦ç†ã‚¨ãƒ©ãƒ¼: {traceback.format_exc()}")
        return False, None, None, None, None

def create_data_processing_tab():
    """ãƒ‡ãƒ¼ã‚¿å‡¦ç†ã‚¿ãƒ–ã®UIå®Ÿè£…"""
    st.header("ğŸ“Š ãƒ‡ãƒ¼ã‚¿å‡¦ç†")
    
    # ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³æƒ…å ±ã®è¡¨ç¤º
    with st.expander("â„¹ï¸ ãƒ‡ãƒ¼ã‚¿å‡¦ç†ã«ã¤ã„ã¦", expanded=False):
        st.markdown("""
        **ãƒ‡ãƒ¼ã‚¿å‡¦ç†ã®æµã‚Œ:**
        1. **å›ºå®šãƒ•ã‚¡ã‚¤ãƒ«**: ãƒ¡ã‚¤ãƒ³ã¨ãªã‚‹å…¥é™¢æ‚£è€…ãƒ‡ãƒ¼ã‚¿ï¼ˆå¿…é ˆï¼‰
        2. **è¿½åŠ ãƒ•ã‚¡ã‚¤ãƒ«**: è£œå®Œãƒ‡ãƒ¼ã‚¿ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ã€è¤‡æ•°å¯ï¼‰
        3. **ç›®æ¨™å€¤ãƒ•ã‚¡ã‚¤ãƒ«**: éƒ¨é–€åˆ¥ç›®æ¨™è¨­å®šï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
        
        **å¯¾å¿œãƒ•ã‚¡ã‚¤ãƒ«å½¢å¼:**
        - Excel: .xlsx, .xls
        - CSV: .csv (ç›®æ¨™å€¤ãƒ•ã‚¡ã‚¤ãƒ«ã®ã¿)
        
        **å¿…è¦ãªåˆ—å:**
        ç—…æ£Ÿã‚³ãƒ¼ãƒ‰, è¨ºç™‚ç§‘å, æ—¥ä»˜, åœ¨é™¢æ‚£è€…æ•°, å…¥é™¢æ‚£è€…æ•°, ç·Šæ€¥å…¥é™¢æ‚£è€…æ•°, é€€é™¢æ‚£è€…æ•°, æ­»äº¡æ‚£è€…æ•°
        """)
    
    if 'data_processing_initialized' not in st.session_state:
        st.session_state.data_processing_initialized = True
        st.session_state.data_processed = False
        st.session_state.df = None
        st.session_state.target_data = None
        st.session_state.all_results = None
        st.session_state.validation_results = None
        st.session_state.latest_data_date_str = "ãƒ‡ãƒ¼ã‚¿èª­è¾¼å‰"
        st.session_state.target_file_debug_info = None
        st.session_state.extracted_targets = None
        if 'performance_metrics' not in st.session_state:
            st.session_state.performance_metrics = {
                'data_load_time': 0,
                'data_conversion_time': 0,
                'processing_time': 0
            }
    
    # ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã‚»ã‚¯ã‚·ãƒ§ãƒ³ï¼ˆæ”¹å–„ç‰ˆï¼‰
    st.subheader("ğŸ“ ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰")
    
    col1, col2, col3 = st.columns(3)
    with col1: 
        st.markdown("**ğŸ”´ å¿…é ˆãƒ•ã‚¡ã‚¤ãƒ«**")
        base_file_uploader = st.file_uploader(
            "å›ºå®šãƒ•ã‚¡ã‚¤ãƒ« (Excel)", 
            type=["xlsx", "xls"], 
            key="base_file_dp_tab",
            help="ç—…é™¢ã®å…¥é™¢æ‚£è€…ãƒ‡ãƒ¼ã‚¿ã‚’å«ã‚€ãƒ¡ã‚¤ãƒ³ã®Excelãƒ•ã‚¡ã‚¤ãƒ«"
        )
        
    with col2: 
        st.markdown("**ğŸŸ¡ ã‚ªãƒ—ã‚·ãƒ§ãƒ³**")
        new_files_uploader = st.file_uploader(
            "è¿½åŠ ãƒ•ã‚¡ã‚¤ãƒ« (Excel)", 
            type=["xlsx", "xls"], 
            accept_multiple_files=True, 
            key="new_files_dp_tab",
            help="è£œå®Œãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆè¤‡æ•°é¸æŠå¯ï¼‰ã€‚åŒã˜åˆ—æ§‹æˆã§ã‚ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚"
        )
        
    with col3: 
        st.markdown("**ğŸŸ¢ ã‚ªãƒ—ã‚·ãƒ§ãƒ³**")
        target_file_uploader = st.file_uploader(
            "ç›®æ¨™å€¤ãƒ•ã‚¡ã‚¤ãƒ« (CSV)", 
            type=["csv"], 
            key="target_file_dp_tab",
            help="éƒ¨é–€åˆ¥ã®ç›®æ¨™å€¤ãƒ‡ãƒ¼ã‚¿ï¼ˆCSVå½¢å¼ï¼‰"
        )

    # â˜…ç›®æ¨™å€¤ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‡ãƒãƒƒã‚°è¡¨ç¤ºï¼ˆã‚µã‚¤ãƒ‰ãƒãƒ¼ã«ç§»å‹•ï¼‰â˜…
    if st.session_state.get('target_data') is not None:
        with st.sidebar:
            st.markdown("---")
            st.subheader("ğŸ¯ ç›®æ¨™å€¤ãƒ•ã‚¡ã‚¤ãƒ«çŠ¶æ³")
            
            # ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿çŠ¶æ³
            st.success("âœ… ç›®æ¨™å€¤ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿æ¸ˆã¿")
            
            # æŠ½å‡ºã•ã‚ŒãŸç›®æ¨™å€¤ã®è¡¨ç¤º
            extracted_targets = st.session_state.get('extracted_targets')
            if extracted_targets:
                if extracted_targets.get('target_days') or extracted_targets.get('target_admissions'):
                    st.success("âœ… ç›®æ¨™å€¤ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰å–å¾—:")
                    if extracted_targets.get('target_days'):
                        st.write(f"- å»¶ã¹åœ¨é™¢æ—¥æ•°ç›®æ¨™: {extracted_targets['target_days']:,.0f}äººæ—¥")
                    if extracted_targets.get('target_admissions'):
                        st.write(f"- æ–°å…¥é™¢æ‚£è€…æ•°ç›®æ¨™: {extracted_targets['target_admissions']:,.0f}äºº")
                    if extracted_targets.get('used_pattern'):
                        st.info(f"æ¤œç´¢æ¡ä»¶: {extracted_targets['used_pattern']}")
                else:
                    st.warning("âš ï¸ ç›®æ¨™å€¤ã‚’æŠ½å‡ºã§ãã¾ã›ã‚“ã§ã—ãŸ")
            
            # ãƒ‡ãƒãƒƒã‚°æƒ…å ±ã®è¡¨ç¤º
            with st.expander("ğŸ” ç›®æ¨™å€¤ãƒ•ã‚¡ã‚¤ãƒ«å†…å®¹ç¢ºèª", expanded=False):
                target_data = st.session_state.get('target_data')
                if target_data is not None:
                    st.write(f"**ãƒ•ã‚¡ã‚¤ãƒ«æƒ…å ±:** {target_data.shape[0]}è¡Œ Ã— {target_data.shape[1]}åˆ—")
                    st.write("**åˆ—å:**")
                    for i, col in enumerate(target_data.columns):
                        st.write(f"{i+1}. {col}")
                    
                    st.write("**ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿:**")
                    st.dataframe(target_data.head(), use_container_width=True)
                    
                    # æ¤œç´¢çµæœã®è©³ç´°
                    debug_info = st.session_state.get('target_file_debug_info')
                    if debug_info and debug_info.get('search_results'):
                        st.write("**æ¤œç´¢çµæœè©³ç´°:**")
                        for keyword, results in debug_info['search_results'].items():
                            if results:
                                st.write(f"ã€Œ{keyword}ã€ã®æ¤œç´¢çµæœ:")
                                for result in results:
                                    st.write(f"  - {result['column']}: {result['matches']}ä»¶")
                            else:
                                st.write(f"ã€Œ{keyword}ã€: è©²å½“ãªã—")

    # ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰å¾Œã«åˆ—åç¢ºèªæ©Ÿèƒ½ã‚’è¿½åŠ 
    if base_file_uploader:
        with st.expander("ğŸ“‹ å›ºå®šãƒ•ã‚¡ã‚¤ãƒ«åˆ—åç¢ºèª", expanded=False):
            show_excel_column_info(base_file_uploader)
    
    if new_files_uploader:
        st.subheader("ğŸ“‚ è¿½åŠ ãƒ•ã‚¡ã‚¤ãƒ«ã®ç¢ºèª")
        for i, file in enumerate(new_files_uploader):
            with st.expander(f"ğŸ“‹ {file.name} ã®åˆ—åç¢ºèª", expanded=False):
                show_excel_column_info(file)

    app_data_dir_check = get_app_data_dir()
    parquet_base_path_check = os.path.join(app_data_dir_check, "processed_base_data.parquet") if app_data_dir_check else None
    can_process = False
    if base_file_uploader: 
        can_process = True
    elif parquet_base_path_check and os.path.exists(parquet_base_path_check): 
        can_process = True 
    elif new_files_uploader: 
        can_process = True

    if can_process:
        if not st.session_state.get('data_processed', False):
            process_data_button = st.button("ãƒ‡ãƒ¼ã‚¿å‡¦ç†ã‚’å®Ÿè¡Œ", key="process_data_button_dp_tab", use_container_width=True)
            if process_data_button:
                files_to_add = new_files_uploader if new_files_uploader is not None else []
                
                progress_bar_ui = st.progress(0, text="ãƒ‡ãƒ¼ã‚¿å‡¦ç†ã‚’é–‹å§‹ã—ã¾ã™...")
                # process_data_with_progress ã®æˆ»ã‚Šå€¤ã®æ•°ã¨é †ç•ªã‚’åˆã‚ã›ã‚‹
                success, df_result, target_data_result, all_results_result, latest_date_or_val_res = process_data_with_progress(
                    base_file_uploader, files_to_add, target_file_uploader, progress_bar_ui
                )
                
                if success and df_result is not None and not df_result.empty:
                    st.session_state.df = df_result
                    st.session_state.target_data = target_data_result # â˜… target_data ã‚’ã‚»ãƒƒã‚·ãƒ§ãƒ³ã«ä¿å­˜
                    st.session_state.all_results = all_results_result 
                    st.session_state.data_processed = True
                    
                    if isinstance(latest_date_or_val_res, pd.Timestamp):
                        st.session_state.latest_data_date_str = latest_date_or_val_res.strftime("%Yå¹´%mæœˆ%dæ—¥")
                    else: 
                        st.session_state.latest_data_date_str = "ãƒ‡ãƒ¼ã‚¿å‡¦ç†å®Œäº† (æ—¥ä»˜ä¸æ˜)"
                        # latest_date_or_val_res ãŒ validation_results ã®å ´åˆã®ã‚¨ãƒ©ãƒ¼è¡¨ç¤º
                        if isinstance(latest_date_or_val_res, dict) and latest_date_or_val_res.get('errors'):
                            for err in latest_date_or_val_res.get('errors', []):
                                st.error(err)
                        
                    st.success(f"ãƒ‡ãƒ¼ã‚¿ã®å‡¦ç†ãŒå®Œäº†ã—ã¾ã—ãŸã€‚æœ€æ–°ãƒ‡ãƒ¼ã‚¿æ—¥ä»˜: {st.session_state.latest_data_date_str}")
                    perform_cleanup(deep=True)
                    st.rerun()
                else: 
                    # å¤±æ•—æ™‚ã€latest_date_or_val_res ã« validation_results ãŒå…¥ã£ã¦ã„ã‚‹å¯èƒ½æ€§ãŒã‚ã‚‹
                    if isinstance(latest_date_or_val_res, dict) and latest_date_or_val_res.get('errors'):
                        for err in latest_date_or_val_res.get('errors', []):
                            st.error(err)
                    # validation_results ãŒ st.session_state ã«ã‚‚ä¿å­˜ã•ã‚Œã¦ã„ã‚‹ã®ã§ã€ãã¡ã‚‰ã‚‚å‚ç…§å¯èƒ½
                    elif st.session_state.get('validation_results') and st.session_state.validation_results.get('errors'):
                         for err in st.session_state.validation_results.get('errors', []):
                            st.error(err)
                    else:
                        st.error("ãƒ‡ãƒ¼ã‚¿å‡¦ç†ã«å¤±æ•—ã—ã¾ã—ãŸã€‚è©³ç´°ã¯ãƒ­ã‚°ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
        else: 
            st.success(f"ãƒ‡ãƒ¼ã‚¿å‡¦ç†æ¸ˆã¿ï¼ˆæœ€æ–°ãƒ‡ãƒ¼ã‚¿æ—¥ä»˜: {st.session_state.latest_data_date_str}ï¼‰")
            if st.session_state.get('target_data') is not None: 
                st.success("ç›®æ¨™å€¤ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿æ¸ˆã¿")
            else: 
                st.info("ç›®æ¨™å€¤ãƒ‡ãƒ¼ã‚¿ã¯æœªèª­ã¿è¾¼ã¿ã§ã™")
            
            if st.session_state.get('df') is not None:
                df_display = st.session_state.df
                with st.expander("ãƒ‡ãƒ¼ã‚¿æ¦‚è¦", expanded=True):
                    col1_sum, col2_sum, col3_sum = st.columns(3)
                    with col1_sum: 
                        if not df_display.empty and 'æ—¥ä»˜' in df_display.columns:
                            st.metric("ãƒ‡ãƒ¼ã‚¿æœŸé–“", f"{df_display['æ—¥ä»˜'].min().strftime('%Y/%m/%d')} - {df_display['æ—¥ä»˜'].max().strftime('%Y/%m/%d')}")
                        else:
                            st.metric("ãƒ‡ãƒ¼ã‚¿æœŸé–“", "N/A")
                    with col2_sum: 
                        st.metric("ç·ãƒ¬ã‚³ãƒ¼ãƒ‰æ•°", f"{len(df_display):,}")
                    with col3_sum: 
                        st.metric("ç—…æ£Ÿæ•°", f"{df_display['ç—…æ£Ÿã‚³ãƒ¼ãƒ‰'].nunique() if 'ç—…æ£Ÿã‚³ãƒ¼ãƒ‰' in df_display.columns else 'N/A'}")
                        
                    col1_sum2, col2_sum2, col3_sum2 = st.columns(3)
                    with col1_sum2: 
                        st.metric("è¨ºç™‚ç§‘æ•°", f"{df_display['è¨ºç™‚ç§‘å'].nunique() if 'è¨ºç™‚ç§‘å' in df_display.columns else 'N/A'}")
                    with col2_sum2: 
                        if "å¹³æ—¥åˆ¤å®š" in df_display.columns:
                            st.metric("å¹³æ—¥æ•°", f"{(df_display['å¹³æ—¥åˆ¤å®š'] == 'å¹³æ—¥').sum()}")
                        else:
                            st.metric("å¹³æ—¥æ•°", "N/A")
                    with col3_sum2: 
                        if "å¹³æ—¥åˆ¤å®š" in df_display.columns:
                            st.metric("ä¼‘æ—¥æ•°", f"{(df_display['å¹³æ—¥åˆ¤å®š'] == 'ä¼‘æ—¥').sum()}")
                        else:
                            st.metric("ä¼‘æ—¥æ•°", "N/A")
                           
                    perf_metrics = st.session_state.get('performance_metrics', {})
                    if perf_metrics:
                        st.subheader("å‡¦ç†ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹")
                        pcol1, pcol2, pcol3, pcol4 = st.columns(4)
                        with pcol1: 
                            st.metric("ãƒ‡ãƒ¼ã‚¿èª­è¾¼æ™‚é–“", f"{perf_metrics.get('data_load_time', 0):.1f}ç§’")
                        with pcol2: 
                            st.metric("Parquetå¤‰æ›æ™‚é–“", f"{perf_metrics.get('data_conversion_time', 0):.1f}ç§’")
                        with pcol3: 
                            st.metric("ãƒ‡ãƒ¼ã‚¿å‡¦ç†æ™‚é–“", f"{perf_metrics.get('processing_time', 0):.1f}ç§’")
                        with pcol4:
                            try: 
                                mem_info = log_memory_usage()
                                if mem_info is not None:
                                    # è¾æ›¸å½¢å¼ã®æˆ»ã‚Šå€¤ã«å¯¾å¿œ
                                    if isinstance(mem_info, dict):
                                        process_mb = mem_info.get('process_mb', 0)
                                        process_percent = mem_info.get('process_percent', 0)
                                        st.metric("ç¾åœ¨ã®ãƒ¡ãƒ¢ãƒªä½¿ç”¨", f"{process_mb:.1f} MB ({process_percent:.1f}%)")
                                    else:
                                        # å¾“æ¥ã®æˆ»ã‚Šå€¤å½¢å¼ï¼ˆã‚¿ãƒ—ãƒ«ï¼‰ã«ã‚‚å¯¾å¿œ
                                        mem_usage, mem_percent = mem_info
                                        st.metric("ç¾åœ¨ã®ãƒ¡ãƒ¢ãƒªä½¿ç”¨", f"{mem_usage:.1f} MB ({mem_percent:.1f}%)")
                                else:
                                    st.metric("ãƒ¡ãƒ¢ãƒªæƒ…å ±", "å–å¾—ä¸å¯")
                            except Exception as e:
                                print(f"ãƒ¡ãƒ¢ãƒªè¡¨ç¤ºã‚¨ãƒ©ãƒ¼: {e}")
                                st.metric("ãƒ¡ãƒ¢ãƒªæƒ…å ±", "å–å¾—ä¸å¯")
                                
                validation_res = st.session_state.get('validation_results')
                if validation_res:
                    if validation_res.get("warnings") or validation_res.get("info") or validation_res.get("errors"):
                        with st.expander("ãƒ‡ãƒ¼ã‚¿æ¤œè¨¼çµæœ", expanded=False):
                            for err in validation_res.get("errors", []):
                                st.error(err)
                            for info_msg in validation_res.get("info", []):
                                st.info(info_msg)
                            for warning_msg in validation_res.get("warnings", []):
                                st.warning(warning_msg)
                                
                # ãƒãƒƒãƒ”ãƒ³ã‚°ç®¡ç†æ©Ÿèƒ½ã‚’è¿½åŠ ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
                if st.session_state.get('data_processed', False) and st.session_state.get('target_data') is not None:
                    with st.expander("è¨ºç™‚ç§‘ãƒãƒƒãƒ”ãƒ³ã‚°è¨­å®š", expanded=False):
                        st.write("å®Ÿç¸¾ãƒ‡ãƒ¼ã‚¿ã®è¨ºç™‚ç§‘ã‚³ãƒ¼ãƒ‰ã¨è¡¨ç¤ºåã®ãƒãƒƒãƒ”ãƒ³ã‚°ã‚’ç¢ºèªã—ã¾ã™ã€‚")
                        
                        # ãƒãƒƒãƒ”ãƒ³ã‚°é–¢æ•°ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆã‚’è©¦ã¿ã‚‹
                        try:
                            from utils import get_display_name_for_dept, create_dept_mapping_table
                            
                            # ç¾åœ¨ã®ãƒãƒƒãƒ”ãƒ³ã‚°ãƒ†ãƒ¼ãƒ–ãƒ«ã‚’å–å¾—
                            dept_mapping = st.session_state.get('dept_mapping', {})
                            
                            if not dept_mapping:
                                # ãƒãƒƒãƒ”ãƒ³ã‚°ãŒãªã‘ã‚Œã°ä½œæˆ
                                create_dept_mapping_table(st.session_state.target_data)
                                dept_mapping = st.session_state.get('dept_mapping', {})
                            
                            if dept_mapping:
                                # ãƒãƒƒãƒ”ãƒ³ã‚°ãƒ†ãƒ¼ãƒ–ãƒ«ã‚’è¡¨ç¤º
                                df_mapping = pd.DataFrame({
                                    'è¨ºç™‚ç§‘ã‚³ãƒ¼ãƒ‰': list(dept_mapping.keys()),
                                    'è¡¨ç¤ºå': list(dept_mapping.values())
                                })
                                
                                st.dataframe(df_mapping)
                                
                                # ç‰¹å®šã®è¨ºç™‚ç§‘ã®ãƒãƒƒãƒ”ãƒ³ã‚°ã‚’ãƒã‚§ãƒƒã‚¯
                                problematic_depts = ["ç·åˆå†…ç§‘ï¼ˆå†…ç§‘é™¤ãï¼‰", "å†…ç§‘æ•‘æ€¥"]
                                st.subheader("ç¢ºèªå¯¾è±¡ã®è¨ºç™‚ç§‘")
                                for dept in problematic_depts:
                                    display_name = get_display_name_for_dept(dept, "ãƒãƒƒãƒ”ãƒ³ã‚°ãªã—")
                                    if display_name == "ãƒãƒƒãƒ”ãƒ³ã‚°ãªã—":
                                        st.warning(f"ã€Œ{dept}ã€â†’ãƒãƒƒãƒ”ãƒ³ã‚°ãŒã‚ã‚Šã¾ã›ã‚“")
                                    else:
                                        st.success(f"ã€Œ{dept}ã€â†’ã€Œ{display_name}ã€")
                            else:
                                st.warning("è¨ºç™‚ç§‘ãƒãƒƒãƒ”ãƒ³ã‚°ãŒä½œæˆã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚ç›®æ¨™å€¤ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
                        except ImportError:
                            st.warning("è¨ºç™‚ç§‘ãƒãƒƒãƒ”ãƒ³ã‚°æ©Ÿèƒ½ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚")
                        except Exception as e:
                            st.error(f"ãƒãƒƒãƒ”ãƒ³ã‚°è¡¨ç¤ºã‚¨ãƒ©ãƒ¼: {str(e)}")

            if st.button("ãƒ‡ãƒ¼ã‚¿ã‚’ãƒªã‚»ãƒƒãƒˆ (Parquetãƒ™ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿ã‚‚å‰Šé™¤)", key="reset_data_button_dp_tab", use_container_width=True):
                st.session_state.data_processed = False
                st.session_state.df = None
                st.session_state.all_results = None
                st.session_state.target_data = None
                st.session_state.validation_results = None
                st.session_state.latest_data_date_str = "ãƒ‡ãƒ¼ã‚¿èª­è¾¼å‰"
                st.session_state.target_file_debug_info = None
                st.session_state.extracted_targets = None
                st.session_state.performance_metrics = {
                    'data_load_time': 0, 
                    'data_conversion_time': 0, 
                    'processing_time': 0
                }
               
                if app_data_dir_check: 
                    parquet_to_delete = os.path.join(app_data_dir_check, "processed_base_data.parquet")
                    info_to_delete = os.path.join(app_data_dir_check, "base_file_info.json")
                    if os.path.exists(parquet_to_delete): 
                        try: os.remove(parquet_to_delete); st.info(f"ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ã‚’å‰Šé™¤ã—ã¾ã—ãŸã€‚")
                        except Exception as e_del: st.warning(f"Parquetå‰Šé™¤ã‚¨ãƒ©ãƒ¼: {e_del}")
                    if os.path.exists(info_to_delete):
                        try: os.remove(info_to_delete)
                        except Exception as e_del: st.warning(f"Infoãƒ•ã‚¡ã‚¤ãƒ«å‰Šé™¤ã‚¨ãƒ©ãƒ¼: {e_del}")
                       
                perform_cleanup(deep=True)
                st.rerun()
    else:
        st.info("ã€Œå›ºå®šãƒ•ã‚¡ã‚¤ãƒ«ã€ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã™ã‚‹ã‹ã€ä»¥å‰å‡¦ç†ã—ãŸãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Œã°ã€Œè¿½åŠ ãƒ•ã‚¡ã‚¤ãƒ«ã€ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ã€‚")
       
        st.markdown("#### ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿")
        st.markdown("ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨ã—ã¦æ©Ÿèƒ½ã‚’è©¦ã™ã“ã¨ãŒã§ãã¾ã™ã€‚")
        if st.button("ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨", key="sample_data_button_dp_tab"):
            st.info("ã“ã®æ©Ÿèƒ½ã¯é–‹ç™ºä¸­ã§ã™ã€‚ç¾åœ¨ã¯è‡ªåˆ†ã®ãƒ‡ãƒ¼ã‚¿ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ã€‚")

def show_excel_column_info(uploaded_file):
    """
    ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸExcelãƒ•ã‚¡ã‚¤ãƒ«ã®åˆ—åæƒ…å ±ã‚’è¡¨ç¤ºï¼ˆæ”¹å–„ç‰ˆï¼‰
    """
    try:
        # ãƒ•ã‚¡ã‚¤ãƒ«ã®å…ˆé ­ã‚’èª­ã¿è¾¼ã‚“ã§åˆ—åã‚’ç¢ºèª
        uploaded_file.seek(0)  # ãƒ•ã‚¡ã‚¤ãƒ«ãƒã‚¤ãƒ³ã‚¿ã‚’ãƒªã‚»ãƒƒãƒˆ
        file_bytes = uploaded_file.getvalue()
        uploaded_file.seek(0)  # å†åº¦ãƒªã‚»ãƒƒãƒˆ
        
        if len(file_bytes) == 0:
            st.warning("ãƒ•ã‚¡ã‚¤ãƒ«ãŒç©ºã§ã™ã€‚")
            return
        
        with tempfile.NamedTemporaryFile(delete=False, suffix='.xlsx') as temp_file:
            temp_file.write(file_bytes)
            temp_path = temp_file.name
        
        try:
            # è¤‡æ•°ã®ã‚¨ãƒ³ã‚¸ãƒ³ã‚’è©¦è¡Œ
            df_sample = None
            engines = ['openpyxl', 'xlrd']  # openpyxlã‚’æœ€åˆã«è©¦è¡Œ
            
            for engine in engines:
                try:
                    df_sample = pd.read_excel(temp_path, nrows=3, engine=engine)
                    break
                except Exception as e:
                    if engine == engines[-1]:  # æœ€å¾Œã®ã‚¨ãƒ³ã‚¸ãƒ³ã§ã‚‚å¤±æ•—
                        raise e
                    continue
            
            if df_sample is None or df_sample.empty:
                st.warning("ãƒ•ã‚¡ã‚¤ãƒ«ã®å†…å®¹ã‚’èª­ã¿å–ã‚Œã¾ã›ã‚“ã§ã—ãŸã€‚")
                return

            # --- ã“ã“ã‹ã‚‰è¿½åŠ /ä¿®æ­£ ---
            # è¡¨ç¤ºç”¨ã®df_sample_display ã‚’ä½œæˆã—ã€ã‚¯ãƒ¬ãƒ³ã‚¸ãƒ³ã‚°ã‚’è¡Œã†
            df_sample_display = df_sample.copy()

            # ã‚¯ãƒ¬ãƒ³ã‚¸ãƒ³ã‚°å¯¾è±¡ã®åˆ—ã‚’æŒ‡å®š (ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã«å‡ºã¦ã„ã‚‹åˆ—ã¯å¿…é ˆ)
            # EXCEL_DTYPES ã§ float ãŒæŒ‡å®šã•ã‚Œã¦ã„ã‚‹åˆ—ã‚‚å¯¾è±¡ã«å«ã‚ã‚‹ã¨ã‚ˆã‚Šå …ç‰¢
            cols_to_clean_in_sample = []
            potential_numeric_cols = ["åœ¨é™¢æ‚£è€…æ•°", "å…¥é™¢æ‚£è€…æ•°", "ç·Šæ€¥å…¥é™¢æ‚£è€…æ•°", "é€€é™¢æ‚£è€…æ•°", "æ­»äº¡æ‚£è€…æ•°", "å…¥é™¢æ‚£è€…æ•°ï¼ˆåœ¨é™¢ï¼‰"]
            for col_name in potential_numeric_cols:
                if col_name in df_sample_display.columns:
                    cols_to_clean_in_sample.append(col_name)

            for col in cols_to_clean_in_sample:
                # objectå‹ã§ãªã„ã¨ .str ã‚¢ã‚¯ã‚»ã‚µãŒä½¿ãˆãªã„å ´åˆãŒã‚ã‚‹ã®ã§ã€å…ˆã«æ–‡å­—åˆ—ã«å¤‰æ›ã™ã‚‹ã“ã¨ã‚’æ¤œè¨
                # ãŸã ã—ã€å…ƒã€…æ•°å€¤å‹ã§ã‚ã‚‹åˆ—ã‚’æ–‡å­—åˆ—ã«ã™ã‚‹ã¨ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã«å½±éŸ¿ã™ã‚‹å¯èƒ½æ€§ã‚‚è€ƒæ…®
                if df_sample_display[col].dtype == 'object':
                    # ãƒã‚¤ãƒ•ãƒ³ã‚„ãã®ä»–ã®éæ•°å€¤çš„ãªè¡¨ç¾ã‚’ np.nan ã«ç½®æ›
                    df_sample_display[col] = df_sample_display[col].replace(['-', 'ï¼', ' ', 'ã€€', 'ãªã—', 'NA', 'N/A', 'NULL'], np.nan, regex=False)
                # pd.to_numeric ã§æ•°å€¤ã«å¤‰æ›ã—ã€å¤‰æ›ã§ããªã„ã‚‚ã®ã¯ NaN ã«ã™ã‚‹
                df_sample_display[col] = pd.to_numeric(df_sample_display[col], errors='coerce')
                # è¡¨ç¤ºä¸Šã€NaNã®ã¾ã¾ã§ã‚‚è‰¯ã„ã—ã€0ã§åŸ‹ã‚ã¦ã‚‚è‰¯ã„ (ã“ã“ã§ã¯NaNã®ã¾ã¾ã«ã—ã¦ãŠã)
                # å¿…è¦ã§ã‚ã‚Œã° .fillna(0) ã‚’è¿½åŠ 

            # --- ã“ã“ã¾ã§è¿½åŠ /ä¿®æ­£ ---

            st.subheader(f"ğŸ“‹ {uploaded_file.name} ã®åˆ—åç¢ºèª")

            # ãƒ•ã‚¡ã‚¤ãƒ«åŸºæœ¬æƒ…å ±
            file_size_mb = len(file_bytes) / (1024 * 1024)
            # ã‚¯ãƒ¬ãƒ³ã‚¸ãƒ³ã‚°å¾Œã®df_sample_displayã®æƒ…å ±ã‚’è¡¨ç¤º
            st.info(f"ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º: {file_size_mb:.2f} MB | è¡Œæ•°ï¼ˆã‚µãƒ³ãƒ—ãƒ«ï¼‰: {len(df_sample_display)} | åˆ—æ•°: {len(df_sample_display.columns)}")

            col1, col2 = st.columns(2)

            with col1:
                st.write("**æ¤œå‡ºã•ã‚ŒãŸåˆ—å:**")
                # ã‚¯ãƒ¬ãƒ³ã‚¸ãƒ³ã‚°å¾Œã®df_sample_displayã®åˆ—æƒ…å ±ã‚’è¡¨ç¤º
                for i, col in enumerate(df_sample_display.columns):
                    # åˆ—ã®ãƒ‡ãƒ¼ã‚¿å‹ã‚‚è¡¨ç¤º
                    dtype_str = str(df_sample_display[col].dtype)
                    st.write(f"{i+1}. {col} ({dtype_str})")

            with col2:
                st.write("**æœŸå¾…ã•ã‚Œã‚‹åˆ—å:**")
                expected = [
                    "ç—…æ£Ÿã‚³ãƒ¼ãƒ‰", "è¨ºç™‚ç§‘å", "æ—¥ä»˜", "åœ¨é™¢æ‚£è€…æ•°",
                    "å…¥é™¢æ‚£è€…æ•°", "ç·Šæ€¥å…¥é™¢æ‚£è€…æ•°", "é€€é™¢æ‚£è€…æ•°", "æ­»äº¡æ‚£è€…æ•°"
                ]

                matched_columns = []
                # ã‚¯ãƒ¬ãƒ³ã‚¸ãƒ³ã‚°å¾Œã®df_sample_displayã®åˆ—ã§æ¯”è¼ƒ
                for col_expected in expected:
                    if col_expected in df_sample_display.columns:
                        st.write(f"âœ… {col_expected}")
                        matched_columns.append(col_expected)
                    else:
                        # é¡ä¼¼ã®åˆ—åã‚’æ¤œç´¢
                        similar_cols = [c for c in df_sample_display.columns if col_expected.replace('æ‚£è€…æ•°', '') in c or col_expected.replace('æ•°', '') in c]
                        if similar_cols:
                            st.write(f"â“ {col_expected} (é¡ä¼¼: {similar_cols[0]})")
                        else:
                            st.write(f"âŒ {col_expected}")

            # åˆ©ç”¨å¯èƒ½æ€§ã®åˆ¤å®šï¼ˆæ”¹å–„ç‰ˆï¼‰
            available_count = len(matched_columns)
            essential_columns = ["ç—…æ£Ÿã‚³ãƒ¼ãƒ‰", "è¨ºç™‚ç§‘å", "æ—¥ä»˜"]  # å¿…é ˆåˆ—
            essential_count = sum(1 for col_essential in essential_columns if col_essential in matched_columns)

            if essential_count >= 2 and available_count >= 4:
                st.success(f"âœ… ã“ã®ãƒ•ã‚¡ã‚¤ãƒ«ã¯å®Œå…¨ã«åˆ©ç”¨å¯èƒ½ã§ã™ï¼ˆ{available_count}/{len(expected)}åˆ—ãŒä¸€è‡´ï¼‰")
            elif essential_count >= 2:
                st.warning(f"âš ï¸ ã“ã®ãƒ•ã‚¡ã‚¤ãƒ«ã¯éƒ¨åˆ†çš„ã«åˆ©ç”¨å¯èƒ½ã§ã™ï¼ˆ{available_count}/{len(expected)}åˆ—ãŒä¸€è‡´ã€å¿…é ˆåˆ—:{essential_count}/3ï¼‰")
                st.info("ä¸è¶³ã—ã¦ã„ã‚‹åˆ—ã¯è‡ªå‹•çš„ã«0ã§è£œå®Œã•ã‚Œã‚‹ã‹ã€å‡¦ç†ãŒã‚¹ã‚­ãƒƒãƒ—ã•ã‚Œã¾ã™ã€‚")
            else:
                st.error(f"âŒ ã“ã®ãƒ•ã‚¡ã‚¤ãƒ«ã¯åˆ©ç”¨ã§ãã¾ã›ã‚“ï¼ˆå¿…é ˆåˆ—ãŒä¸è¶³: {essential_count}/3ï¼‰")
                st.info("ã“ã®ãƒ•ã‚¡ã‚¤ãƒ«ã¯å‡¦ç†ã‚’ã‚¹ã‚­ãƒƒãƒ—ã•ã‚Œã¾ã™ã€‚")

            # ãƒ‡ãƒ¼ã‚¿å“è³ªãƒã‚§ãƒƒã‚¯
            st.write("**ãƒ‡ãƒ¼ã‚¿å“è³ªãƒã‚§ãƒƒã‚¯:**")
            quality_issues = []

            # ã‚¯ãƒ¬ãƒ³ã‚¸ãƒ³ã‚°å¾Œã®df_sample_displayã§ãƒã‚§ãƒƒã‚¯
            for col_match in matched_columns:
                # pd.to_numeric ã§æ•°å€¤åŒ–ã•ã‚ŒãŸåˆ—ã¯ .isnull() ã§æ¬ æã‚’ç¢ºèªã§ãã‚‹
                null_count = df_sample_display[col_match].isnull().sum()
                if null_count > 0:
                    quality_issues.append(f"'{col_match}': {null_count}å€‹ã®æ¬ æå€¤ï¼ˆæ•°å€¤å¤‰æ›å¾Œï¼‰")

            if quality_issues:
                st.warning("å“è³ªä¸Šã®å•é¡Œ:")
                for issue in quality_issues:
                    st.write(f"  â€¢ {issue}")
            else:
                st.success("ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã«å“è³ªä¸Šã®å•é¡Œã¯è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")

            # ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã®è¡¨ç¤ºï¼ˆæ”¹å–„ç‰ˆï¼‰
            st.markdown("---")
            st.write("**ğŸ“Š ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ï¼ˆæœ€åˆã®3è¡Œï¼‰:**")

            # è©³ç´°è¡¨ç¤ºã®åˆ‡ã‚Šæ›¿ãˆ
            show_details = st.checkbox(f"è©³ç´°ãƒ‡ãƒ¼ã‚¿ã‚’è¡¨ç¤º - {uploaded_file.name}", key=f"show_details_{uploaded_file.name}")

            if show_details:
                st.dataframe(df_sample_display, use_container_width=True) # ã‚¯ãƒ¬ãƒ³ã‚¸ãƒ³ã‚°æ¸ˆã¿ã®DataFrameã‚’ä½¿ç”¨

                # çµ±è¨ˆæƒ…å ±ï¼ˆæ•°å€¤åˆ—ã®ã¿ï¼‰
                # ã‚¯ãƒ¬ãƒ³ã‚¸ãƒ³ã‚°å¾Œã®df_sample_displayã‹ã‚‰æ•°å€¤åˆ—ã‚’é¸æŠ
                numeric_cols_display = df_sample_display.select_dtypes(include=[np.number]).columns
                if len(numeric_cols_display) > 0:
                    st.write("**æ•°å€¤åˆ—ã®åŸºæœ¬çµ±è¨ˆ:**")
                    st.dataframe(df_sample_display[numeric_cols_display].describe(), use_container_width=True) # ã‚¯ãƒ¬ãƒ³ã‚¸ãƒ³ã‚°æ¸ˆã¿ã®DataFrameã‚’ä½¿ç”¨
            else:
                # ç°¡æ˜“è¡¨ç¤º
                st.dataframe(df_sample_display.head(2), use_container_width=True) # ã‚¯ãƒ¬ãƒ³ã‚¸ãƒ³ã‚°æ¸ˆã¿ã®DataFrameã‚’ä½¿ç”¨
            
        finally:
            if os.path.exists(temp_path):
                os.unlink(temp_path)
            
    except Exception as e:
        st.error(f"åˆ—åç¢ºèªã‚¨ãƒ©ãƒ¼: {e}")
        
        st.markdown("---")
        st.write("**ğŸ”§ è©³ç´°ã‚¨ãƒ©ãƒ¼æƒ…å ±:**")
        
        # è©³ç´°ã‚¨ãƒ©ãƒ¼è¡¨ç¤ºã®åˆ‡ã‚Šæ›¿ãˆ
        show_error_details = st.checkbox(f"ã‚¨ãƒ©ãƒ¼è©³ç´°ã‚’è¡¨ç¤º - {uploaded_file.name}", key=f"show_error_{uploaded_file.name}")
        
        if show_error_details:
            import traceback
            st.code(traceback.format_exc())