# data_processing_tab.py - æ°¸ç¶šåŒ–å¯¾å¿œç‰ˆ

import streamlit as st
import pandas as pd
from datetime import datetime
import os

# æ°¸ç¶šåŒ–æ©Ÿèƒ½ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from persistent_data import (
    auto_load_persistent_data,
    save_persistent_data, 
    get_persistent_data_info,
    clear_persistent_data,
    restore_from_backup,
    export_data_info
)

# æ—¢å­˜æ©Ÿèƒ½ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from integrated_preprocessing import integrated_preprocess_data
from loader import load_files

def create_data_processing_tab():
    """ãƒ‡ãƒ¼ã‚¿å‡¦ç†ã‚¿ãƒ–ï¼ˆæ°¸ç¶šåŒ–å¯¾å¿œç‰ˆï¼‰"""
    
    st.header("ğŸ“Š ãƒ‡ãƒ¼ã‚¿å‡¦ç†")
    
    # ===== ã‚¢ãƒ—ãƒªèµ·å‹•æ™‚ã®è‡ªå‹•ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ =====
    if not st.session_state.get('auto_load_attempted', False):
        with st.spinner("ä¿å­˜ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ã‚’ç¢ºèªä¸­..."):
            if auto_load_persistent_data():
                st.success("ğŸ’¾ å‰å›ã®ãƒ‡ãƒ¼ã‚¿ã‚’è‡ªå‹•èª­ã¿è¾¼ã¿ã—ã¾ã—ãŸï¼")
                st.balloons()
            st.session_state['auto_load_attempted'] = True
            st.rerun()
    
    # ===== ãƒ‡ãƒ¼ã‚¿çŠ¶æ³ã®è¡¨ç¤º =====
    display_data_status()
    
    # ===== ãƒ‡ãƒ¼ã‚¿å‡¦ç†ã‚»ã‚¯ã‚·ãƒ§ãƒ³ =====
    if st.session_state.get('data_processed', False):
        # ãƒ‡ãƒ¼ã‚¿ãŒæ—¢ã«å­˜åœ¨ã™ã‚‹å ´åˆ
        display_data_update_section()
    else:
        # ãƒ‡ãƒ¼ã‚¿ãŒå­˜åœ¨ã—ãªã„å ´åˆ
        display_initial_upload_section()
    
    # ===== ãƒ‡ãƒ¼ã‚¿ç®¡ç†ã‚»ã‚¯ã‚·ãƒ§ãƒ³ =====
    display_data_management_section()

def display_data_status():
    """ãƒ‡ãƒ¼ã‚¿çŠ¶æ³ã®è¡¨ç¤º"""
    st.markdown("### ğŸ“Š ç¾åœ¨ã®ãƒ‡ãƒ¼ã‚¿çŠ¶æ³")
    
    info = get_persistent_data_info()
    
    if info.get('exists') and st.session_state.get('data_processed', False):
        # ãƒ‡ãƒ¼ã‚¿å­˜åœ¨æ™‚ã®è¡¨ç¤º
        col1, col2, col3 = st.columns(3)
        
        with col1:
            st.metric(
                "ãƒ‡ãƒ¼ã‚¿çŠ¶æ³",
                "âœ… èª­ã¿è¾¼ã¿æ¸ˆã¿",
                delta="ç¶™ç¶šåˆ©ç”¨å¯èƒ½"
            )
        
        with col2:
            record_count = info.get('record_count', 0)
            st.metric(
                "ãƒ‡ãƒ¼ã‚¿ä»¶æ•°", 
                f"{record_count:,}ä»¶",
                delta=info.get('date_range', 'N/A')
            )
        
        with col3:
            if 'save_timestamp' in info:
                last_update = info['save_timestamp']
                if isinstance(last_update, datetime):
                    update_str = last_update.strftime('%m/%d %H:%M')
                    days_ago = (datetime.now() - last_update).days
                    delta_str = f"{days_ago}æ—¥å‰" if days_ago > 0 else "ä»Šæ—¥"
                else:
                    update_str = "ä¸æ˜"
                    delta_str = ""
                
                st.metric(
                    "æœ€çµ‚æ›´æ–°",
                    update_str,
                    delta=delta_str
                )
        
        # è©³ç´°æƒ…å ±ã®è¡¨ç¤º
        with st.expander("ğŸ“‹ ãƒ‡ãƒ¼ã‚¿è©³ç´°æƒ…å ±", expanded=False):
            data_info = export_data_info()
            for key, value in data_info.items():
                st.write(f"**{key}**: {value}")
    
    else:
        # ãƒ‡ãƒ¼ã‚¿æœªå­˜åœ¨æ™‚ã®è¡¨ç¤º
        st.info("ğŸ“ ãƒ‡ãƒ¼ã‚¿ãŒã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚ä¸‹è¨˜ã‹ã‚‰ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ã€‚")

def display_initial_upload_section():
    """åˆå›ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã‚»ã‚¯ã‚·ãƒ§ãƒ³"""
    st.markdown("### ğŸ“ ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰")
    
    # ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ€ãƒ¼
    uploaded_files = st.file_uploader(
        "ãƒ•ã‚¡ã‚¤ãƒ«ã‚’é¸æŠã—ã¦ãã ã•ã„",
        type=['xlsx', 'xls', 'csv'],
        accept_multiple_files=True,
        help="Excelå½¢å¼ã¾ãŸã¯CSVå½¢å¼ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã§ãã¾ã™"
    )
    
    # ç›®æ¨™å€¤ãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
    with st.expander("ğŸ¯ ç›®æ¨™å€¤ãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰", expanded=False):
        target_file = st.file_uploader(
            "ç›®æ¨™å€¤ãƒ•ã‚¡ã‚¤ãƒ«",
            type=['xlsx', 'xls', 'csv'],
            help="ç›®æ¨™å€¤ãŒè¨­å®šã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã§ãã¾ã™"
        )
    
    # å‡¦ç†å®Ÿè¡Œ
    if uploaded_files:
        if st.button("ğŸ“Š ãƒ‡ãƒ¼ã‚¿å‡¦ç†ã‚’é–‹å§‹", type="primary", use_container_width=True):
            process_and_save_data(uploaded_files, target_file)

def display_data_update_section():
    """ãƒ‡ãƒ¼ã‚¿æ›´æ–°ã‚»ã‚¯ã‚·ãƒ§ãƒ³"""
    st.markdown("### ğŸ”„ ãƒ‡ãƒ¼ã‚¿æ›´æ–°")
    
    with st.expander("ğŸ“ æ–°ã—ã„ãƒ‡ãƒ¼ã‚¿ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰", expanded=False):
        st.info("ğŸ’¡ æ–°ã—ã„ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã‚’æ›´æ–°ã§ãã¾ã™ã€‚")
        
        # æ›´æ–°ã‚ªãƒ—ã‚·ãƒ§ãƒ³
        update_mode = st.radio(
            "æ›´æ–°æ–¹æ³•ã‚’é¸æŠ",
            ["å®Œå…¨ç½®æ›", "ãƒ‡ãƒ¼ã‚¿è¿½åŠ "],
            help="å®Œå…¨ç½®æ›: æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã‚’æ–°ãƒ‡ãƒ¼ã‚¿ã§ç½®ãæ›ãˆ\nãƒ‡ãƒ¼ã‚¿è¿½åŠ : æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã«æ–°ãƒ‡ãƒ¼ã‚¿ã‚’è¿½åŠ "
        )
        
        # ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ€ãƒ¼
        new_files = st.file_uploader(
            "æ–°ã—ã„ãƒ•ã‚¡ã‚¤ãƒ«ã‚’é¸æŠ",
            type=['xlsx', 'xls', 'csv'],
            accept_multiple_files=True,
            key="update_files"
        )
        
        # ç›®æ¨™å€¤ãƒ•ã‚¡ã‚¤ãƒ«æ›´æ–°
        new_target_file = st.file_uploader(
            "æ–°ã—ã„ç›®æ¨™å€¤ãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰",
            type=['xlsx', 'xls', 'csv'],
            key="update_target_file"
        )
        
        col1, col2 = st.columns(2)
        
        with col1:
            if new_files and st.button("ğŸ”„ ãƒ‡ãƒ¼ã‚¿ã‚’æ›´æ–°", type="primary"):
                update_existing_data(new_files, new_target_file, update_mode)
        
        with col2:
            if st.button("ğŸ”ƒ ç¾åœ¨ã®ãƒ‡ãƒ¼ã‚¿ã‚’å†å‡¦ç†"):
                reprocess_current_data()

def display_data_management_section():
    """ãƒ‡ãƒ¼ã‚¿ç®¡ç†ã‚»ã‚¯ã‚·ãƒ§ãƒ³"""
    
    if st.session_state.get('data_processed', False):
        st.markdown("### âš™ï¸ ãƒ‡ãƒ¼ã‚¿ç®¡ç†")
        
        col1, col2, col3, col4 = st.columns(4)
        
        with col1:
            if st.button("ğŸ“¤ ãƒ‡ãƒ¼ã‚¿ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ"):
                export_current_data()
        
        with col2:
            if st.button("ğŸ”„ ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—å¾©å…ƒ"):
                if restore_from_backup():
                    st.success("ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã‹ã‚‰å¾©å…ƒã—ã¾ã—ãŸã€‚")
                    st.rerun()
        
        with col3:
            if st.button("ğŸ”ƒ ãƒ‡ãƒ¼ã‚¿å†èª­ã¿è¾¼ã¿"):
                reload_persistent_data()
        
        with col4:
            if st.button("ğŸ—‘ï¸ ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒªã‚¢", type="secondary"):
                show_data_clear_confirmation()

def load_single_file(uploaded_file):
    """
    å˜ä¸€ãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿ï¼ˆExcel/CSVå¯¾å¿œï¼‰
    
    Parameters:
    -----------
    uploaded_file: StreamlitUploadedFile
        ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«
        
    Returns:
    --------
    pd.DataFrame
        èª­ã¿è¾¼ã¾ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ 
    """
    try:
        # ãƒ•ã‚¡ã‚¤ãƒ«æ‹¡å¼µå­ã®ç¢ºèª
        file_extension = os.path.splitext(uploaded_file.name)[1].lower()
        
        if file_extension in ['.xlsx', '.xls']:
            # Excel ãƒ•ã‚¡ã‚¤ãƒ«ã®å ´åˆã¯æ—¢å­˜ã®load_filesé–¢æ•°ã‚’ä½¿ç”¨
            return load_files(None, [uploaded_file])
        
        elif file_extension == '.csv':
            # CSV ãƒ•ã‚¡ã‚¤ãƒ«ã®ç›´æ¥èª­ã¿è¾¼ã¿
            try:
                # ãƒ•ã‚¡ã‚¤ãƒ«ãƒã‚¤ãƒ³ã‚¿ã‚’å…ˆé ­ã«æˆ»ã™
                uploaded_file.seek(0)
                
                # ã¾ãšUTF-8ã§è©¦è¡Œ
                try:
                    df = pd.read_csv(uploaded_file, encoding='utf-8')
                except UnicodeDecodeError:
                    # UTF-8ã§å¤±æ•—ã—ãŸå ´åˆã¯Shift-JISã§è©¦è¡Œ
                    uploaded_file.seek(0)
                    df = pd.read_csv(uploaded_file, encoding='shift_jis')
                
                print(f"CSVèª­è¾¼æˆåŠŸ: {uploaded_file.name} - {len(df)}è¡Œ Ã— {len(df.columns)}åˆ—")
                print(f"CSVåˆ—å: {list(df.columns)}")
                
                return df
                
            except Exception as e:
                print(f"CSVèª­è¾¼ã‚¨ãƒ©ãƒ¼ ({uploaded_file.name}): {str(e)}")
                return pd.DataFrame()
        
        else:
            print(f"ã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ãªã„ãƒ•ã‚¡ã‚¤ãƒ«å½¢å¼: {file_extension}")
            return pd.DataFrame()
            
    except Exception as e:
        print(f"ãƒ•ã‚¡ã‚¤ãƒ«èª­è¾¼ã‚¨ãƒ©ãƒ¼ ({uploaded_file.name}): {str(e)}")
        return pd.DataFrame()

def process_and_save_data(uploaded_files, target_file=None):
    """ãƒ‡ãƒ¼ã‚¿å‡¦ç†ã¨ä¿å­˜ï¼ˆCSVå¯¾å¿œç‰ˆï¼‰"""
    try:
        with st.spinner("ãƒ‡ãƒ¼ã‚¿ã‚’å‡¦ç†ä¸­..."):
            # ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿
            progress_bar = st.progress(0)
            progress_bar.progress(25, "ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿ä¸­...")
            
            df_raw = load_files(None, uploaded_files)
            if df_raw is None or df_raw.empty:
                st.error("ãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸã€‚")
                return
            
            st.write(f"ğŸ“Š èª­ã¿è¾¼ã¿å®Œäº†: {len(df_raw):,}ä»¶")
            
            progress_bar.progress(50, "ç›®æ¨™ãƒ‡ãƒ¼ã‚¿ã‚’å‡¦ç†ä¸­...")
            
            # âœ… ä¿®æ­£ï¼šç›®æ¨™ãƒ‡ãƒ¼ã‚¿ã®å‡¦ç†ï¼ˆCSVå¯¾å¿œï¼‰
            target_data = None
            if target_file is not None:
                try:
                    # âœ… ä¿®æ­£ï¼šCSVãƒ•ã‚¡ã‚¤ãƒ«ã«å¯¾å¿œã—ãŸèª­ã¿è¾¼ã¿
                    target_data = load_single_file(target_file)
                    
                    if target_data is not None and not target_data.empty:
                        st.write(f"ğŸ¯ ç›®æ¨™ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿å®Œäº†: {len(target_data):,}ä»¶")
                        st.write(f"ğŸ·ï¸ ç›®æ¨™ãƒ‡ãƒ¼ã‚¿åˆ—: {list(target_data.columns)}")
                        
                        # éƒ¨é–€ã‚³ãƒ¼ãƒ‰åˆ—ã®ç¢ºèª
                        if 'éƒ¨é–€ã‚³ãƒ¼ãƒ‰' in target_data.columns:
                            unique_depts = target_data['éƒ¨é–€ã‚³ãƒ¼ãƒ‰'].nunique()
                            dept_list = target_data['éƒ¨é–€ã‚³ãƒ¼ãƒ‰'].unique()[:10]  # æœ€åˆã®10å€‹ã‚’è¡¨ç¤º
                            st.success(f"âœ… éƒ¨é–€ã‚³ãƒ¼ãƒ‰åˆ—ã‚’ç¢ºèª: {unique_depts}éƒ¨é–€")
                            st.info(f"ğŸ“‹ éƒ¨é–€ã‚³ãƒ¼ãƒ‰ä¾‹: {list(dept_list)}")
                        else:
                            st.warning("âš ï¸ ç›®æ¨™ãƒ‡ãƒ¼ã‚¿ã«'éƒ¨é–€ã‚³ãƒ¼ãƒ‰'åˆ—ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
                            st.write(f"åˆ©ç”¨å¯èƒ½ãªåˆ—: {list(target_data.columns)}")
                        
                        # éƒ¨é–€ååˆ—ã®ç¢ºèª
                        if 'éƒ¨é–€å' in target_data.columns:
                            dept_names = target_data['éƒ¨é–€å'].unique()[:10]
                            st.info(f"ğŸ¥ éƒ¨é–€åä¾‹: {list(dept_names)}")
                            
                    else:
                        st.warning("âš ï¸ ç›®æ¨™ãƒ‡ãƒ¼ã‚¿ãŒç©ºã§ã™")
                        
                except Exception as e:
                    st.error(f"ç›®æ¨™ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
                    import traceback
                    st.error(traceback.format_exc())
                    target_data = None
            
            progress_bar.progress(75, "ãƒ‡ãƒ¼ã‚¿ã‚’å‰å‡¦ç†ä¸­...")
            
            # âœ… ä¿®æ­£ï¼štarget_dataã‚’æ˜ç¤ºçš„ã«æ¸¡ã™
            df_processed, validation_results = integrated_preprocess_data(df_raw, target_data_df=target_data)
            if df_processed is None or df_processed.empty:
                st.error("ãƒ‡ãƒ¼ã‚¿ã®å‰å‡¦ç†ã«å¤±æ•—ã—ã¾ã—ãŸã€‚")
                return
            
            st.write(f"ğŸ“Š å‰å‡¦ç†å®Œäº†: {len(df_processed):,}ä»¶")
            loss_count = len(df_raw) - len(df_processed)
            if loss_count > 0:
                loss_rate = (loss_count / len(df_raw)) * 100
                st.warning(f"âš ï¸ å‰å‡¦ç†ã§{loss_count:,}ä»¶å‰Šé™¤ã•ã‚Œã¾ã—ãŸï¼ˆ{loss_rate:.1f}%ï¼‰")
                if loss_rate > 10:
                    st.error("ğŸš¨ å¤§é‡ã®ãƒ‡ãƒ¼ã‚¿ãŒå¤±ã‚ã‚Œã¦ã„ã¾ã™ã€‚è¨­å®šã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
            else:
                st.success("âœ… ãƒ‡ãƒ¼ã‚¿æå¤±ãªã—")
            
            progress_bar.progress(90, "ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜ä¸­...")
            
            # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã®æº–å‚™
            metadata = {
                'upload_files': [f.name for f in uploaded_files],
                'target_file': target_file.name if target_file else None,
                'validation_results': validation_results,
                'processing_timestamp': datetime.now(),
                'raw_record_count': len(df_raw),
                'processed_record_count': len(df_processed)
            }
            
            # åŸºæœ¬è¨­å®šã®æº–å‚™
            settings = {
                'total_beds': st.session_state.get('total_beds', 612),
                'bed_occupancy_rate': st.session_state.get('bed_occupancy_rate', 0.85),
                'avg_length_of_stay': st.session_state.get('avg_length_of_stay', 12.0),
                'avg_admission_fee': st.session_state.get('avg_admission_fee', 55000)
            }
            
            # ãƒ‡ãƒ¼ã‚¿ä¿å­˜
            if save_persistent_data(df_processed, target_data, settings, metadata):
                # ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã®æ›´æ–°
                st.session_state['df'] = df_processed
                st.session_state['target_data'] = target_data
                st.session_state['data_processed'] = True
                st.session_state['data_loaded_from_persistent'] = True
                
                progress_bar.progress(100, "å®Œäº†!")
                st.success("âœ… ãƒ‡ãƒ¼ã‚¿ã®å‡¦ç†ã¨ä¿å­˜ãŒå®Œäº†ã—ã¾ã—ãŸï¼")
                st.balloons()
                
                # ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³çµæœã®è©³ç´°è¡¨ç¤º
                if validation_results:
                    with st.expander("ğŸ” å‡¦ç†çµæœè©³ç´°", expanded=False):
                        if validation_results.get('warnings'):
                            st.warning("âš ï¸ è­¦å‘Š:")
                            for warning in validation_results['warnings']:
                                st.write(f"â€¢ {warning}")
                        
                        if validation_results.get('info'):
                            st.info("â„¹ï¸ æƒ…å ±:")
                            for info in validation_results['info']:
                                st.write(f"â€¢ {info}")
                        
                        if validation_results.get('errors'):
                            st.error("âŒ ã‚¨ãƒ©ãƒ¼:")
                            for error in validation_results['errors']:
                                st.write(f"â€¢ {error}")
                
                # âœ… è¿½åŠ ï¼šè¨ºç™‚ç§‘ãƒãƒƒãƒ”ãƒ³ã‚°çŠ¶æ³ã®è¡¨ç¤º
                if target_data is not None and not target_data.empty:
                    with st.expander("ğŸ¥ è¨ºç™‚ç§‘ãƒãƒƒãƒ”ãƒ³ã‚°çŠ¶æ³", expanded=False):
                        # å®Ÿãƒ‡ãƒ¼ã‚¿ã®è¨ºç™‚ç§‘å
                        actual_depts = df_processed['è¨ºç™‚ç§‘å'].unique() if 'è¨ºç™‚ç§‘å' in df_processed.columns else []
                        st.write(f"**å®Ÿãƒ‡ãƒ¼ã‚¿ã®è¨ºç™‚ç§‘:** {len(actual_depts)}ç¨®é¡")
                        st.write(f"ä¾‹: {list(actual_depts[:10])}")
                        
                        # ç›®æ¨™ãƒ‡ãƒ¼ã‚¿ã®éƒ¨é–€
                        target_dept_codes = target_data['éƒ¨é–€ã‚³ãƒ¼ãƒ‰'].unique() if 'éƒ¨é–€ã‚³ãƒ¼ãƒ‰' in target_data.columns else []
                        target_dept_names = target_data['éƒ¨é–€å'].unique() if 'éƒ¨é–€å' in target_data.columns else []
                        
                        st.write(f"**ç›®æ¨™ãƒ‡ãƒ¼ã‚¿ã®éƒ¨é–€ã‚³ãƒ¼ãƒ‰:** {len(target_dept_codes)}ç¨®é¡")
                        st.write(f"ä¾‹: {list(target_dept_codes[:10])}")
                        
                        if len(target_dept_names) > 0:
                            st.write(f"**ç›®æ¨™ãƒ‡ãƒ¼ã‚¿ã®éƒ¨é–€å:** {len(target_dept_names)}ç¨®é¡")
                            st.write(f"ä¾‹: {list(target_dept_names[:10])}")
                
                # ãƒ‡ãƒ¼ã‚¿çµ±è¨ˆã®è¡¨ç¤º
                show_processing_results(df_processed, validation_results)
                
            else:
                st.error("ãƒ‡ãƒ¼ã‚¿ã®ä¿å­˜ã«å¤±æ•—ã—ã¾ã—ãŸã€‚")
                
    except Exception as e:
        st.error(f"ãƒ‡ãƒ¼ã‚¿å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}")
        import traceback
        st.error(traceback.format_exc())

def update_existing_data(new_files, target_file, update_mode):
    """æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã®æ›´æ–°ï¼ˆç›®æ¨™å€¤ãƒ•ã‚¡ã‚¤ãƒ«å¯¾å¿œç‰ˆï¼‰"""
    try:
        with st.spinner(f"ãƒ‡ãƒ¼ã‚¿ã‚’{update_mode}ä¸­..."):
            # æ–°ã—ã„ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿ãƒ»å‡¦ç†
            df_new = load_files(None, new_files)
            st.write(f"ğŸ“Š æ–°ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿: {len(df_new):,}ä»¶")
            
            # âœ… ä¿®æ­£ï¼šç›®æ¨™ãƒ‡ãƒ¼ã‚¿ã®å‡¦ç†
            target_data = None
            if target_file is not None:
                try:
                    target_data = load_files(None, [target_file])
                    if target_data is not None and not target_data.empty:
                        st.write(f"ğŸ¯ ç›®æ¨™ãƒ‡ãƒ¼ã‚¿æ›´æ–°: {len(target_data):,}ä»¶")
                except Exception as e:
                    st.warning(f"ç›®æ¨™ãƒ‡ãƒ¼ã‚¿ã®æ›´æ–°ã«å¤±æ•—: {e}")
            else:
                target_data = st.session_state.get('target_data')
            
            # âœ… ä¿®æ­£ï¼šå‰å‡¦ç†æ™‚ã«ç›®æ¨™ãƒ‡ãƒ¼ã‚¿ã‚’æ¸¡ã™
            df_new_processed, validation_results = integrated_preprocess_data(df_new, target_data_df=target_data)
            st.write(f"ğŸ“Š æ–°ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†å®Œäº†: {len(df_new_processed):,}ä»¶")
            
            if update_mode == "å®Œå…¨ç½®æ›":
                df_final = df_new_processed
                st.info("æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã‚’æ–°ã—ã„ãƒ‡ãƒ¼ã‚¿ã§å®Œå…¨ã«ç½®ãæ›ãˆã¾ã—ãŸã€‚")
            else:  # ãƒ‡ãƒ¼ã‚¿è¿½åŠ 
                df_existing = st.session_state.get('df')
                if df_existing is not None:
                    st.write(f"ğŸ“Š æ—¢å­˜ãƒ‡ãƒ¼ã‚¿: {len(df_existing):,}ä»¶")
                    
                    # ãƒ‡ãƒ¼ã‚¿çµåˆ
                    df_combined = pd.concat([df_existing, df_new_processed], ignore_index=True)
                    st.write(f"ğŸ“Š çµåˆå¾Œ: {len(df_combined):,}ä»¶")
                    
                    # é‡è¤‡é™¤å»ã‚’å®‰å…¨ã«å®Ÿè¡Œ
                    before_dedup = len(df_combined)
                    if 'æ—¥ä»˜' in df_combined.columns and 'ç—…æ£Ÿã‚³ãƒ¼ãƒ‰' in df_combined.columns:
                        df_final = df_combined.drop_duplicates(
                            subset=['æ—¥ä»˜', 'ç—…æ£Ÿã‚³ãƒ¼ãƒ‰', 'è¨ºç™‚ç§‘å'] if 'è¨ºç™‚ç§‘å' in df_combined.columns else ['æ—¥ä»˜', 'ç—…æ£Ÿã‚³ãƒ¼ãƒ‰']
                        )
                    else:
                        df_final = df_combined
                        st.warning("âš ï¸ é©åˆ‡ãªé‡è¤‡é™¤å»ã‚­ãƒ¼ãŒè¦‹ã¤ã‹ã‚‰ãªã„ãŸã‚ã€é‡è¤‡é™¤å»ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã—ãŸã€‚")
                    
                    after_dedup = len(df_final)
                    removed_count = before_dedup - after_dedup
                    
                    st.write(f"ğŸ“Š é‡è¤‡é™¤å»å¾Œ: {len(df_final):,}ä»¶")
                    if removed_count > 0:
                        st.info(f"ğŸ”„ {removed_count:,}ä»¶ã®é‡è¤‡ãƒ‡ãƒ¼ã‚¿ã‚’é™¤å»ã—ã¾ã—ãŸã€‚")
                else:
                    df_final = df_new_processed
            
            st.write(f"ğŸ“Š æœ€çµ‚ãƒ‡ãƒ¼ã‚¿: {len(df_final):,}ä»¶")
            
            # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã¨è¨­å®šã®æº–å‚™
            metadata = {
                'update_mode': update_mode,
                'update_files': [f.name for f in new_files],
                'target_file': target_file.name if target_file else None,
                'update_timestamp': datetime.now(),
                'final_record_count': len(df_final)
            }
            
            settings = {key: st.session_state.get(key) for key in [
                'total_beds', 'bed_occupancy_rate', 'avg_length_of_stay', 'avg_admission_fee'
            ]}
            
            # ä¿å­˜ã¨çŠ¶æ…‹æ›´æ–°
            if save_persistent_data(df_final, target_data, settings, metadata):
                st.session_state['df'] = df_final
                st.session_state['target_data'] = target_data
                st.success("âœ… ãƒ‡ãƒ¼ã‚¿ã®æ›´æ–°ãŒå®Œäº†ã—ã¾ã—ãŸï¼")
                st.rerun()
                
    except Exception as e:
        st.error(f"ãƒ‡ãƒ¼ã‚¿æ›´æ–°ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}")
        import traceback
        st.error(traceback.format_exc())

def reprocess_current_data():
    """ç¾åœ¨ã®ãƒ‡ãƒ¼ã‚¿ã®å†å‡¦ç†"""
    try:
        with st.spinner("ãƒ‡ãƒ¼ã‚¿ã‚’å†å‡¦ç†ä¸­..."):
            df_current = st.session_state.get('df')
            if df_current is None:
                st.error("å†å‡¦ç†ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
                return
            
            # å†å‡¦ç†ï¼ˆè¨­å®šå€¤ã®å¤‰æ›´ã‚’åæ˜ ï¼‰
            settings = {key: st.session_state.get(key) for key in [
                'total_beds', 'bed_occupancy_rate', 'avg_length_of_stay', 'avg_admission_fee'
            ]}
            
            metadata = {
                'reprocess_timestamp': datetime.now(),
                'action': 'reprocess'
            }
            
            if save_persistent_data(df_current, st.session_state.get('target_data'), settings, metadata):
                st.success("âœ… ãƒ‡ãƒ¼ã‚¿ã®å†å‡¦ç†ãŒå®Œäº†ã—ã¾ã—ãŸï¼")
                st.rerun()
                
    except Exception as e:
        st.error(f"å†å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}")

def export_current_data():
    """ç¾åœ¨ã®ãƒ‡ãƒ¼ã‚¿ã®ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ"""
    try:
        df = st.session_state.get('df')
        if df is None:
            st.error("ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆã™ã‚‹ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
            return
        
        # CSVå½¢å¼ã§ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
        csv_data = df.to_csv(index=False).encode('utf-8-sig')
        
        st.download_button(
            label="ğŸ“¥ CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
            data=csv_data,
            file_name=f"hospital_data_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv",
            mime="text/csv"
        )
        
        st.success("ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãƒœã‚¿ãƒ³ãŒç”Ÿæˆã•ã‚Œã¾ã—ãŸã€‚")
        
    except Exception as e:
        st.error(f"ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}")

def reload_persistent_data():
    """æ°¸ç¶šåŒ–ãƒ‡ãƒ¼ã‚¿ã®å†èª­ã¿è¾¼ã¿"""
    try:
        with st.spinner("ãƒ‡ãƒ¼ã‚¿ã‚’å†èª­ã¿è¾¼ã¿ä¸­..."):
            if auto_load_persistent_data():
                st.success("âœ… ãƒ‡ãƒ¼ã‚¿ã‚’å†èª­ã¿è¾¼ã¿ã—ã¾ã—ãŸï¼")
                st.rerun()
            else:
                st.error("ãƒ‡ãƒ¼ã‚¿ã®å†èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸã€‚")
                
    except Exception as e:
        st.error(f"å†èª­ã¿è¾¼ã¿ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}")

def show_data_clear_confirmation():
    """ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒªã‚¢ç¢ºèªãƒ€ã‚¤ã‚¢ãƒ­ã‚°"""
    st.warning("âš ï¸ ã“ã®æ“ä½œã«ã‚ˆã‚Šã€ä¿å­˜ã•ã‚ŒãŸã™ã¹ã¦ã®ãƒ‡ãƒ¼ã‚¿ãŒå‰Šé™¤ã•ã‚Œã¾ã™ã€‚")
    
    if st.button("ğŸ—‘ï¸ ç¢ºèªã—ã¦ãƒ‡ãƒ¼ã‚¿ã‚’å‰Šé™¤", type="secondary"):
        if clear_persistent_data():
            # ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã®ã‚¯ãƒªã‚¢
            keys_to_clear = ['df', 'target_data', 'data_processed', 'data_loaded_from_persistent']
            for key in keys_to_clear:
                if key in st.session_state:
                    del st.session_state[key]
            
            st.success("âœ… ãƒ‡ãƒ¼ã‚¿ã‚’å‰Šé™¤ã—ã¾ã—ãŸã€‚")
            st.rerun()
        else:
            st.error("ãƒ‡ãƒ¼ã‚¿ã®å‰Šé™¤ã«å¤±æ•—ã—ã¾ã—ãŸã€‚")

def show_processing_results(df, validation_results):
    """å‡¦ç†çµæœã®è¡¨ç¤º"""
    st.markdown("### ğŸ“Š å‡¦ç†çµæœ")
    
    col1, col2, col3 = st.columns(3)
    
    with col1:
        st.metric("ç·ãƒ¬ã‚³ãƒ¼ãƒ‰æ•°", f"{len(df):,}ä»¶")
    
    with col2:
        date_range = "ä¸æ˜"
        if 'æ—¥ä»˜' in df.columns:
            min_date = df['æ—¥ä»˜'].min().strftime('%Y-%m-%d')
            max_date = df['æ—¥ä»˜'].max().strftime('%Y-%m-%d')
            date_range = f"{min_date} ï½ {max_date}"
        st.metric("ãƒ‡ãƒ¼ã‚¿æœŸé–“", date_range)
    
    with col3:
        unique_wards = df['ç—…æ£Ÿã‚³ãƒ¼ãƒ‰'].nunique() if 'ç—…æ£Ÿã‚³ãƒ¼ãƒ‰' in df.columns else 0
        st.metric("ç—…æ£Ÿæ•°", f"{unique_wards}ç®‡æ‰€")
    
    # ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³çµæœ
    if validation_results:
        with st.expander("ğŸ” ãƒ‡ãƒ¼ã‚¿æ¤œè¨¼çµæœ", expanded=False):
            for key, value in validation_results.items():
                st.write(f"**{key}**: {value}")