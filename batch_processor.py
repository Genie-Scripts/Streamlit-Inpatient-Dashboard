import pandas as pd
import streamlit as st # Only for type hints or if st.session_state is used in main process
from io import BytesIO
import zipfile
import os
import time
from functools import partial
import multiprocessing
import tempfile
import gc
import psutil
import re # re ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor, as_completed
import threading

# forecast ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®é–¢æ•°
from forecast import generate_filtered_summaries, create_forecast_dataframe

# pdf_generator ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®é–¢æ•° (ä¿®æ­£ã•ã‚ŒãŸã‚‚ã®ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ)
from pdf_generator import (
    create_pdf, create_landscape_pdf, register_fonts,
    MATPLOTLIB_FONT_NAME, REPORTLAB_FONT_NAME, # ãƒ•ã‚©ãƒ³ãƒˆå
    create_alos_chart_for_pdf,
    create_patient_chart_with_target_wrapper, # pdf_generatorå†…ã®ãƒ©ãƒƒãƒ‘ãƒ¼é–¢æ•°
    create_dual_axis_chart_for_pdf, # pdf_generatorå†…ã®MatplotlibäºŒè»¸ã‚°ãƒ©ãƒ•é–¢æ•°
    get_chart_cache_key as get_pdf_gen_chart_cache_key, # pdf_generatorã®ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚­ãƒ¼é–¢æ•°
    compute_data_hash as compute_pdf_gen_data_hash,   # pdf_generatorã®ãƒãƒƒã‚·ãƒ¥é–¢æ•°
    get_chart_cache as get_pdf_gen_main_process_cache # ãƒ¡ã‚¤ãƒ³ãƒ—ãƒ­ã‚»ã‚¹ç”¨ã‚­ãƒ£ãƒƒã‚·ãƒ¥å–å¾—
)

# ğŸš€ æœ€é©åŒ–è¨­å®š
OPTIMAL_CHUNK_SIZE = 3
MAX_MEMORY_USAGE_PERCENT = 85
GRAPH_THREAD_WORKERS = 4

class SystemOptimizer:
    """ã‚·ã‚¹ãƒ†ãƒ ãƒªã‚½ãƒ¼ã‚¹ã‚’ç›£è¦–ãƒ»æœ€é©åŒ–ã™ã‚‹ã‚¯ãƒ©ã‚¹"""
    
    @staticmethod
    def get_optimal_config():
        """ã‚·ã‚¹ãƒ†ãƒ ç’°å¢ƒã«åŸºã¥ãæœ€é©è¨­å®šã‚’è¨ˆç®—"""
        cpu_cores = multiprocessing.cpu_count()
        memory_gb = psutil.virtual_memory().total / (1024**3)
        
        # æœ€é©ãªãƒ¯ãƒ¼ã‚«ãƒ¼æ•°ã‚’è¨ˆç®—
        optimal_workers = min(
            cpu_cores - 1,  # CPUã‚³ã‚¢æ•° - 1
            int(memory_gb / 1.5),  # ãƒ¡ãƒ¢ãƒªGB / 1.5
            8  # æœ€å¤§8ãƒ¯ãƒ¼ã‚«ãƒ¼
        )
        
        # ãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚ºã®æœ€é©åŒ–
        chunk_size = max(2, min(OPTIMAL_CHUNK_SIZE, optimal_workers))
        
        return {
            'max_workers': max(1, optimal_workers),
            'chunk_size': chunk_size,
            'memory_limit': MAX_MEMORY_USAGE_PERCENT,
            'cpu_cores': cpu_cores,
            'memory_gb': memory_gb
        }
    
    @staticmethod
    def check_memory_usage():
        """ç¾åœ¨ã®ãƒ¡ãƒ¢ãƒªä½¿ç”¨ç‡ã‚’ãƒã‚§ãƒƒã‚¯"""
        return psutil.virtual_memory().percent
    
    @staticmethod
    def force_cleanup():
        """å¼·åˆ¶çš„ãªãƒ¡ãƒ¢ãƒªã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—"""
        gc.collect()

class DataOptimizer:
    """ãƒ‡ãƒ¼ã‚¿å‡¦ç†ã‚’æœ€é©åŒ–ã™ã‚‹ã‚¯ãƒ©ã‚¹"""
    
    @staticmethod
    def optimize_dataframe(df):
        """ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã®ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚’æœ€é©åŒ–"""
        if df.empty:
            return df
        
        try:
            optimized_df = df.copy()
            
            # ã‚«ãƒ†ã‚´ãƒªã‚«ãƒ«å¤‰æ›
            categorical_columns = ['è¨ºç™‚ç§‘å', 'ç—…æ£Ÿã‚³ãƒ¼ãƒ‰']
            for col in categorical_columns:
                if col in optimized_df.columns and optimized_df[col].dtype == 'object':
                    optimized_df[col] = optimized_df[col].astype('category')
            
            # æ•°å€¤å‹ã®æœ€é©åŒ–
            numeric_cols = optimized_df.select_dtypes(include=['int64', 'float64']).columns
            for col in numeric_cols:
                if col != 'æ—¥ä»˜':  # æ—¥ä»˜åˆ—ã¯é™¤å¤–
                    if optimized_df[col].dtype == 'int64':
                        col_min = optimized_df[col].min()
                        col_max = optimized_df[col].max()
                        
                        if col_min >= 0 and col_max <= 4294967295:
                            optimized_df[col] = optimized_df[col].astype('uint32')
                        elif col_min >= -2147483648 and col_max <= 2147483647:
                            optimized_df[col] = optimized_df[col].astype('int32')
                    elif optimized_df[col].dtype == 'float64':
                        optimized_df[col] = optimized_df[col].astype('float32')
            
            return optimized_df
            
        except Exception as e:
            print(f"Data optimization failed: {e}")
            return df

# ğŸ”§ æ–°ã—ã„ç›®æ¨™å€¤ãƒ•ã‚¡ã‚¤ãƒ«æ§‹é€ ã«å¯¾å¿œã—ãŸç›®æ¨™å€¤å–å¾—é–¢æ•°
def get_targets_for_pdf(task_value, task_type, target_data_df):
    """æ–°ã—ã„ç›®æ¨™å€¤ãƒ•ã‚¡ã‚¤ãƒ«æ§‹é€ ã«å¯¾å¿œã—ãŸç›®æ¨™å€¤å–å¾—é–¢æ•°"""
    t_all, t_wd, t_hd = None, None, None
    if target_data_df is None or target_data_df.empty: 
        return t_all, t_wd, t_hd
    
    filter_code = task_value if task_type != "all" else "å…¨ä½“"
    
    # ğŸ”§ æ–°ã—ã„CSVãƒ•ã‚¡ã‚¤ãƒ«æ§‹é€ ã‚’ãƒã‚§ãƒƒã‚¯
    required_cols_new = ['éƒ¨é–€ã‚³ãƒ¼ãƒ‰', 'æŒ‡æ¨™ã‚¿ã‚¤ãƒ—', 'æœŸé–“åŒºåˆ†', 'ç›®æ¨™å€¤']  # æ–°ã—ã„æ§‹é€ 
    required_cols_old = ['éƒ¨é–€ã‚³ãƒ¼ãƒ‰', 'åŒºåˆ†', 'ç›®æ¨™å€¤']  # å¤ã„æ§‹é€ 
    
    if all(col in target_data_df.columns for col in required_cols_new):
        # ğŸ”§ æ–°ã—ã„æ§‹é€ ã®å‡¦ç†
        # æ—¥å¹³å‡åœ¨é™¢æ‚£è€…æ•°ã®ç›®æ¨™å€¤ã‚’å–å¾—
        daily_census_targets = target_data_df[
            (target_data_df['æŒ‡æ¨™ã‚¿ã‚¤ãƒ—'].str.contains('æ—¥å¹³å‡åœ¨é™¢æ‚£è€…æ•°|åœ¨é™¢æ‚£è€…æ•°', case=False, na=False)) &
            (target_data_df['éƒ¨é–€ã‚³ãƒ¼ãƒ‰'].astype(str) == str(filter_code))
        ]
        
        if not daily_census_targets.empty:
            for _, row_t in daily_census_targets.iterrows():
                val_t = row_t.get('ç›®æ¨™å€¤')
                period_category = row_t.get('æœŸé–“åŒºåˆ†', '')
                
                if pd.notna(val_t):
                    if period_category == 'å…¨æ—¥': 
                        t_all = float(val_t)
                    elif period_category == 'å¹³æ—¥': 
                        t_wd = float(val_t)
                    elif period_category == 'ä¼‘æ—¥': 
                        t_hd = float(val_t)
        
    elif all(col in target_data_df.columns for col in required_cols_old):
        # ğŸ”§ å¤ã„æ§‹é€ ã®å‡¦ç†ï¼ˆæ—¢å­˜ã‚³ãƒ¼ãƒ‰ã¨åŒã˜ï¼‰
        target_rows_df = target_data_df[target_data_df['éƒ¨é–€ã‚³ãƒ¼ãƒ‰'].astype(str) == str(filter_code)]
        if not target_rows_df.empty:
            for _, row_t in target_rows_df.iterrows():
                val_t = row_t.get('ç›®æ¨™å€¤')
                if pd.notna(val_t):
                    if row_t.get('åŒºåˆ†') == 'å…¨æ—¥': 
                        t_all = float(val_t)
                    elif row_t.get('åŒºåˆ†') == 'å¹³æ—¥': 
                        t_wd = float(val_t)
                    elif row_t.get('åŒºåˆ†') == 'ä¼‘æ—¥': 
                        t_hd = float(val_t)
    
    return t_all, t_wd, t_hd

# ğŸ”§ æ–°ã—ã„ç›®æ¨™å€¤ãƒ•ã‚¡ã‚¤ãƒ«æ§‹é€ ã«å¯¾å¿œã—ãŸè¡¨ç¤ºåãƒãƒƒãƒ”ãƒ³ã‚°ä½œæˆ
def create_display_mapping_with_new_target_format(target_data_main):
    """æ–°ã—ã„ç›®æ¨™å€¤ãƒ•ã‚¡ã‚¤ãƒ«æ§‹é€ ã«å¯¾å¿œã—ãŸè¡¨ç¤ºåãƒãƒƒãƒ”ãƒ³ã‚°ä½œæˆ"""
    dept_display_map = {}
    ward_display_map = {}
    
    if target_data_main is not None and not target_data_main.empty:
        # ğŸ”§ æ–°æ—§ä¸¡æ–¹ã®æ§‹é€ ã«å¯¾å¿œ
        if 'éƒ¨é–€ã‚³ãƒ¼ãƒ‰' in target_data_main.columns and 'éƒ¨é–€å' in target_data_main.columns:
            # é‡è¤‡ã‚’é¿ã‘ã‚‹ãŸã‚ã€éƒ¨é–€ã‚³ãƒ¼ãƒ‰ã¨éƒ¨é–€åã®ãƒ¦ãƒ‹ãƒ¼ã‚¯ãªçµ„ã¿åˆã‚ã›ã‚’å–å¾—
            unique_mappings = target_data_main[['éƒ¨é–€ã‚³ãƒ¼ãƒ‰', 'éƒ¨é–€å']].drop_duplicates()
            
            for _, row in unique_mappings.iterrows():
                if pd.notna(row['éƒ¨é–€ã‚³ãƒ¼ãƒ‰']) and pd.notna(row['éƒ¨é–€å']):
                    code_str = str(row['éƒ¨é–€ã‚³ãƒ¼ãƒ‰'])
                    name_str = str(row['éƒ¨é–€å'])
                    dept_display_map[code_str] = name_str
                    ward_display_map[code_str] = name_str
                    
                    # è¨ºç™‚ç§‘åã‚‚éƒ¨é–€åã¨ã—ã¦ä½¿ç”¨ã•ã‚Œã‚‹å ´åˆãŒã‚ã‚‹ãŸã‚ã€é€†ãƒãƒƒãƒ”ãƒ³ã‚°ã‚‚ä½œæˆ
                    dept_display_map[name_str] = name_str
    
    return dept_display_map, ward_display_map

def process_pdf_in_worker_revised(
    df_path, filter_type, filter_value, display_name, latest_date_str, landscape,
    target_data_path=None, reduced_graphs=True,
    alos_chart_buffers_payload=None,
    patient_chart_buffers_payload=None,
    dual_axis_chart_buffers_payload=None
    ):
    """
    ãƒ¯ãƒ¼ã‚«ãƒ¼ãƒ—ãƒ­ã‚»ã‚¹ã§PDFã‚’ç”Ÿæˆã™ã‚‹ (ã‚°ãƒ©ãƒ•ãƒãƒƒãƒ•ã‚¡ã‚’å—ã‘å–ã‚‹)
    """
    try:
        pid = os.getpid()
        # register_fonts() # ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚¤ãƒ³ãƒãƒ¼ãƒˆæ™‚ã«å®Ÿè¡Œã•ã‚Œã‚‹ã¯ãš
        # print(f"PID {pid}: Worker for '{display_name}' started. Font: {MATPLOTLIB_FONT_NAME if MATPLOTLIB_FONT_NAME else 'Default'}")

        df_worker = pd.read_feather(df_path)
        latest_date_worker = pd.Timestamp(latest_date_str)
        
        target_data_worker = None
        if target_data_path and os.path.exists(target_data_path):
            target_data_worker = pd.read_feather(target_data_path)
        
        current_data_for_tables_worker = df_worker.copy() # ãƒ†ãƒ¼ãƒ–ãƒ«ç”Ÿæˆç”¨
        current_filter_code_worker = "å…¨ä½“"
        title_prefix_for_pdf = "å…¨ä½“"

        if filter_type == "dept":
            current_data_for_tables_worker = df_worker[df_worker["è¨ºç™‚ç§‘å"] == filter_value].copy()
            current_filter_code_worker = filter_value
            title_prefix_for_pdf = f"è¨ºç™‚ç§‘åˆ¥ {display_name}"
        elif filter_type == "ward":
            current_data_for_tables_worker = df_worker[df_worker["ç—…æ£Ÿã‚³ãƒ¼ãƒ‰"] == filter_value].copy()
            current_filter_code_worker = str(filter_value)
            title_prefix_for_pdf = f"ç—…æ£Ÿåˆ¥ {display_name}"
        
        if current_data_for_tables_worker.empty and filter_type != "all":
            # print(f"PID {pid}: Filtered data for tables empty for {title_prefix_for_pdf}. Skipping.")
            return None # ãƒ‡ãƒ¼ã‚¿ãŒãªã„å ´åˆã¯Noneã‚’è¿”ã™
            
        summaries_worker = generate_filtered_summaries(
            current_data_for_tables_worker, 
            "è¨ºç™‚ç§‘å" if filter_type == "dept" else ("ç—…æ£Ÿã‚³ãƒ¼ãƒ‰" if filter_type == "ward" else None),
            filter_value if filter_type != "all" else None
        )
        
        if not summaries_worker:
            # print(f"PID {pid}: Failed to generate summaries for {title_prefix_for_pdf}.")
            return None

        forecast_df_for_pdf = create_forecast_dataframe(
            summaries_worker.get("summary"), summaries_worker.get("weekday"), 
            summaries_worker.get("holiday"), latest_date_worker
        )
        
        # ğŸ”§ ã‚°ãƒ©ãƒ•æ—¥æ•°ãƒªã‚¹ãƒˆã‚’çµ±ä¸€ï¼ˆreduced_graphsã«é–¢ä¿‚ãªãå›ºå®šï¼‰
        graph_days_list_for_pdf = ["90"] if reduced_graphs else ["90", "180"]

        pdf_creation_func = create_landscape_pdf if landscape else create_pdf
        
        pdf_bytes_io_result = pdf_creation_func(
            forecast_df=forecast_df_for_pdf,
            df_weekday=summaries_worker.get("weekday"),
            df_holiday=summaries_worker.get("holiday"),
            df_all_avg=summaries_worker.get("summary"),
            chart_data=current_data_for_tables_worker, # éƒ¨é–€åˆ¥ãƒ†ãƒ¼ãƒ–ãƒ«ç”¨
            title_prefix=title_prefix_for_pdf,
            latest_date=latest_date_worker,
            target_data=target_data_worker,
            filter_code=current_filter_code_worker,
            graph_days=graph_days_list_for_pdf, # ã“ã®å¼•æ•°ã¯pdf_generatorå´ã§ä½¿ã‚ã‚Œãªããªã‚‹æƒ³å®š
            alos_chart_buffers=alos_chart_buffers_payload,
            patient_chart_buffers=patient_chart_buffers_payload,
            dual_axis_chart_buffers=dual_axis_chart_buffers_payload
        )
        
        # ãƒ¡ãƒ¢ãƒªè§£æ”¾
        del df_worker, current_data_for_tables_worker, summaries_worker, forecast_df_for_pdf, target_data_worker
        gc.collect()
        
        return (title_prefix_for_pdf, pdf_bytes_io_result) if pdf_bytes_io_result else None

    except Exception as e:
        print(f"PID {os.getpid()}: Error in worker for {filter_type} {filter_value} ('{display_name}'): {e}")
        import traceback
        print(traceback.format_exc())
        return None

def batch_generate_pdfs_mp_optimized(df_main, mode="all", landscape=False, target_data_main=None, 
                                    progress_callback=None, max_workers=None, fast_mode=True):
    batch_start_time = time.time()
    # register_fonts() # ãƒ¡ã‚¤ãƒ³ãƒ—ãƒ­ã‚»ã‚¹é–‹å§‹æ™‚ã«ä¸€åº¦å®Ÿè¡Œ (ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚¤ãƒ³ãƒãƒ¼ãƒˆæ™‚ã«ã‚‚å®Ÿè¡Œã•ã‚Œã‚‹)

    if progress_callback: progress_callback(0.05, "ãƒ‡ãƒ¼ã‚¿ã‚’æº–å‚™ä¸­...")

    temp_dir_main = tempfile.mkdtemp()
    df_path_main = os.path.join(temp_dir_main, "main_data.feather")
    df_main.reset_index(drop=True).to_feather(df_path_main)
    
    target_data_path_main = None
    if target_data_main is not None and not target_data_main.empty:
        target_data_path_main = os.path.join(temp_dir_main, "target_data.feather")
        target_data_main.reset_index(drop=True).to_feather(target_data_path_main)

    # ãƒ¡ã‚¤ãƒ³ãƒ—ãƒ­ã‚»ã‚¹ã§ã®ã¿ä½¿ç”¨ã™ã‚‹ã‚­ãƒ£ãƒƒã‚·ãƒ¥ (pdf_generator.py ã‹ã‚‰å–å¾—)
    main_process_chart_cache = get_pdf_gen_main_process_cache()

    try:
        summaries_for_latest_date = generate_filtered_summaries(df_main)
        latest_date_for_batch = summaries_for_latest_date.get("latest_date", pd.Timestamp.now().normalize())
        
        if progress_callback: progress_callback(0.10, "PDFç”Ÿæˆã‚¿ã‚¹ã‚¯ã¨ã‚°ãƒ©ãƒ•ã‚’æº–å‚™ä¸­...")
        
        tasks_for_worker_with_buffers = []
        
        # ğŸ”§ æ–°ã—ã„ç›®æ¨™å€¤ãƒ•ã‚¡ã‚¤ãƒ«æ§‹é€ ã«å¯¾å¿œã—ãŸè¡¨ç¤ºåãƒãƒƒãƒ”ãƒ³ã‚°
        dept_display_map, ward_display_map = create_display_mapping_with_new_target_format(target_data_main)
        
        # æ—¢å­˜ã®ç—…æ£Ÿè¡¨ç¤ºåç”Ÿæˆãƒ­ã‚¸ãƒƒã‚¯ã¯ä¿æŒ
        unique_wards = df_main["ç—…æ£Ÿã‚³ãƒ¼ãƒ‰"].astype(str).unique()
        for ward in unique_wards:
            if ward not in ward_display_map:
                match = re.match(r'0*(\d+)([A-Za-z]*)', ward)
                if match: ward_display_map[ward] = f"{match.group(1)}{match.group(2)}ç—…æ£Ÿ"
                else: ward_display_map[ward] = ward
        
        # æ—¢å­˜ã®è¨ºç™‚ç§‘è¡¨ç¤ºåç”Ÿæˆãƒ­ã‚¸ãƒƒã‚¯ã¯ä¿æŒ
        unique_depts = df_main["è¨ºç™‚ç§‘å"].unique()
        for dept in unique_depts:
            if dept not in dept_display_map:
                 dept_display_map[dept] = dept

        # ğŸ”§ ã‚°ãƒ©ãƒ•æ—¥æ•°ãƒªã‚¹ãƒˆã‚’çµ±ä¸€
        graph_days_to_pre_generate = ["90"] if fast_mode else ["90", "180"]
        
        task_definitions_list = []
        if mode == "all_only_filter":
            task_definitions_list.append({"type": "all", "value": "å…¨ä½“", "display_name": "å…¨ä½“", "data_for_graphs": df_main.copy()})
        else:
            if mode == "all":
                task_definitions_list.append({"type": "all", "value": "å…¨ä½“", "display_name": "å…¨ä½“", "data_for_graphs": df_main.copy()})
            if mode == "all" or mode == "dept":
                for dept_val in unique_depts:
                    task_definitions_list.append({
                        "type": "dept", "value": dept_val, 
                        "display_name": dept_display_map.get(dept_val, dept_val),
                        "data_for_graphs": df_main[df_main["è¨ºç™‚ç§‘å"] == dept_val].copy()
                    })
            if mode == "all" or mode == "ward":
                for ward_val in unique_wards:
                    task_definitions_list.append({
                        "type": "ward", "value": ward_val, 
                        "display_name": ward_display_map.get(ward_val, ward_val),
                        "data_for_graphs": df_main[df_main["ç—…æ£Ÿã‚³ãƒ¼ãƒ‰"] == ward_val].copy()
                    })

        num_task_defs = len(task_definitions_list)
        for i, task_def_item in enumerate(task_definitions_list):
            graph_buffers_for_task = {"alos": {}, "patient_all": {}, "patient_weekday": {}, "patient_holiday": {}, "dual_axis": {}}
            data_for_current_task_graphs = task_def_item["data_for_graphs"]
            display_name_for_graphs = task_def_item["display_name"]
            
            target_all, target_weekday, target_holiday = get_targets_for_pdf(task_def_item["value"], task_def_item["type"], target_data_main)

            # ALOSã‚°ãƒ©ãƒ• - ğŸ”§ çµ±ä¸€ã•ã‚ŒãŸã‚°ãƒ©ãƒ•æ—¥æ•°ãƒªã‚¹ãƒˆã‚’ä½¿ç”¨
            for days_val_str in graph_days_to_pre_generate:
                days_val_int = int(days_val_str)
                key = get_pdf_gen_chart_cache_key(f"ALOS_{display_name_for_graphs}", days_val_int, None, "alos_pdf", compute_pdf_gen_data_hash(data_for_current_task_graphs))
                buffer_val = main_process_chart_cache.get(key)
                if buffer_val is None and not data_for_current_task_graphs.empty:
                    img_buf = create_alos_chart_for_pdf(data_for_current_task_graphs, display_name_for_graphs, latest_date_for_batch, 30, MATPLOTLIB_FONT_NAME, days_to_show=days_val_int)
                    if img_buf: 
                        buffer_val = img_buf.getvalue()
                        main_process_chart_cache[key] = buffer_val
                        img_buf.close()  # ğŸ”§ ãƒãƒƒãƒ•ã‚¡ã‚’é©åˆ‡ã«é–‰ã˜ã‚‹
                if buffer_val: 
                    graph_buffers_for_task["alos"][days_val_str] = buffer_val
            
            # æ‚£è€…æ•°æ¨ç§»ã‚°ãƒ©ãƒ• - ğŸ”§ çµ±ä¸€ã•ã‚ŒãŸã‚°ãƒ©ãƒ•æ—¥æ•°ãƒªã‚¹ãƒˆã‚’ä½¿ç”¨
            patient_chart_types = {"all": target_all, "weekday": target_weekday, "holiday": target_holiday}
            for type_key, target_val in patient_chart_types.items():
                data_subset = data_for_current_task_graphs
                if type_key == "weekday" and "å¹³æ—¥åˆ¤å®š" in data_for_current_task_graphs.columns: 
                    data_subset = data_for_current_task_graphs[data_for_current_task_graphs["å¹³æ—¥åˆ¤å®š"] == "å¹³æ—¥"]
                elif type_key == "holiday" and "å¹³æ—¥åˆ¤å®š" in data_for_current_task_graphs.columns: 
                    data_subset = data_for_current_task_graphs[data_for_current_task_graphs["å¹³æ—¥åˆ¤å®š"] == "ä¼‘æ—¥"]
                if data_subset.empty and type_key != "all": 
                    continue

                for days_val_str in graph_days_to_pre_generate:  # ğŸ”§ çµ±ä¸€ã•ã‚ŒãŸãƒªã‚¹ãƒˆã‚’ä½¿ç”¨
                    days_val_int = int(days_val_str)
                    key = get_pdf_gen_chart_cache_key(f"Patient_{type_key}_{display_name_for_graphs}", days_val_int, target_val, f"patient_{type_key}_pdf", compute_pdf_gen_data_hash(data_subset))
                    buffer_val = main_process_chart_cache.get(key)
                    if buffer_val is None and not data_subset.empty:
                        img_buf = create_patient_chart_with_target_wrapper(data_subset, title=f"{display_name_for_graphs} {type_key.capitalize()}æ¨ç§»({days_val_int}æ—¥)", days=days_val_int, target_value=target_val, font_name_for_mpl_to_use=MATPLOTLIB_FONT_NAME)
                        if img_buf: 
                            buffer_val = img_buf.getvalue()
                            main_process_chart_cache[key] = buffer_val
                            img_buf.close()  # ğŸ”§ ãƒãƒƒãƒ•ã‚¡ã‚’é©åˆ‡ã«é–‰ã˜ã‚‹
                    if buffer_val: 
                        graph_buffers_for_task[f"patient_{type_key}"][days_val_str] = buffer_val
            
            # äºŒè»¸ã‚°ãƒ©ãƒ• - ğŸ”§ çµ±ä¸€ã•ã‚ŒãŸã‚°ãƒ©ãƒ•æ—¥æ•°ãƒªã‚¹ãƒˆã‚’ä½¿ç”¨
            for days_val_str in graph_days_to_pre_generate:  # ğŸ”§ çµ±ä¸€ã•ã‚ŒãŸãƒªã‚¹ãƒˆã‚’ä½¿ç”¨
                days_val_int = int(days_val_str)
                key = get_pdf_gen_chart_cache_key(f"DualAxis_{display_name_for_graphs}", days_val_int, None, "dual_axis_pdf", compute_pdf_gen_data_hash(data_for_current_task_graphs))
                buffer_val = main_process_chart_cache.get(key)
                if buffer_val is None and not data_for_current_task_graphs.empty:
                    img_buf = create_dual_axis_chart_for_pdf(data_for_current_task_graphs, title=f"{display_name_for_graphs} æ‚£è€…ç§»å‹•({days_val_int}æ—¥)", days=days_val_int, font_name_for_mpl_to_use=MATPLOTLIB_FONT_NAME)
                    if img_buf: 
                        buffer_val = img_buf.getvalue()
                        main_process_chart_cache[key] = buffer_val
                        img_buf.close()  # ğŸ”§ ãƒãƒƒãƒ•ã‚¡ã‚’é©åˆ‡ã«é–‰ã˜ã‚‹
                if buffer_val: 
                    graph_buffers_for_task["dual_axis"][days_val_str] = buffer_val
            
            tasks_for_worker_with_buffers.append(
                (df_path_main, task_def_item["type"], task_def_item["value"], task_def_item["display_name"], 
                 latest_date_for_batch.isoformat(), landscape, target_data_path_main, fast_mode,
                 graph_buffers_for_task["alos"], 
                 {"all": graph_buffers_for_task["patient_all"], "weekday": graph_buffers_for_task["patient_weekday"], "holiday": graph_buffers_for_task["patient_holiday"]},
                 graph_buffers_for_task["dual_axis"])
            )
            if progress_callback and num_task_defs > 0:
                progress_val = int(10 + ( (i+1) / num_task_defs) * 15) # 10-25%
                progress_callback(progress_val / 100.0, f"ã‚°ãƒ©ãƒ•æº–å‚™ä¸­: {i+1}/{num_task_defs}")
        
        del df_main, target_data_main, task_definitions_list # ãƒ¡ãƒ¢ãƒªè§£æ”¾
        gc.collect()

        total_tasks_to_process = len(tasks_for_worker_with_buffers)
        if progress_callback: progress_callback(0.25, f"ã‚¿ã‚¹ã‚¯æº–å‚™å®Œäº† (åˆè¨ˆ: {total_tasks_to_process}ä»¶)")
        
        if max_workers is None:
            cpu_cores = multiprocessing.cpu_count()
            max_workers = max(1, min(cpu_cores -1 if cpu_cores > 1 else 1, 4))
        
        zip_archive_buffer = BytesIO()
        with zipfile.ZipFile(zip_archive_buffer, 'w', zipfile.ZIP_DEFLATED, compresslevel=6) as zipf_archive:
            date_suffix_str = latest_date_for_batch.strftime("%Y%m%d")
            pdfs_completed = 0
            
            if total_tasks_to_process == 0: # ã‚¿ã‚¹ã‚¯ãŒãªã„å ´åˆã¯ç©ºã®ZIP
                if progress_callback: progress_callback(1.0, "å‡¦ç†å¯¾è±¡ãªã—")
                print("ä¸€æ‹¬PDFç”Ÿæˆ: å‡¦ç†å¯¾è±¡ãªã—")
                zip_archive_buffer.seek(0)
                return zip_archive_buffer

            with multiprocessing.Pool(processes=max_workers) as pool_obj:
                pdf_results = pool_obj.starmap(process_pdf_in_worker_revised, tasks_for_worker_with_buffers)

            for result_item_pdf in pdf_results:
                if result_item_pdf:
                    title_from_worker, pdf_content_io_obj = result_item_pdf
                    if pdf_content_io_obj and pdf_content_io_obj.getbuffer().nbytes > 0:
                        safe_pdf_title = "".join(c if c.isalnum() or c in ['-', '_'] else '_' for c in title_from_worker)
                        folder_prefix = ""
                        if "è¨ºç™‚ç§‘åˆ¥" in title_from_worker: folder_prefix = "è¨ºç™‚ç§‘åˆ¥/"
                        elif "ç—…æ£Ÿåˆ¥" in title_from_worker: folder_prefix = "ç—…æ£Ÿåˆ¥/"
                        pdf_file_name_in_zip = f"{folder_prefix}å…¥é™¢æ‚£è€…æ•°äºˆæ¸¬_{safe_pdf_title}_{date_suffix_str}.pdf"
                        zipf_archive.writestr(pdf_file_name_in_zip, pdf_content_io_obj.getvalue())
                        pdfs_completed +=1
                        pdf_content_io_obj.close()
            
                if progress_callback and total_tasks_to_process > 0 :
                    current_progress_val = int(25 + (pdfs_completed / total_tasks_to_process) * 75)
                    progress_callback(min(100, current_progress_val) / 100.0, f"PDFç”Ÿæˆä¸­: {pdfs_completed}/{total_tasks_to_process} å®Œäº†")
            
        batch_end_time_main = time.time()
        total_batch_duration = batch_end_time_main - batch_start_time
        if progress_callback: progress_callback(1.0, f"å‡¦ç†å®Œäº†! ({pdfs_completed}ä»¶) æ‰€è¦æ™‚é–“: {total_batch_duration:.1f}ç§’")
        
        zip_archive_buffer.seek(0)
        return zip_archive_buffer

    except Exception as e_main_batch:
        print(f"ä¸€æ‹¬PDFç”Ÿæˆ(MP)ã®ãƒ¡ã‚¤ãƒ³å‡¦ç†ã§ã‚¨ãƒ©ãƒ¼: {e_main_batch}")
        import traceback
        print(traceback.format_exc())
        if progress_callback: progress_callback(1.0, f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e_main_batch)}")
        return BytesIO()
    finally:
        try:
            import shutil
            shutil.rmtree(temp_dir_main, ignore_errors=True)
        except Exception as e_cleanup:
            print(f"ä¸€æ™‚ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®å‰Šé™¤ã«å¤±æ•—: {e_cleanup}")

# ğŸš€ ãƒã‚¤ãƒ‘ãƒ¼æœ€é©åŒ–ã•ã‚ŒãŸPDFä¸€æ‹¬ç”Ÿæˆ
def batch_generate_pdfs_hyper_optimized(
    df_main, mode="all", landscape=False, target_data_main=None,
    progress_callback=None, max_workers=None, fast_mode=True
):
    """ãƒã‚¤ãƒ‘ãƒ¼æœ€é©åŒ–ã•ã‚ŒãŸPDFä¸€æ‹¬ç”Ÿæˆ"""
    start_time = time.time()
    
    if progress_callback:
        progress_callback(0.01, "ã‚·ã‚¹ãƒ†ãƒ æœ€é©åŒ–ä¸­...")
    
    # ã‚·ã‚¹ãƒ†ãƒ è¨­å®šã®æœ€é©åŒ–
    config = SystemOptimizer.get_optimal_config()
    
    if progress_callback:
        progress_callback(0.03, f"ãƒ‡ãƒ¼ã‚¿æœ€é©åŒ–ä¸­... (CPU:{config['cpu_cores']}ã‚³ã‚¢, RAM:{config['memory_gb']:.1f}GB)")
    
    # ãƒ‡ãƒ¼ã‚¿ã®æœ€é©åŒ–
    df_optimized = DataOptimizer.optimize_dataframe(df_main)
    
    # ä¸€æ™‚ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®æº–å‚™
    temp_dir = tempfile.mkdtemp()
    df_path = os.path.join(temp_dir, "hyper_optimized_data.feather")
    df_optimized.reset_index(drop=True).to_feather(df_path)
    
    target_data_path = None
    if target_data_main is not None and not target_data_main.empty:
        target_data_path = os.path.join(temp_dir, "target_data.feather")
        target_data_main.reset_index(drop=True).to_feather(target_data_path)
    
    try:
        # æœ€æ–°æ—¥ä»˜ã®å–å¾—
        summaries = generate_filtered_summaries(df_optimized)
        latest_date = summaries.get("latest_date", pd.Timestamp.now().normalize())
        
        if progress_callback:
            progress_callback(0.08, "ã‚¿ã‚¹ã‚¯æœ€é©åŒ–ä¸­...")
        
        # ğŸ”§ æ–°ã—ã„ç›®æ¨™å€¤ãƒ•ã‚¡ã‚¤ãƒ«æ§‹é€ ã«å¯¾å¿œã—ãŸè¡¨ç¤ºåãƒãƒƒãƒ”ãƒ³ã‚°
        dept_display_map, ward_display_map = create_display_mapping_with_new_target_format(target_data_main)
        
        # æ—¢å­˜ã®ç—…æ£Ÿè¡¨ç¤ºåç”Ÿæˆãƒ­ã‚¸ãƒƒã‚¯ã¯ä¿æŒ
        unique_wards = df_optimized["ç—…æ£Ÿã‚³ãƒ¼ãƒ‰"].astype(str).unique()
        for ward in unique_wards:
            if ward not in ward_display_map:
                match = re.match(r'0*(\d+)([A-Za-z]*)', ward)
                if match: ward_display_map[ward] = f"{match.group(1)}{match.group(2)}ç—…æ£Ÿ"
                else: ward_display_map[ward] = ward
        
        # æ—¢å­˜ã®è¨ºç™‚ç§‘è¡¨ç¤ºåç”Ÿæˆãƒ­ã‚¸ãƒƒã‚¯ã¯ä¿æŒ
        unique_depts = df_optimized["è¨ºç™‚ç§‘å"].unique()
        for dept in unique_depts:
            if dept not in dept_display_map:
                dept_display_map[dept] = dept

        task_definitions_list = []
        if mode == "all_only_filter":
            task_definitions_list.append({"type": "all", "value": "å…¨ä½“", "display_name": "å…¨ä½“"})
        else:
            if mode == "all":
                task_definitions_list.append({"type": "all", "value": "å…¨ä½“", "display_name": "å…¨ä½“"})
            if mode == "all" or mode == "dept":
                for dept_val in unique_depts:
                    task_definitions_list.append({
                        "type": "dept", "value": dept_val, 
                        "display_name": dept_display_map.get(dept_val, dept_val)
                    })
            if mode == "all" or mode == "ward":
                for ward_val in unique_wards:
                    task_definitions_list.append({
                        "type": "ward", "value": ward_val, 
                        "display_name": ward_display_map.get(ward_val, ward_val)
                    })

        if not task_definitions_list:
            return BytesIO()
        
        if progress_callback:
            progress_callback(0.15, f"ãƒã‚¤ãƒ‘ãƒ¼ä¸¦åˆ—å‡¦ç†é–‹å§‹ ({len(task_definitions_list)}ã‚¿ã‚¹ã‚¯, {config['max_workers']}ãƒ¯ãƒ¼ã‚«ãƒ¼)")
        
        # æœ€é©ãªãƒ¯ãƒ¼ã‚«ãƒ¼æ•°ã®æ±ºå®š
        if max_workers is None:
            max_workers = config['max_workers']
        
        # ZIPä½œæˆã¨ãƒ‡ãƒ¼ã‚¿åé›†
        zip_buffer = BytesIO()
        with zipfile.ZipFile(zip_buffer, 'w', zipfile.ZIP_DEFLATED, compresslevel=6) as zipf:
            date_suffix = latest_date.strftime("%Y%m%d")
            completed_count = 0
            total_tasks = len(task_definitions_list)
            
            # ãƒãƒ£ãƒ³ã‚¯å˜ä½ã§å‡¦ç†
            chunk_size = config['chunk_size']
            task_chunks = [task_definitions_list[i:i + chunk_size] for i in range(0, len(task_definitions_list), chunk_size)]
            
            # ProcessPoolExecutorã§ãƒã‚¤ãƒ‘ãƒ¼ä¸¦åˆ—å‡¦ç†
            with ProcessPoolExecutor(max_workers=max_workers) as executor:
                # å…¨ãƒãƒ£ãƒ³ã‚¯ã‚’ä¸¦åˆ—ã§ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒªãƒ³ã‚°
                future_to_chunk = {}
                for chunk in task_chunks:
                    future = executor.submit(
                        process_chunk_hyper_optimized,
                        chunk, df_path, landscape, target_data_path,
                        fast_mode, latest_date.isoformat()
                    )
                    future_to_chunk[future] = chunk
                
                # çµæœã®ä¸¦åˆ—åé›†ã¨ZIPæ›¸ãè¾¼ã¿
                for future in as_completed(future_to_chunk):
                    try:
                        chunk_results = future.result(timeout=900)  # 15åˆ†ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ
                        
                        for result in chunk_results:
                            if result:
                                title, pdf_content = result
                                if pdf_content and hasattr(pdf_content, 'getbuffer') and pdf_content.getbuffer().nbytes > 0:
                                    # å®‰å…¨ãªãƒ•ã‚¡ã‚¤ãƒ«åç”Ÿæˆ
                                    safe_title = "".join(
                                        c if c.isalnum() or c in ['-', '_'] else '_'
                                        for c in title
                                    )
                                    
                                    # ãƒ•ã‚©ãƒ«ãƒ€æ§‹é€ 
                                    folder = ""
                                    if "è¨ºç™‚ç§‘åˆ¥" in title:
                                        folder = "è¨ºç™‚ç§‘åˆ¥/"
                                    elif "ç—…æ£Ÿåˆ¥" in title:
                                        folder = "ç—…æ£Ÿåˆ¥/"
                                    
                                    filename = f"{folder}å…¥é™¢æ‚£è€…æ•°äºˆæ¸¬_{safe_title}_{date_suffix}.pdf"
                                    zipf.writestr(filename, pdf_content.getvalue())
                                    pdf_content.close()
                                    completed_count += 1
                        
                        # ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹æ›´æ–°
                        if progress_callback and total_tasks > 0:
                            progress = int(15 + (completed_count / total_tasks) * 80)
                            progress_callback(
                                min(95, progress) / 100.0,
                                f"ãƒã‚¤ãƒ‘ãƒ¼å‡¦ç†ä¸­: {completed_count}/{total_tasks} å®Œäº† (ãƒ¡ãƒ¢ãƒª: {SystemOptimizer.check_memory_usage():.1f}%)"
                            )
                    
                    except Exception as e:
                        print(f"Hyper processing error for chunk: {e}")
                        continue
        
        end_time = time.time()
        duration = end_time - start_time
        
        if progress_callback:
            rate = completed_count / duration if duration > 0 else 0
            progress_callback(
                1.0,
                f"ãƒã‚¤ãƒ‘ãƒ¼å‡¦ç†å®Œäº†! {completed_count}ä»¶ã®PDFã‚’{duration:.1f}ç§’ã§ç”Ÿæˆ ({rate:.1f}ä»¶/ç§’)"
            )
        
        zip_buffer.seek(0)
        return zip_buffer
        
    except Exception as e:
        print(f"Hyper optimized batch generation error: {e}")
        import traceback
        print(traceback.format_exc())
        if progress_callback:
            progress_callback(1.0, f"ã‚¨ãƒ©ãƒ¼: {str(e)}")
        return BytesIO()
    
    finally:
        # å¾¹åº•çš„ãªã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
        try:
            import shutil
            shutil.rmtree(temp_dir, ignore_errors=True)
        except:
            pass
        SystemOptimizer.force_cleanup()

def process_chunk_hyper_optimized(task_chunk, df_path, landscape, target_data_path, fast_mode, latest_date_str):
    """ãƒã‚¤ãƒ‘ãƒ¼æœ€é©åŒ–ã•ã‚ŒãŸãƒãƒ£ãƒ³ã‚¯å‡¦ç†"""
    results = []
    
    try:
        for task in task_chunk:
            result = process_pdf_in_worker_revised(
                df_path, task["type"], task["value"], task["display_name"],
                latest_date_str, landscape, target_data_path, fast_mode,
                None, None, None  # ã‚°ãƒ©ãƒ•ãƒãƒƒãƒ•ã‚¡ã¯å¾Œã§æœ€é©åŒ–
            )
            
            if result:
                results.append(result)
            
            # ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ãƒã‚§ãƒƒã‚¯
            if SystemOptimizer.check_memory_usage() > MAX_MEMORY_USAGE_PERCENT:
                SystemOptimizer.force_cleanup()
        
        return results
        
    except Exception as e:
        print(f"Chunk processing error: {e}")
        return []
    finally:
        gc.collect()

# ğŸ”§ æ—¢å­˜ã®batch_generate_pdfs_full_optimizedé–¢æ•°ã‚’ä¿®æ­£
def batch_generate_pdfs_full_optimized(
    df, mode="all", landscape=False, target_data=None, 
    progress_callback=None, use_parallel=True, max_workers=None, fast_mode=True,
    use_hyper_optimization=False  # ğŸš€ æ–°ã—ã„ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’è¿½åŠ 
    ):
    if df is None or df.empty:
        if progress_callback: progress_callback(0, "ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
        print("batch_generate_pdfs_full_optimized: åˆ†æå¯¾è±¡ã®ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ãŒç©ºã§ã™ã€‚")
        return BytesIO()

    if progress_callback: progress_callback(0, "å‡¦ç†é–‹å§‹...")
    
    if use_parallel:
        if use_hyper_optimization:
            # ğŸš€ ãƒã‚¤ãƒ‘ãƒ¼æœ€é©åŒ–ç‰ˆã‚’ä½¿ç”¨
            return batch_generate_pdfs_hyper_optimized(df, mode, landscape, target_data, progress_callback, max_workers, fast_mode)
        else:
            # æ—¢å­˜ã®æœ€é©åŒ–ç‰ˆã‚’ä½¿ç”¨
            return batch_generate_pdfs_mp_optimized(df, mode, landscape, target_data, progress_callback, max_workers, fast_mode)
    else:
        # ã‚·ãƒ³ã‚°ãƒ«ãƒ—ãƒ­ã‚»ã‚¹ç‰ˆ (ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã¾ãŸã¯ãƒ‡ãƒãƒƒã‚°ç”¨)
        print("Parallel processing disabled. Using sequential PDF generation.")
        temp_dir_seq = tempfile.mkdtemp()
        df_path_seq = os.path.join(temp_dir_seq, "main_data_seq.feather")
        df.reset_index(drop=True).to_feather(df_path_seq)
        target_data_path_seq = None
        if target_data is not None and not target_data.empty:
            target_data_path_seq = os.path.join(temp_dir_seq, "target_data_seq.feather")
            target_data.reset_index(drop=True).to_feather(target_data_path_seq)

        all_summaries = generate_filtered_summaries(df)
        latest_date_seq = all_summaries.get("latest_date", pd.Timestamp.now().normalize())
        
        tasks_seq = []
        # ğŸ”§ æ–°ã—ã„ç›®æ¨™å€¤ãƒ•ã‚¡ã‚¤ãƒ«æ§‹é€ ã«å¯¾å¿œã—ãŸè¡¨ç¤ºåãƒãƒƒãƒ”ãƒ³ã‚°
        dept_display_map_seq, ward_display_map_seq = create_display_mapping_with_new_target_format(target_data)
        
        # ç—…æ£Ÿè¡¨ç¤ºåã®ç”Ÿæˆ
        for ward in df["ç—…æ£Ÿã‚³ãƒ¼ãƒ‰"].astype(str).unique():
            if ward not in ward_display_map_seq:
                match = re.match(r'0*(\d+)([A-Za-z]*)', ward)
                if match: ward_display_map_seq[ward] = f"{match.group(1)}{match.group(2)}ç—…æ£Ÿ"
                else: ward_display_map_seq[ward] = ward
        
        # è¨ºç™‚ç§‘è¡¨ç¤ºåã®è¨­å®š
        for dept in df["è¨ºç™‚ç§‘å"].unique():
            if dept not in dept_display_map_seq:
                dept_display_map_seq[dept] = dept

        if mode == "all_only_filter": tasks_seq.append({"type": "all", "value": "å…¨ä½“", "display_name": "å…¨ä½“"})
        else:
            if mode == "all": tasks_seq.append({"type": "all", "value": "å…¨ä½“", "display_name": "å…¨ä½“"})
            if mode == "all" or mode == "dept":
                for dept in sorted(df["è¨ºç™‚ç§‘å"].unique()): tasks_seq.append({"type": "dept", "value": dept, "display_name": dept_display_map_seq.get(dept, dept)})
            if mode == "all" or mode == "ward":
                for ward in sorted(df["ç—…æ£Ÿã‚³ãƒ¼ãƒ‰"].astype(str).unique()): tasks_seq.append({"type": "ward", "value": ward, "display_name": ward_display_map_seq.get(ward, ward)})

        zip_buffer_seq = BytesIO()
        with zipfile.ZipFile(zip_buffer_seq, 'w', zipfile.ZIP_DEFLATED) as zipf_seq:
            date_suffix_seq = latest_date_seq.strftime("%Y%m%d")
            completed_seq = 0
            total_seq = len(tasks_seq)
            for task_item in tasks_seq:
                # ã‚°ãƒ©ãƒ•ãƒãƒƒãƒ•ã‚¡ã¯ã“ã“ã§ã¯ç”Ÿæˆã—ãªã„ (process_pdf_in_worker_revised ãŒå†…éƒ¨ã§ç”Ÿæˆã™ã‚‹)
                # process_pdf_in_worker_revised ã¯Featherãƒ‘ã‚¹ã¨ã‚°ãƒ©ãƒ•ãƒãƒƒãƒ•ã‚¡å¼•æ•°ã‚’æœŸå¾…ã™ã‚‹
                # ã‚·ãƒ³ã‚°ãƒ«ãƒ—ãƒ­ã‚»ã‚¹ã§ã¯ã‚°ãƒ©ãƒ•ãƒãƒƒãƒ•ã‚¡ã‚’Noneã¨ã—ã¦æ¸¡ã™ã‹ã€process_pdf_in_worker_revisedã‚’ä¿®æ­£
                # ã¾ãŸã¯ã€ã‚·ãƒ³ã‚°ãƒ«ãƒ—ãƒ­ã‚»ã‚¹ç”¨ã®åˆ¥é–¢æ•°ã‚’ç”¨æ„ã™ã‚‹æ–¹ãŒè‰¯ã„ã€‚
                # ã“ã“ã§ã¯ã€process_pdf_in_worker_revised ã‚’ãã®ã¾ã¾ä½¿ã„ã€ãƒãƒƒãƒ•ã‚¡ã¯ None ã¨ã™ã‚‹ã€‚
                # ãã®å ´åˆã€process_pdf_in_worker_revised å´ã§ãƒãƒƒãƒ•ã‚¡ãŒNoneã®å ´åˆã®å‡¦ç†ãŒå¿…è¦ã«ãªã‚‹ã€‚
                # (ã¾ãŸã¯ã€ã‚·ãƒ³ã‚°ãƒ«ãƒ—ãƒ­ã‚»ã‚¹ã§ã¯ã‚°ãƒ©ãƒ•ç”Ÿæˆã‚’ process_pdf_in_worker_revised ã«ä»»ã›ã‚‹)

                # ã‚·ãƒ³ã‚°ãƒ«ãƒ—ãƒ­ã‚»ã‚¹å®Ÿè¡Œã®ãŸã‚ã«ã€ã‚°ãƒ©ãƒ•ã‚’éƒ½åº¦ç”Ÿæˆã™ã‚‹ã€‚
                # ã“ã®éƒ¨åˆ†ã¯ã€ãƒ¡ã‚¤ãƒ³ãƒ—ãƒ­ã‚»ã‚¹ã®ã‚°ãƒ©ãƒ•ç”Ÿæˆãƒ­ã‚¸ãƒƒã‚¯ã‚’ã“ã“ã«ç§»æ¤ã™ã‚‹å¿…è¦ãŒã‚ã‚‹ã€‚
                # ç°¡å˜ã®ãŸã‚ã€ALOSã‚°ãƒ©ãƒ•ã®ã¿ã‚’ä»®ã«ç”Ÿæˆã—ã¦æ¸¡ã™å½¢ã«ã™ã‚‹ã€‚
                # **æ³¨æ„:** ã“ã®ã‚·ãƒ³ã‚°ãƒ«ãƒ—ãƒ­ã‚»ã‚¹ç‰ˆã¯ã€ã‚°ãƒ©ãƒ•äº‹å‰ç”Ÿæˆã¨ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã®æ©æµã‚’å—ã‘ãªã„ãŸã‚ã€
                #           ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãŒå¤§å¹…ã«åŠ£ã‚‹ã€‚ã‚ãã¾ã§ãƒ‡ãƒãƒƒã‚°ç”¨ã‚„ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã€‚

                current_task_data_seq = df.copy()
                if task_item["type"] == "dept": current_task_data_seq = df[df["è¨ºç™‚ç§‘å"] == task_item["value"]].copy()
                elif task_item["type"] == "ward": current_task_data_seq = df[df["ç—…æ£Ÿã‚³ãƒ¼ãƒ‰"] == task_item["value"]].copy()

                alos_bufs_seq = {}
                if not current_task_data_seq.empty:
                     for days_str_seq in (["90"] if fast_mode else ["90", "180"]):
                        buf_io = create_alos_chart_for_pdf(current_task_data_seq, task_item["display_name"], latest_date_seq, 30, MATPLOTLIB_FONT_NAME, days_to_show=int(days_str_seq))
                        if buf_io: alos_bufs_seq[days_str_seq] = buf_io.getvalue() # ãƒã‚¤ãƒˆåˆ—ã‚’ä¿å­˜
                
                # patient_chart_buffers ã¨ dual_axis_chart_buffers ã‚‚åŒæ§˜ã«ç”ŸæˆãŒå¿…è¦ã ãŒã€ç°¡ç•¥åŒ–ã®ãŸã‚çœç•¥ã€‚
                # å®Ÿéš›ã«ã¯ãƒ¡ã‚¤ãƒ³ãƒ—ãƒ­ã‚»ã‚¹ã¨åŒã˜ãƒ­ã‚¸ãƒƒã‚¯ã§ã“ã‚Œã‚‰ã‚‚ç”Ÿæˆã—ã¦æ¸¡ã™ã€‚
                patient_bufs_seq = {"all": {}, "weekday": {}, "holiday": {}} # ãƒ€ãƒŸãƒ¼
                dual_bufs_seq = {} # ãƒ€ãƒŸãƒ¼


                result_seq = process_pdf_in_worker_revised(
                    df_path_seq, task_item["type"], task_item["value"], task_item["display_name"],
                    latest_date_seq.isoformat(), landscape, target_data_path_seq, fast_mode,
                    alos_chart_buffers_payload=alos_bufs_seq, # ãƒã‚¤ãƒˆåˆ—ã®è¾æ›¸
                    patient_chart_buffers_payload=patient_bufs_seq, # ãƒ€ãƒŸãƒ¼
                    dual_axis_chart_buffers_payload=dual_bufs_seq   # ãƒ€ãƒŸãƒ¼
                )
                if result_seq:
                    title_res_seq, pdf_io_seq = result_seq
                    if pdf_io_seq and pdf_io_seq.getbuffer().nbytes > 0:
                        safe_title_seq = "".join(c if c.isalnum() else '_' for c in title_res_seq)
                        folder_seq = "è¨ºç™‚ç§‘åˆ¥/" if "è¨ºç™‚ç§‘åˆ¥" in title_res_seq else ("ç—…æ£Ÿåˆ¥/" if "ç—…æ£Ÿåˆ¥" in title_res_seq else "")
                        zipf_seq.writestr(f"{folder_seq}å…¥é™¢æ‚£è€…æ•°äºˆæ¸¬_{safe_title_seq}_{date_suffix_seq}.pdf", pdf_io_seq.getvalue())
                        completed_seq += 1
                if progress_callback: progress_callback( (completed_seq/total_seq) if total_seq > 0 else 1, f"PDFç”Ÿæˆä¸­ (é †æ¬¡): {completed_seq}/{total_seq}")
        
        try: 
            import shutil
            shutil.rmtree(temp_dir_seq, ignore_errors=True)
        except Exception: 
            pass
        zip_buffer_seq.seek(0)
        return zip_buffer_seq