import warnings
warnings.filterwarnings('ignore', category=FutureWarning, module='pandas')
import streamlit as st
import pandas as pd
import numpy as np
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots
from datetime import datetime, timedelta
import time
import hashlib
import gc
import logging

# çµ±ä¸€ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼é–¢é€£ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from unified_filters import (
    create_unified_filter_sidebar,
    apply_unified_filters,
    get_unified_filter_summary,
    initialize_unified_filters,
    validate_unified_filters,
    get_unified_filter_config
)

# ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£é–¢æ•°ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from utils import safe_date_filter

# æ—¢å­˜ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‹ã‚‰ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
try:
    from alos_analysis_tab import display_alos_analysis_tab
    from dow_analysis_tab import display_dow_analysis_tab
    from individual_analysis_tab import display_individual_analysis_tab
    from forecast_analysis_tab import display_forecast_analysis_tab
    from chart import (
        create_interactive_patient_chart,
        create_interactive_dual_axis_chart,
        create_forecast_comparison_chart
    )
    from pdf_generator import create_pdf, create_landscape_pdf
    from forecast import generate_filtered_summaries, create_forecast_dataframe
    from kpi_calculator import calculate_kpis, analyze_kpi_insights
    from utils import get_display_name_for_dept
except ImportError as e:
    st.error(f"å¿…è¦ãªãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
    display_alos_analysis_tab = None
    display_dow_analysis_tab = None
    display_individual_analysis_tab = None
    display_forecast_analysis_tab = None
    create_interactive_patient_chart = None
    create_interactive_dual_axis_chart = None
    create_forecast_comparison_chart = None
    create_pdf = None
    create_landscape_pdf = None
    generate_filtered_summaries = None
    create_forecast_dataframe = None
    calculate_kpis = None
    analyze_kpi_insights = None
    get_display_name_for_dept = None
    get_unified_filter_config = None

logger = logging.getLogger(__name__)

# ===============================================================================
# ãƒ¡ã‚¤ãƒ³é–¢æ•°ç¾¤ï¼ˆçµ±ä¸€ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼å¯¾å¿œç‰ˆï¼‰
# ===============================================================================
def create_detailed_analysis_tab():
    """è©³ç´°åˆ†æã‚¿ãƒ–ã®ãƒ¡ã‚¤ãƒ³é–¢æ•°ï¼ˆçµ±ä¸€ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼å¯¾å¿œç‰ˆï¼‰"""
    st.header("ğŸ“ˆ è©³ç´°åˆ†æ")

    if not st.session_state.get('data_processed', False):
        st.warning("ã¾ãšã€Œãƒ‡ãƒ¼ã‚¿å‡¦ç†ã€ã‚¿ãƒ–ã§ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã‚“ã§ãã ã•ã„ã€‚")
        return

    df = st.session_state.get('df')
    if df is None or df.empty:
        st.error("åˆ†æå¯¾è±¡ã®ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
        return

    if get_unified_filter_config is None:
        st.error("çµ±ä¸€ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼æ©Ÿèƒ½ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚unified_filters.py ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
        return

    initialize_unified_filters(df)
    filter_config = create_unified_filter_sidebar(df)
    if filter_config is None:
        return

    is_valid, validation_message = validate_unified_filters(df)
    if not is_valid:
        st.error(f"ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼è¨­å®šã‚¨ãƒ©ãƒ¼: {validation_message}")
        return

    df_filtered = apply_unified_filters(df)

    if df_filtered.empty:
        filter_summary_on_empty = get_unified_filter_summary()
        st.info(f"ğŸ” {filter_summary_on_empty}")
        st.warning("é¸æŠã•ã‚ŒãŸãƒ•ã‚£ãƒ«ã‚¿ãƒ¼æ¡ä»¶ã«ãƒãƒƒãƒã™ã‚‹ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
        return

    filter_summary = get_unified_filter_summary()
    data_count = len(df_filtered)
    st.info(f"ğŸ” {filter_summary}")
    st.success(f"ğŸ“Š è©²å½“ãƒ‡ãƒ¼ã‚¿: {data_count:,}è¡Œ")

    common_config = st.session_state.get('common_config', {})

    los_tab, weekday_tab, individual_tab = st.tabs([
        "ğŸ“Š å¹³å‡åœ¨é™¢æ—¥æ•°åˆ†æ",
        "ğŸ“… æ›œæ—¥åˆ¥å…¥é€€é™¢åˆ†æ",
        "ğŸ” å€‹åˆ¥åˆ†æ"
    ])

    with los_tab:
        create_los_analysis_section(df_filtered, filter_config, common_config)

    with weekday_tab:
        create_weekday_analysis_section(df_filtered, filter_config, common_config)

    with individual_tab:
        create_individual_analysis_section(df_filtered, filter_config)

def create_data_tables_tab():
    """ãƒ‡ãƒ¼ã‚¿ãƒ†ãƒ¼ãƒ–ãƒ«ã‚¿ãƒ–ã®ãƒ¡ã‚¤ãƒ³é–¢æ•°ï¼ˆçµ±ä¸€ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼å¯¾å¿œç‰ˆï¼‰"""
    st.header("ğŸ“‹ ãƒ‡ãƒ¼ã‚¿ãƒ†ãƒ¼ãƒ–ãƒ«")

    if not st.session_state.get('data_processed', False):
        st.warning("ã¾ãšã€Œãƒ‡ãƒ¼ã‚¿å‡¦ç†ã€ã‚¿ãƒ–ã§ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã‚“ã§ãã ã•ã„ã€‚")
        return

    df_original = st.session_state.get('df')
    if df_original is None or df_original.empty:
        st.error("åˆ†æå¯¾è±¡ã®ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
        return

    if get_unified_filter_config is None:
        st.error("çµ±ä¸€ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼æ©Ÿèƒ½ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚unified_filters.py ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
        return
    initialize_unified_filters(df_original)

    # çµ±ä¸€ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ã‚’é©ç”¨ã™ã‚‹ãŸã‚ã« filter_config ã‚’å–å¾— (ã‚µã‚¤ãƒ‰ãƒãƒ¼è¡¨ç¤ºã¯ãƒ¡ã‚¤ãƒ³ã® create_detailed_analysis_tab ã§è¡Œã‚ã‚Œã‚‹æƒ³å®š)
    # ã‚‚ã—ã“ã®ã‚¿ãƒ–å˜ç‹¬ã§ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ã‚’æ“ä½œã•ã›ãŸã„å ´åˆã¯ create_unified_filter_sidebar ã‚’å‘¼ã¶å¿…è¦ãŒã‚ã‚‹ãŒã€
    # é€šå¸¸ã¯ãƒ¡ã‚¤ãƒ³ã®ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼è¨­å®šãŒå¼•ãç¶™ãŒã‚Œã‚‹ã¹ãã€‚
    # get_unified_filter_config() ã§ç¾åœ¨ã®ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼è¨­å®šã‚’å–å¾—ã§ãã‚‹ã€‚
    current_filter_config = get_unified_filter_config()
    if current_filter_config is None:
        # ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ãŒã¾ã è¨­å®šã•ã‚Œã¦ã„ãªã„å ´åˆï¼ˆåˆå›ã‚¢ã‚¯ã‚»ã‚¹ãªã©ï¼‰ã€
        # create_unified_filter_sidebar ã‚’å‘¼ã‚“ã§åˆæœŸåŒ–ãƒ»è¨­å®šã‚’ä¿ƒã™ã‹ã€
        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ã‚’é©ç”¨ã™ã‚‹ã€‚ã“ã“ã§ã¯ apply_unified_filters ãŒã‚ˆã—ãªã«æ‰±ã†ã“ã¨ã‚’æœŸå¾…ã€‚
        pass


    df_filtered = apply_unified_filters(df_original)
    filter_summary = get_unified_filter_summary()
    st.info(f"ğŸ” {filter_summary}")

    if df_filtered.empty:
        st.warning("é¸æŠã•ã‚ŒãŸãƒ•ã‚£ãƒ«ã‚¿ãƒ¼æ¡ä»¶ã«ãƒãƒƒãƒã™ã‚‹ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
        return

    ward_table_tab, dept_table_tab = st.tabs([
        "ğŸ¥ ç—…æ£Ÿåˆ¥ãƒ‡ãƒ¼ã‚¿ãƒ†ãƒ¼ãƒ–ãƒ«",
        "ğŸ©º è¨ºç™‚ç§‘åˆ¥ãƒ‡ãƒ¼ã‚¿ãƒ†ãƒ¼ãƒ–ãƒ«"
    ])

    with ward_table_tab:
        create_ward_table_section(df_filtered)

    with dept_table_tab:
        create_department_table_section(df_filtered)


def create_output_prediction_tab():
    """å‡ºåŠ›ãƒ»äºˆæ¸¬ã‚¿ãƒ–ã®ãƒ¡ã‚¤ãƒ³é–¢æ•°ï¼ˆçµ±ä¸€ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼å¯¾å¿œç‰ˆï¼‰"""
    st.header("ğŸ“„ å‡ºåŠ›ãƒ»äºˆæ¸¬")

    if not st.session_state.get('data_processed', False):
        st.warning("ã¾ãšã€Œãƒ‡ãƒ¼ã‚¿å‡¦ç†ã€ã‚¿ãƒ–ã§ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã‚“ã§ãã ã•ã„ã€‚")
        return

    df_original = st.session_state.get('df')
    if df_original is None or df_original.empty:
        st.error("åˆ†æå¯¾è±¡ã®ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
        return

    if get_unified_filter_config is None:
        st.error("çµ±ä¸€ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼æ©Ÿèƒ½ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚unified_filters.py ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
        return
    initialize_unified_filters(df_original)

    df_filtered = apply_unified_filters(df_original)
    filter_summary = get_unified_filter_summary()
    st.info(f"ğŸ” å‡ºåŠ›ãƒ»äºˆæ¸¬æœŸé–“: {filter_summary}")

    if df_filtered.empty:
        st.warning("é¸æŠã•ã‚ŒãŸãƒ•ã‚£ãƒ«ã‚¿ãƒ¼æ¡ä»¶ã«ãƒãƒƒãƒã™ã‚‹ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
        return

    individual_pdf_tab, bulk_pdf_tab, prediction_tab = st.tabs([
        "ğŸ“„ å€‹åˆ¥PDFå‡ºåŠ›",
        "ğŸ“š ä¸€æ‹¬PDFå‡ºåŠ›",
        "ğŸ”® äºˆæ¸¬åˆ†æ"
    ])

    with individual_pdf_tab:
        create_individual_pdf_section(df_filtered)

    with bulk_pdf_tab:
        create_bulk_pdf_section(df_filtered)

    with prediction_tab:
        create_prediction_analysis_section(df_filtered)

# ===============================================================================
# è©³ç´°åˆ†æã‚»ã‚¯ã‚·ãƒ§ãƒ³ï¼ˆçµ±ä¸€ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼å¯¾å¿œç‰ˆï¼‰
# ===============================================================================

def create_los_analysis_section(df_filtered, filter_config, common_config):
    """å¹³å‡åœ¨é™¢æ—¥æ•°åˆ†æã‚»ã‚¯ã‚·ãƒ§ãƒ³ï¼ˆçµ±ä¸€ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼å¯¾å¿œç‰ˆï¼‰"""
    st.subheader("ğŸ“Š å¹³å‡åœ¨é™¢æ—¥æ•°åˆ†æ")

    if display_alos_analysis_tab:
        try:
            start_date = filter_config.get('start_date')
            end_date = filter_config.get('end_date')

            if start_date is None or end_date is None:
                st.error("æœŸé–“è¨­å®šãŒãƒ•ã‚£ãƒ«ã‚¿ãƒ¼è¨­å®šã«å«ã¾ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
                logger.error("LOSåˆ†æ: filter_configã‹ã‚‰start_dateã¾ãŸã¯end_dateãŒå–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚")
                return

            if not isinstance(start_date, pd.Timestamp):
                start_date = pd.Timestamp(start_date)
            if not isinstance(end_date, pd.Timestamp):
                end_date = pd.Timestamp(end_date)

            display_alos_analysis_tab(df_filtered, start_date, end_date, common_config)

        except Exception as e:
            logger.error(f"å¹³å‡åœ¨é™¢æ—¥æ•°åˆ†æã§ã‚¨ãƒ©ãƒ¼: {e}", exc_info=True)
            st.error(f"å¹³å‡åœ¨é™¢æ—¥æ•°åˆ†æã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
            st.info("è©³ç´°ãªã‚¨ãƒ©ãƒ¼æƒ…å ±ã¯ãƒ­ã‚°ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
    else:
        st.warning("å¹³å‡åœ¨é™¢æ—¥æ•°åˆ†ææ©Ÿèƒ½ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚alos_analysis_tab.pyã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
        create_fallback_los_analysis(df_filtered, filter_config)

def create_weekday_analysis_section(df_filtered, filter_config, common_config):
    """æ›œæ—¥åˆ¥åˆ†æã‚»ã‚¯ã‚·ãƒ§ãƒ³ï¼ˆçµ±ä¸€ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼å¯¾å¿œç‰ˆï¼‰"""
    st.subheader("ğŸ“… æ›œæ—¥åˆ¥å…¥é€€é™¢åˆ†æ")

    if display_dow_analysis_tab:
        try:
            start_date = filter_config.get('start_date')
            end_date = filter_config.get('end_date')

            if start_date is None or end_date is None:
                st.error("æœŸé–“è¨­å®šãŒãƒ•ã‚£ãƒ«ã‚¿ãƒ¼è¨­å®šã«å«ã¾ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
                logger.error("æ›œæ—¥åˆ¥åˆ†æ: filter_configã‹ã‚‰start_dateã¾ãŸã¯end_dateãŒå–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚")
                return

            if not isinstance(start_date, pd.Timestamp):
                start_date = pd.Timestamp(start_date)
            if not isinstance(end_date, pd.Timestamp):
                end_date = pd.Timestamp(end_date)

            display_dow_analysis_tab(df_filtered, start_date, end_date, common_config)
        except Exception as e:
            logger.error(f"æ›œæ—¥åˆ¥åˆ†æã§ã‚¨ãƒ©ãƒ¼: {e}", exc_info=True)
            st.error(f"æ›œæ—¥åˆ¥åˆ†æã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
            st.info("è©³ç´°ãªã‚¨ãƒ©ãƒ¼æƒ…å ±ã¯ãƒ­ã‚°ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
    else:
        st.warning("æ›œæ—¥åˆ¥åˆ†ææ©Ÿèƒ½ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚dow_analysis_tab.pyã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
        create_fallback_dow_analysis(df_filtered, filter_config)

def create_individual_analysis_section(df_filtered, filter_config_from_caller):
    """å€‹åˆ¥åˆ†æã‚»ã‚¯ã‚·ãƒ§ãƒ³ï¼ˆçµ±ä¸€ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼å¯¾å¿œç‰ˆï¼‰"""
    st.subheader("ğŸ” å€‹åˆ¥åˆ†æ")

    if display_individual_analysis_tab:
        original_df_in_session = st.session_state.get('df')
        original_all_results = st.session_state.get('all_results') # å…ƒã®all_resultsã‚’ä¿æŒ
        original_latest_date_str = st.session_state.get('latest_data_date_str') # å…ƒã®latest_data_date_strã‚’ä¿æŒ


        # ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ã«åŸºã¥ã„ã¦ all_results ã‚’ç”Ÿæˆ/è¨­å®š
        if generate_filtered_summaries and df_filtered is not None and not df_filtered.empty:
            st.session_state.all_results = generate_filtered_summaries(df_filtered, None, None)
        else:
            st.session_state.all_results = None

        # ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ã«åŸºã¥ã„ã¦ latest_data_date_str ã‚’è¨­å®š
        if df_filtered is not None and not df_filtered.empty and 'æ—¥ä»˜' in df_filtered.columns:
            st.session_state.latest_data_date_str = df_filtered['æ—¥ä»˜'].max().strftime("%Yå¹´%mæœˆ%dæ—¥")
        elif original_latest_date_str: # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã¨ã—ã¦å…ƒã®å€¤
             st.session_state.latest_data_date_str = original_latest_date_str
        else: # ãã‚Œã‚‚ãªã‘ã‚Œã°ç¾åœ¨æ™‚åˆ»
            st.session_state.latest_data_date_str = pd.Timestamp.now().strftime("%Yå¹´%mæœˆ%dæ—¥")


        st.session_state['df'] = df_filtered
        st.session_state['unified_filter_applied'] = True
        # filter_config_from_caller ã¯ individual_analysis_tab.py ãŒ get_unified_filter_config() ã§å–å¾—ã™ã‚‹ãŸã‚ã€
        # ã“ã“ã§ã‚»ãƒƒã‚·ãƒ§ãƒ³ã« 'current_filter_config' ã¨ã—ã¦ä¿å­˜ã™ã‚‹å¿…è¦ã¯ãªã„ã€‚
        # get_unified_filter_config() ãŒæ­£ã—ã filter_config_from_caller (ã¾ãŸã¯åŒç­‰ã®ã‚‚ã®) ã‚’è¿”ã™ã‚ˆã†ã«
        # unified_filters.py ãŒ st.session_state[self.config_key] ã«ä¿å­˜ã—ã¦ã„ã‚‹ã“ã¨ãŒå‰æã€‚

        try:
            display_individual_analysis_tab()
        except Exception as e:
            logger.error(f"å€‹åˆ¥åˆ†æã§ã‚¨ãƒ©ãƒ¼: {e}", exc_info=True)
            st.error(f"å€‹åˆ¥åˆ†æã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
            st.info("è©³ç´°ãªã‚¨ãƒ©ãƒ¼æƒ…å ±ã¯ãƒ­ã‚°ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
        finally:
            st.session_state['df'] = original_df_in_session
            st.session_state['unified_filter_applied'] = False
            st.session_state['all_results'] = original_all_results # å…ƒã®all_resultsã«æˆ»ã™
            st.session_state['latest_data_date_str'] = original_latest_date_str # å…ƒã®æ—¥ä»˜æ–‡å­—åˆ—ã«æˆ»ã™

    else:
        st.warning("å€‹åˆ¥åˆ†ææ©Ÿèƒ½ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚individual_analysis_tab.pyã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
        create_fallback_individual_analysis(df_filtered, filter_config_from_caller)

# ===============================================================================
# ãƒ‡ãƒ¼ã‚¿ãƒ†ãƒ¼ãƒ–ãƒ«ã‚»ã‚¯ã‚·ãƒ§ãƒ³ï¼ˆçµ±ä¸€ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼å¯¾å¿œç‰ˆï¼‰
# ===============================================================================

def create_ward_table_section(df_filtered):
    """ç—…æ£Ÿåˆ¥ãƒ‡ãƒ¼ã‚¿ãƒ†ãƒ¼ãƒ–ãƒ«ã‚»ã‚¯ã‚·ãƒ§ãƒ³ï¼ˆçµ±ä¸€ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼å¯¾å¿œç‰ˆï¼‰"""
    st.subheader("ğŸ¥ ç—…æ£Ÿåˆ¥ãƒ‡ãƒ¼ã‚¿ãƒ†ãƒ¼ãƒ–ãƒ«")
    
    try:
        if df_filtered.empty:
            st.warning("æŒ‡å®šã•ã‚ŒãŸæœŸé–“ã«ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
            return
        
        # ç—…æ£Ÿãƒãƒƒãƒ”ãƒ³ã‚°ã®åˆæœŸåŒ–
        from utils import initialize_all_mappings, get_ward_display_name
        initialize_all_mappings(df_filtered)
        ward_mapping = st.session_state.get('ward_mapping', {})
        
        # æœŸé–“æƒ…å ±ã®è¡¨ç¤º
        if 'æ—¥ä»˜' in df_filtered.columns:
            min_date = df_filtered['æ—¥ä»˜'].min().date()
            max_date = df_filtered['æ—¥ä»˜'].max().date()
            st.info(f"ãƒ‡ãƒ¼ã‚¿æœŸé–“: {min_date} ï½ {max_date}")
        
        # ç—…æ£Ÿåˆ¥é›†è¨ˆ
        ward_summary = calculate_ward_summary(df_filtered)
        
        if not ward_summary.empty:
            # ç—…æ£Ÿååˆ—ã‚’è¿½åŠ 
            ward_summary['ç—…æ£Ÿå'] = ward_summary['ç—…æ£Ÿã‚³ãƒ¼ãƒ‰'].apply(
                lambda x: get_ward_display_name(x, ward_mapping)
            )
            
            # åˆ—ã®é †åºã‚’èª¿æ•´ï¼ˆç—…æ£Ÿåã‚’ç—…æ£Ÿã‚³ãƒ¼ãƒ‰ã®å¾Œã«é…ç½®ï¼‰
            cols = ward_summary.columns.tolist()
            if 'ç—…æ£Ÿå' in cols:
                # ç—…æ£Ÿã‚³ãƒ¼ãƒ‰ã®å¾Œã«ç—…æ£Ÿåã‚’é…ç½®
                code_idx = cols.index('ç—…æ£Ÿã‚³ãƒ¼ãƒ‰')
                cols.insert(code_idx + 1, cols.pop(cols.index('ç—…æ£Ÿå')))
                ward_summary = ward_summary[cols]
            
            # ãƒ†ãƒ¼ãƒ–ãƒ«è¡¨ç¤ºã¨å‡¦ç†
            display_ward_table_with_filters(ward_summary, df_filtered)
            
            # ç—…æ£Ÿåˆ¥ã‚°ãƒ©ãƒ•
            create_ward_comparison_charts(ward_summary)
        else:
            st.warning("ç—…æ£Ÿåˆ¥é›†è¨ˆãƒ‡ãƒ¼ã‚¿ã‚’ä½œæˆã§ãã¾ã›ã‚“ã§ã—ãŸã€‚")
            
    except Exception as e:
        logger.error(f"ç—…æ£Ÿåˆ¥ãƒ†ãƒ¼ãƒ–ãƒ«ä½œæˆã‚¨ãƒ©ãƒ¼: {e}")
        st.error(f"ç—…æ£Ÿåˆ¥ãƒ†ãƒ¼ãƒ–ãƒ«ä½œæˆä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")

def create_department_table_section(df_filtered):
    """è¨ºç™‚ç§‘åˆ¥ãƒ‡ãƒ¼ã‚¿ãƒ†ãƒ¼ãƒ–ãƒ«ã‚»ã‚¯ã‚·ãƒ§ãƒ³ï¼ˆçµ±ä¸€ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼å¯¾å¿œç‰ˆï¼‰"""
    st.subheader("ğŸ©º è¨ºç™‚ç§‘åˆ¥ãƒ‡ãƒ¼ã‚¿ãƒ†ãƒ¼ãƒ–ãƒ«")
    
    try:
        if df_filtered.empty:
            st.warning("æŒ‡å®šã•ã‚ŒãŸæœŸé–“ã«ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
            return
        
        # æœŸé–“æƒ…å ±ã®è¡¨ç¤º
        if 'æ—¥ä»˜' in df_filtered.columns:
            min_date = df_filtered['æ—¥ä»˜'].min().date()
            max_date = df_filtered['æ—¥ä»˜'].max().date()
            st.info(f"ãƒ‡ãƒ¼ã‚¿æœŸé–“: {min_date} ï½ {max_date}")
        
        # è¨ºç™‚ç§‘åˆ¥é›†è¨ˆ
        dept_summary = calculate_department_summary(df_filtered)
        
        if not dept_summary.empty:
            # è¨ºç™‚ç§‘åã®è¡¨ç¤ºåå¤‰æ›
            if get_display_name_for_dept:
                dept_summary['è¨ºç™‚ç§‘è¡¨ç¤ºå'] = dept_summary['è¨ºç™‚ç§‘å'].apply(
                    lambda x: get_display_name_for_dept(x, default_name=x)
                )
                # è¡¨ç¤ºç”¨ã«åˆ—ã®é †åºã‚’èª¿æ•´
                cols = dept_summary.columns.tolist()
                if 'è¨ºç™‚ç§‘è¡¨ç¤ºå' in cols:
                    cols.insert(1, cols.pop(cols.index('è¨ºç™‚ç§‘è¡¨ç¤ºå')))
                    dept_summary = dept_summary[cols]
            
            # ãƒ†ãƒ¼ãƒ–ãƒ«è¡¨ç¤ºã¨å‡¦ç†
            display_department_table_with_filters(dept_summary, df_filtered)
            
            # è¨ºç™‚ç§‘åˆ¥ã‚°ãƒ©ãƒ•
            create_department_comparison_charts(dept_summary)
        else:
            st.warning("è¨ºç™‚ç§‘åˆ¥é›†è¨ˆãƒ‡ãƒ¼ã‚¿ã‚’ä½œæˆã§ãã¾ã›ã‚“ã§ã—ãŸã€‚")
            
    except Exception as e:
        logger.error(f"è¨ºç™‚ç§‘åˆ¥ãƒ†ãƒ¼ãƒ–ãƒ«ä½œæˆã‚¨ãƒ©ãƒ¼: {e}")
        st.error(f"è¨ºç™‚ç§‘åˆ¥ãƒ†ãƒ¼ãƒ–ãƒ«ä½œæˆä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")

# ===============================================================================
# ãƒ†ãƒ¼ãƒ–ãƒ«è¡¨ç¤ºãƒ˜ãƒ«ãƒ‘ãƒ¼é–¢æ•°
# ===============================================================================

def display_ward_table_with_filters(ward_summary, df_filtered):
    """ç—…æ£Ÿãƒ†ãƒ¼ãƒ–ãƒ«ã®è¡¨ç¤ºã¨ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°å‡¦ç†"""
    # ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã‚ªãƒ—ã‚·ãƒ§ãƒ³
    col1, col2 = st.columns(2)
    with col1:
        # é¸æŠè‚¢ã‚’ã€Œã‚³ãƒ¼ãƒ‰ï¼ˆåå‰ï¼‰ã€å½¢å¼ã§è¡¨ç¤º
        ward_display_options = []
        for _, row in ward_summary.iterrows():
            code = row['ç—…æ£Ÿã‚³ãƒ¼ãƒ‰']
            name = row['ç—…æ£Ÿå']
            if name != str(code):
                display_option = f"{code}ï¼ˆ{name}ï¼‰"
            else:
                display_option = str(code)
            ward_display_options.append(display_option)
        
        selected_wards = st.multiselect(
            "è¡¨ç¤ºã™ã‚‹ç—…æ£Ÿã‚’é¸æŠï¼ˆç©ºç™½ã®å ´åˆã¯å…¨ã¦è¡¨ç¤ºï¼‰",
            options=ward_display_options,
            key="ward_table_filter"
        )
    
    with col2:
        sort_column = st.selectbox(
            "ä¸¦ã³æ›¿ãˆåŸºæº–",
            options=['ç—…æ£Ÿã‚³ãƒ¼ãƒ‰', 'å¹³å‡åœ¨é™¢æ‚£è€…æ•°', 'ç·å…¥é™¢æ‚£è€…æ•°', 'ç·é€€é™¢æ‚£è€…æ•°', 'å¹³å‡åœ¨é™¢æ—¥æ•°'],
            key="ward_table_sort"
        )
    
    # ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã¨ä¸¦ã³æ›¿ãˆ
    display_summary = ward_summary.copy()
    if selected_wards:
        # é¸æŠã•ã‚ŒãŸè¡¨ç¤ºåã‹ã‚‰ç—…æ£Ÿã‚³ãƒ¼ãƒ‰ã‚’æŠ½å‡º
        selected_codes = []
        for display_ward in selected_wards:
            # ã€Œã‚³ãƒ¼ãƒ‰ï¼ˆåå‰ï¼‰ã€å½¢å¼ã‹ã‚‰ç—…æ£Ÿã‚³ãƒ¼ãƒ‰ã‚’æŠ½å‡º
            if 'ï¼ˆ' in display_ward:
                code = display_ward.split('ï¼ˆ')[0]
            else:
                code = display_ward
            selected_codes.append(code)
        display_summary = display_summary[display_summary['ç—…æ£Ÿã‚³ãƒ¼ãƒ‰'].isin(selected_codes)]
    
    if sort_column in display_summary.columns:
        ascending = st.checkbox("æ˜‡é †ã§ä¸¦ã³æ›¿ãˆ", key="ward_table_ascending")
        display_summary = display_summary.sort_values(sort_column, ascending=ascending)
    
    # ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆé©ç”¨ã¨ãƒ†ãƒ¼ãƒ–ãƒ«è¡¨ç¤º
    format_dict = create_table_format_dict(display_summary)
    st.dataframe(
        display_summary.style.format(format_dict),
        use_container_width=True,
        height=400
    )
    
    # CSVãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
    create_csv_download_button(display_summary, df_filtered, "ç—…æ£Ÿåˆ¥ãƒ‡ãƒ¼ã‚¿")

def display_department_table_with_filters(dept_summary, df_filtered):
    """è¨ºç™‚ç§‘ãƒ†ãƒ¼ãƒ–ãƒ«ã®è¡¨ç¤ºã¨ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°å‡¦ç†"""
    # ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã‚ªãƒ—ã‚·ãƒ§ãƒ³
    col1, col2 = st.columns(2)
    with col1:
        selected_depts = st.multiselect(
            "è¡¨ç¤ºã™ã‚‹è¨ºç™‚ç§‘ã‚’é¸æŠï¼ˆç©ºç™½ã®å ´åˆã¯å…¨ã¦è¡¨ç¤ºï¼‰",
            options=sorted(dept_summary['è¨ºç™‚ç§‘å'].unique()),
            key="dept_table_filter"
        )
    
    with col2:
        sort_column = st.selectbox(
            "ä¸¦ã³æ›¿ãˆåŸºæº–",
            options=['è¨ºç™‚ç§‘å', 'å¹³å‡åœ¨é™¢æ‚£è€…æ•°', 'ç·å…¥é™¢æ‚£è€…æ•°', 'ç·é€€é™¢æ‚£è€…æ•°', 'å¹³å‡åœ¨é™¢æ—¥æ•°'],
            key="dept_table_sort"
        )
    
    # ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã¨ä¸¦ã³æ›¿ãˆ
    display_summary = dept_summary.copy()
    if selected_depts:
        display_summary = display_summary[display_summary['è¨ºç™‚ç§‘å'].isin(selected_depts)]
    
    if sort_column in display_summary.columns:
        ascending = st.checkbox("æ˜‡é †ã§ä¸¦ã³æ›¿ãˆ", key="dept_table_ascending")
        display_summary = display_summary.sort_values(sort_column, ascending=ascending)
    
    # ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆé©ç”¨ã¨ãƒ†ãƒ¼ãƒ–ãƒ«è¡¨ç¤º
    format_dict = create_table_format_dict(display_summary)
    st.dataframe(
        display_summary.style.format(format_dict),
        use_container_width=True,
        height=400
    )
    
    # CSVãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
    create_csv_download_button(display_summary, df_filtered, "è¨ºç™‚ç§‘åˆ¥ãƒ‡ãƒ¼ã‚¿")

def create_table_format_dict(summary_df):
    """ãƒ†ãƒ¼ãƒ–ãƒ«è¡¨ç¤ºç”¨ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆè¾æ›¸ã®ä½œæˆ"""
    format_dict = {}
    
    for col in summary_df.columns:
        if col in ['ç—…æ£Ÿã‚³ãƒ¼ãƒ‰', 'è¨ºç™‚ç§‘å', 'è¨ºç™‚ç§‘è¡¨ç¤ºå', 'ç—…æ£Ÿå', 'é›†è¨ˆå˜ä½']:
            # æ–‡å­—åˆ—åˆ—ã¯ãã®ã¾ã¾
            continue
        elif col in ['æœŸé–“æ—¥æ•°', 'å»¶ã¹åœ¨é™¢æ‚£è€…æ•°', 'ç·å…¥é™¢æ‚£è€…æ•°', 'ç·é€€é™¢æ‚£è€…æ•°', 
                   'ç·Šæ€¥å…¥é™¢æ‚£è€…æ•°', 'æ­»äº¡æ‚£è€…æ•°']:
            # æ•´æ•°å€¤ã¨ã—ã¦è¡¨ç¤ºï¼ˆåˆè¨ˆå€¤ãƒ»ã‚«ã‚¦ãƒ³ãƒˆæ•°ï¼‰
            format_dict[col] = "{:.0f}"
        elif col in ['å¹³å‡åœ¨é™¢æ‚£è€…æ•°']:
            # å°æ•°ç‚¹1æ¡ã§è¡¨ç¤ºï¼ˆå¹³å‡å€¤ï¼‰
            format_dict[col] = "{:.1f}"
        elif col in ['å¹³å‡åœ¨é™¢æ—¥æ•°', 'ç—…åºŠå›è»¢ç‡']:
            # å°æ•°ç‚¹1æ¡ã§è¡¨ç¤ºï¼ˆæ¯”ç‡ãƒ»æ—¥æ•°ï¼‰
            format_dict[col] = "{:.1f}"
        elif col in ['ç·Šæ€¥å…¥é™¢ç‡', 'æ­»äº¡ç‡']:
            # ãƒ‘ãƒ¼ã‚»ãƒ³ãƒ†ãƒ¼ã‚¸ã¯å°æ•°ç‚¹1æ¡ + %
            format_dict[col] = "{:.1f}%"
        else:
            # ãã®ä»–ã®æ•°å€¤åˆ—ã¯å°æ•°ç‚¹1æ¡
            if pd.api.types.is_numeric_dtype(summary_df[col]):
                # æ•´æ•°ã‹ã©ã†ã‹ã‚’åˆ¤å®š
                if summary_df[col].dtype in ['int64', 'int32', 'Int64', 'Int32']:
                    format_dict[col] = "{:.0f}"
                else:
                    # å¹³å‡å€¤ã‹ã©ã†ã‹ã‚’åå‰ã‹ã‚‰åˆ¤å®š
                    if 'å¹³å‡' in col or 'ç‡' in col or 'æ—¥æ•°' in col:
                        format_dict[col] = "{:.1f}"
                    else:
                        format_dict[col] = "{:.0f}"
    
    return format_dict

def create_csv_download_button(summary_df, df_filtered, data_type):
    """CSVãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãƒœã‚¿ãƒ³ã®ä½œæˆ"""
    csv_data = summary_df.to_csv(index=False).encode('utf-8-sig')
    
    # æœŸé–“æ–‡å­—åˆ—ã®ç”Ÿæˆ
    if 'æ—¥ä»˜' in df_filtered.columns:
        min_date = df_filtered['æ—¥ä»˜'].min().date()
        max_date = df_filtered['æ—¥ä»˜'].max().date()
        period_str = f"{min_date}_{max_date}"
    else:
        period_str = "å…¨æœŸé–“"
    
    st.download_button(
        label=f"{data_type}ã‚’CSVãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
        data=csv_data,
        file_name=f"{data_type}_{period_str}.csv",
        mime="text/csv"
    )

# ===============================================================================
# é›†è¨ˆå‡¦ç†é–¢æ•°ï¼ˆæ—¢å­˜é–¢æ•°ã‚’ãã®ã¾ã¾ä½¿ç”¨ï¼‰
# ===============================================================================

def calculate_ward_summary(df):
    """ç—…æ£Ÿåˆ¥ã‚µãƒãƒªãƒ¼ãƒ‡ãƒ¼ã‚¿ã®è¨ˆç®—"""
    try:
        available_columns = df.columns.tolist()
        column_mapping = {
            'åœ¨é™¢æ‚£è€…æ•°': ['å…¥é™¢æ‚£è€…æ•°ï¼ˆåœ¨é™¢ï¼‰', 'åœ¨é™¢æ‚£è€…æ•°', 'ç¾åœ¨æ‚£è€…æ•°'],
            'å…¥é™¢æ‚£è€…æ•°': ['ç·å…¥é™¢æ‚£è€…æ•°', 'å…¥é™¢æ‚£è€…æ•°', 'æ–°è¦å…¥é™¢æ‚£è€…æ•°'],
            'é€€é™¢æ‚£è€…æ•°': ['ç·é€€é™¢æ‚£è€…æ•°', 'é€€é™¢æ‚£è€…æ•°', 'é€€é™¢è€…æ•°'],
            'ç·Šæ€¥å…¥é™¢æ‚£è€…æ•°': ['ç·Šæ€¥å…¥é™¢æ‚£è€…æ•°', 'æ•‘æ€¥å…¥é™¢æ‚£è€…æ•°', 'ç·Šæ€¥å…¥é™¢'],
            'æ­»äº¡æ‚£è€…æ•°': ['æ­»äº¡æ‚£è€…æ•°', 'æ­»äº¡è€…æ•°', 'æ­»äº¡']
        }
        actual_columns = {}
        missing_columns = []
        
        for standard_name, possible_names in column_mapping.items():
            found_column = next((pn for pn in possible_names if pn in available_columns), None)
            if found_column:
                actual_columns[standard_name] = found_column
            else:
                missing_columns.append(standard_name)
        
        if missing_columns:
            # st.error ã¯å‘¼ã³å‡ºã—å…ƒã§è¡Œã†ã‹ã€ã‚¨ãƒ©ãƒ¼çŠ¶æ…‹ã‚’è¿”ã™ã‚ˆã†ã«ã™ã‚‹
            logger.error(f"ç—…æ£Ÿåˆ¥é›†è¨ˆã«å¿…è¦ãªåˆ—ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {missing_columns}")
            return pd.DataFrame() # ç©ºã®DataFrameã‚’è¿”ã™
        
        ward_groups = df.groupby('ç—…æ£Ÿã‚³ãƒ¼ãƒ‰', observed=True)
        ward_summary_data = {
            'ç—…æ£Ÿã‚³ãƒ¼ãƒ‰': ward_groups['ç—…æ£Ÿã‚³ãƒ¼ãƒ‰'].first(),
            'æœŸé–“æ—¥æ•°': ward_groups['æ—¥ä»˜'].nunique(),
        }
        for standard_name, actual_col_name in actual_columns.items():
            # Ensure the column exists before trying to sum it, even if mapped
            if actual_col_name in df.columns:
                 # Standardize the output column name using the sum of the actual column name
                if standard_name == 'åœ¨é™¢æ‚£è€…æ•°':
                    ward_summary_data['å»¶ã¹åœ¨é™¢æ‚£è€…æ•°'] = ward_groups[actual_col_name].sum()
                elif standard_name == 'å…¥é™¢æ‚£è€…æ•°':
                     ward_summary_data['ç·å…¥é™¢æ‚£è€…æ•°'] = ward_groups[actual_col_name].sum()
                elif standard_name == 'é€€é™¢æ‚£è€…æ•°':
                     ward_summary_data['ç·é€€é™¢æ‚£è€…æ•°'] = ward_groups[actual_col_name].sum()
                elif standard_name == 'ç·Šæ€¥å…¥é™¢æ‚£è€…æ•°':
                     ward_summary_data['ç·Šæ€¥å…¥é™¢æ‚£è€…æ•°'] = ward_groups[actual_col_name].sum()
                elif standard_name == 'æ­»äº¡æ‚£è€…æ•°':
                     ward_summary_data['æ­»äº¡æ‚£è€…æ•°'] = ward_groups[actual_col_name].sum()

            else:
                # This case should ideally not be reached if missing_columns check is robust
                logger.warning(f"ç—…æ£Ÿåˆ¥é›†è¨ˆ: åˆ— '{actual_col_name}' (å¯¾å¿œã™ã‚‹æ¨™æº–å: '{standard_name}') ãŒãƒ‡ãƒ¼ã‚¿ã«å­˜åœ¨ã—ã¾ã›ã‚“ã€‚é›†è¨ˆã‹ã‚‰é™¤å¤–ã—ã¾ã™ã€‚")


        ward_summary = pd.DataFrame(ward_summary_data).reset_index(drop=True)

        # Check if all necessary sum columns were created before proceeding
        required_sum_cols = ['å»¶ã¹åœ¨é™¢æ‚£è€…æ•°', 'ç·å…¥é™¢æ‚£è€…æ•°', 'ç·é€€é™¢æ‚£è€…æ•°', 'ç·Šæ€¥å…¥é™¢æ‚£è€…æ•°', 'æ­»äº¡æ‚£è€…æ•°']
        if not all(col in ward_summary.columns for col in required_sum_cols):
            logger.error(f"ç—…æ£Ÿåˆ¥ã‚µãƒãƒªãƒ¼è¨ˆç®—ã«å¿…è¦ãªé›†è¨ˆåˆ—ãŒä¸è¶³ã—ã¦ã„ã¾ã™ã€‚ç”Ÿæˆã•ã‚ŒãŸåˆ—: {list(ward_summary.columns)}")
            return pd.DataFrame()

        ward_summary['å¹³å‡åœ¨é™¢æ‚£è€…æ•°'] = ward_summary['å»¶ã¹åœ¨é™¢æ‚£è€…æ•°'] / ward_summary['æœŸé–“æ—¥æ•°']
        ward_summary['å¹³å‡åœ¨é™¢æ—¥æ•°'] = ward_summary.apply(
            lambda row: row['å»¶ã¹åœ¨é™¢æ‚£è€…æ•°'] / ((row['ç·å…¥é™¢æ‚£è€…æ•°'] + row['ç·é€€é™¢æ‚£è€…æ•°']) / 2)
            if (row['ç·å…¥é™¢æ‚£è€…æ•°'] + row['ç·é€€é™¢æ‚£è€…æ•°']) > 0 else 0,
            axis=1
        )
        ward_summary['ç—…åºŠå›è»¢ç‡'] = ward_summary.apply(
            lambda row: row['ç·é€€é™¢æ‚£è€…æ•°'] / row['å¹³å‡åœ¨é™¢æ‚£è€…æ•°'] 
            if row['å¹³å‡åœ¨é™¢æ‚£è€…æ•°'] > 0 else 0,
            axis=1
        )
        ward_summary['ç·Šæ€¥å…¥é™¢ç‡'] = ward_summary.apply(
            lambda row: (row['ç·Šæ€¥å…¥é™¢æ‚£è€…æ•°'] / row['ç·å…¥é™¢æ‚£è€…æ•°'] * 100)
            if row['ç·å…¥é™¢æ‚£è€…æ•°'] > 0 else 0,
            axis=1
        )
        ward_summary['æ­»äº¡ç‡'] = ward_summary.apply(
            lambda row: (row['æ­»äº¡æ‚£è€…æ•°'] / row['ç·é€€é™¢æ‚£è€…æ•°'] * 100)
            if row['ç·é€€é™¢æ‚£è€…æ•°'] > 0 else 0,
            axis=1
        )
        return ward_summary
        
    except Exception as e:
        logger.error(f"ç—…æ£Ÿåˆ¥ã‚µãƒãƒªãƒ¼è¨ˆç®—ã‚¨ãƒ©ãƒ¼: {e}", exc_info=True)
        # st.error(f"ç—…æ£Ÿåˆ¥ã‚µãƒãƒªãƒ¼è¨ˆç®—ä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}") # UIã‚¨ãƒ©ãƒ¼ã¯å‘¼ã³å‡ºã—å…ƒã§
        return pd.DataFrame()

def calculate_department_summary(df):
    """è¨ºç™‚ç§‘åˆ¥ã‚µãƒãƒªãƒ¼ãƒ‡ãƒ¼ã‚¿ã®è¨ˆç®—"""
    try:
        available_columns = df.columns.tolist()
        column_mapping = {
            'åœ¨é™¢æ‚£è€…æ•°': ['å…¥é™¢æ‚£è€…æ•°ï¼ˆåœ¨é™¢ï¼‰', 'åœ¨é™¢æ‚£è€…æ•°', 'ç¾åœ¨æ‚£è€…æ•°'],
            'å…¥é™¢æ‚£è€…æ•°': ['ç·å…¥é™¢æ‚£è€…æ•°', 'å…¥é™¢æ‚£è€…æ•°', 'æ–°è¦å…¥é™¢æ‚£è€…æ•°'],
            'é€€é™¢æ‚£è€…æ•°': ['ç·é€€é™¢æ‚£è€…æ•°', 'é€€é™¢æ‚£è€…æ•°', 'é€€é™¢è€…æ•°'],
            'ç·Šæ€¥å…¥é™¢æ‚£è€…æ•°': ['ç·Šæ€¥å…¥é™¢æ‚£è€…æ•°', 'æ•‘æ€¥å…¥é™¢æ‚£è€…æ•°', 'ç·Šæ€¥å…¥é™¢'],
            'æ­»äº¡æ‚£è€…æ•°': ['æ­»äº¡æ‚£è€…æ•°', 'æ­»äº¡è€…æ•°', 'æ­»äº¡']
        }
        actual_columns = {}
        missing_columns = []

        for standard_name, possible_names in column_mapping.items():
            found_column = next((pn for pn in possible_names if pn in available_columns), None)
            if found_column:
                actual_columns[standard_name] = found_column
            else:
                missing_columns.append(standard_name)
        
        if missing_columns:
            logger.error(f"è¨ºç™‚ç§‘åˆ¥é›†è¨ˆã«å¿…è¦ãªåˆ—ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {missing_columns}")
            return pd.DataFrame()
        
        dept_groups = df.groupby('è¨ºç™‚ç§‘å', observed=True)
        dept_summary_data = {
            'è¨ºç™‚ç§‘å': dept_groups['è¨ºç™‚ç§‘å'].first(),
            'æœŸé–“æ—¥æ•°': dept_groups['æ—¥ä»˜'].nunique(),
        }

        for standard_name, actual_col_name in actual_columns.items():
            if actual_col_name in df.columns:
                if standard_name == 'åœ¨é™¢æ‚£è€…æ•°':
                    dept_summary_data['å»¶ã¹åœ¨é™¢æ‚£è€…æ•°'] = dept_groups[actual_col_name].sum()
                elif standard_name == 'å…¥é™¢æ‚£è€…æ•°':
                    dept_summary_data['ç·å…¥é™¢æ‚£è€…æ•°'] = dept_groups[actual_col_name].sum()
                elif standard_name == 'é€€é™¢æ‚£è€…æ•°':
                    dept_summary_data['ç·é€€é™¢æ‚£è€…æ•°'] = dept_groups[actual_col_name].sum()
                elif standard_name == 'ç·Šæ€¥å…¥é™¢æ‚£è€…æ•°':
                    dept_summary_data['ç·Šæ€¥å…¥é™¢æ‚£è€…æ•°'] = dept_groups[actual_col_name].sum()
                elif standard_name == 'æ­»äº¡æ‚£è€…æ•°':
                    dept_summary_data['æ­»äº¡æ‚£è€…æ•°'] = dept_groups[actual_col_name].sum()
            else:
                logger.warning(f"è¨ºç™‚ç§‘åˆ¥é›†è¨ˆ: åˆ— '{actual_col_name}' (å¯¾å¿œã™ã‚‹æ¨™æº–å: '{standard_name}') ãŒãƒ‡ãƒ¼ã‚¿ã«å­˜åœ¨ã—ã¾ã›ã‚“ã€‚é›†è¨ˆã‹ã‚‰é™¤å¤–ã—ã¾ã™ã€‚")


        dept_summary = pd.DataFrame(dept_summary_data).reset_index(drop=True)

        required_sum_cols = ['å»¶ã¹åœ¨é™¢æ‚£è€…æ•°', 'ç·å…¥é™¢æ‚£è€…æ•°', 'ç·é€€é™¢æ‚£è€…æ•°', 'ç·Šæ€¥å…¥é™¢æ‚£è€…æ•°', 'æ­»äº¡æ‚£è€…æ•°']
        if not all(col in dept_summary.columns for col in required_sum_cols):
            logger.error(f"è¨ºç™‚ç§‘åˆ¥ã‚µãƒãƒªãƒ¼è¨ˆç®—ã«å¿…è¦ãªé›†è¨ˆåˆ—ãŒä¸è¶³ã—ã¦ã„ã¾ã™ã€‚ç”Ÿæˆã•ã‚ŒãŸåˆ—: {list(dept_summary.columns)}")
            return pd.DataFrame()

        dept_summary['å¹³å‡åœ¨é™¢æ‚£è€…æ•°'] = dept_summary['å»¶ã¹åœ¨é™¢æ‚£è€…æ•°'] / dept_summary['æœŸé–“æ—¥æ•°']
        dept_summary['å¹³å‡åœ¨é™¢æ—¥æ•°'] = dept_summary.apply(
            lambda row: row['å»¶ã¹åœ¨é™¢æ‚£è€…æ•°'] / ((row['ç·å…¥é™¢æ‚£è€…æ•°'] + row['ç·é€€é™¢æ‚£è€…æ•°']) / 2)
            if (row['ç·å…¥é™¢æ‚£è€…æ•°'] + row['ç·é€€é™¢æ‚£è€…æ•°']) > 0 else 0,
            axis=1
        )
        dept_summary['ç—…åºŠå›è»¢ç‡'] = dept_summary.apply(
            lambda row: row['ç·é€€é™¢æ‚£è€…æ•°'] / row['å¹³å‡åœ¨é™¢æ‚£è€…æ•°'] 
            if row['å¹³å‡åœ¨é™¢æ‚£è€…æ•°'] > 0 else 0,
            axis=1
        )
        dept_summary['ç·Šæ€¥å…¥é™¢ç‡'] = dept_summary.apply(
            lambda row: (row['ç·Šæ€¥å…¥é™¢æ‚£è€…æ•°'] / row['ç·å…¥é™¢æ‚£è€…æ•°'] * 100)
            if row['ç·å…¥é™¢æ‚£è€…æ•°'] > 0 else 0,
            axis=1
        )
        dept_summary['æ­»äº¡ç‡'] = dept_summary.apply(
            lambda row: (row['æ­»äº¡æ‚£è€…æ•°'] / row['ç·é€€é™¢æ‚£è€…æ•°'] * 100)
            if row['ç·é€€é™¢æ‚£è€…æ•°'] > 0 else 0,
            axis=1
        )
        return dept_summary
        
    except Exception as e:
        logger.error(f"è¨ºç™‚ç§‘åˆ¥ã‚µãƒãƒªãƒ¼è¨ˆç®—ã‚¨ãƒ©ãƒ¼: {e}", exc_info=True)
        # st.error(f"è¨ºç™‚ç§‘åˆ¥ã‚µãƒãƒªãƒ¼è¨ˆç®—ä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}") # UIã‚¨ãƒ©ãƒ¼ã¯å‘¼ã³å‡ºã—å…ƒã§
        return pd.DataFrame()

# ===============================================================================
# ã‚°ãƒ©ãƒ•ä½œæˆé–¢æ•°ï¼ˆæ—¢å­˜é–¢æ•°ã‚’ãã®ã¾ã¾ä½¿ç”¨ï¼‰
# ===============================================================================

def create_ward_comparison_charts(ward_summary):
    """ç—…æ£Ÿåˆ¥æ¯”è¼ƒãƒãƒ£ãƒ¼ãƒˆã®ä½œæˆ"""
    try:
        st.markdown("---")
        st.subheader("ç—…æ£Ÿåˆ¥æ¯”è¼ƒã‚°ãƒ©ãƒ•")
        
        col1, col2 = st.columns(2)
        
        with col1:
            # å¹³å‡åœ¨é™¢æ‚£è€…æ•°ã®æ¯”è¼ƒ
            fig_census = px.bar(
                ward_summary,
                x='ç—…æ£Ÿã‚³ãƒ¼ãƒ‰',
                y='å¹³å‡åœ¨é™¢æ‚£è€…æ•°',
                title='ç—…æ£Ÿåˆ¥ å¹³å‡åœ¨é™¢æ‚£è€…æ•°',
                color='å¹³å‡åœ¨é™¢æ‚£è€…æ•°',
                color_continuous_scale='Blues'
            )
            fig_census.update_layout(height=400)
            st.plotly_chart(fig_census, use_container_width=True)
        
        with col2:
            # å¹³å‡åœ¨é™¢æ—¥æ•°ã®æ¯”è¼ƒ
            fig_alos = px.bar(
                ward_summary,
                x='ç—…æ£Ÿã‚³ãƒ¼ãƒ‰',
                y='å¹³å‡åœ¨é™¢æ—¥æ•°',
                title='ç—…æ£Ÿåˆ¥ å¹³å‡åœ¨é™¢æ—¥æ•°',
                color='å¹³å‡åœ¨é™¢æ—¥æ•°',
                color_continuous_scale='Reds'
            )
            fig_alos.update_layout(height=400)
            st.plotly_chart(fig_alos, use_container_width=True)
        
        # æ•£å¸ƒå›³ã«ã‚ˆã‚‹ç›¸é–¢åˆ†æ
        fig_scatter = px.scatter(
            ward_summary,
            x='å¹³å‡åœ¨é™¢æ‚£è€…æ•°',
            y='å¹³å‡åœ¨é™¢æ—¥æ•°',
            size='ç·å…¥é™¢æ‚£è€…æ•°',
            hover_name='ç—…æ£Ÿã‚³ãƒ¼ãƒ‰',
            title='å¹³å‡åœ¨é™¢æ‚£è€…æ•° vs å¹³å‡åœ¨é™¢æ—¥æ•°ï¼ˆãƒãƒ–ãƒ«ã‚µã‚¤ã‚ºï¼šç·å…¥é™¢æ‚£è€…æ•°ï¼‰',
            labels={'å¹³å‡åœ¨é™¢æ‚£è€…æ•°': 'å¹³å‡åœ¨é™¢æ‚£è€…æ•°ï¼ˆäººï¼‰', 'å¹³å‡åœ¨é™¢æ—¥æ•°': 'å¹³å‡åœ¨é™¢æ—¥æ•°ï¼ˆæ—¥ï¼‰'}
        )
        fig_scatter.update_layout(height=400)
        st.plotly_chart(fig_scatter, use_container_width=True)
        
    except Exception as e:
        logger.error(f"ç—…æ£Ÿåˆ¥ã‚°ãƒ©ãƒ•ä½œæˆã‚¨ãƒ©ãƒ¼: {e}")
        st.error(f"ç—…æ£Ÿåˆ¥ã‚°ãƒ©ãƒ•ä½œæˆä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")

def create_department_comparison_charts(dept_summary):
    """è¨ºç™‚ç§‘åˆ¥æ¯”è¼ƒãƒãƒ£ãƒ¼ãƒˆã®ä½œæˆ"""
    try:
        st.markdown("---")
        st.subheader("è¨ºç™‚ç§‘åˆ¥æ¯”è¼ƒã‚°ãƒ©ãƒ•")
        
        col1, col2 = st.columns(2)
        
        with col1:
            # å¹³å‡åœ¨é™¢æ‚£è€…æ•°ã®æ¯”è¼ƒï¼ˆä¸Šä½10ä½ï¼‰
            top_census = dept_summary.nlargest(10, 'å¹³å‡åœ¨é™¢æ‚£è€…æ•°')
            fig_census = px.bar(
                top_census,
                x='å¹³å‡åœ¨é™¢æ‚£è€…æ•°',
                y='è¨ºç™‚ç§‘å',
                orientation='h',
                title='è¨ºç™‚ç§‘åˆ¥ å¹³å‡åœ¨é™¢æ‚£è€…æ•°ï¼ˆä¸Šä½10ä½ï¼‰',
                color='å¹³å‡åœ¨é™¢æ‚£è€…æ•°',
                color_continuous_scale='Blues'
            )
            fig_census.update_layout(height=400)
            st.plotly_chart(fig_census, use_container_width=True)
        
        with col2:
            # å¹³å‡åœ¨é™¢æ—¥æ•°ã®æ¯”è¼ƒï¼ˆä¸Šä½10ä½ï¼‰
            top_alos = dept_summary.nlargest(10, 'å¹³å‡åœ¨é™¢æ—¥æ•°')
            fig_alos = px.bar(
                top_alos,
                x='å¹³å‡åœ¨é™¢æ—¥æ•°',
                y='è¨ºç™‚ç§‘å',
                orientation='h',
                title='è¨ºç™‚ç§‘åˆ¥ å¹³å‡åœ¨é™¢æ—¥æ•°ï¼ˆä¸Šä½10ä½ï¼‰',
                color='å¹³å‡åœ¨é™¢æ—¥æ•°',
                color_continuous_scale='Reds'
            )
            fig_alos.update_layout(height=400)
            st.plotly_chart(fig_alos, use_container_width=True)
        
        # ç·Šæ€¥å…¥é™¢ç‡ã¨æ­»äº¡ç‡ã®æ•£å¸ƒå›³
        fig_rates = px.scatter(
            dept_summary,
            x='ç·Šæ€¥å…¥é™¢ç‡',
            y='æ­»äº¡ç‡',
            size='ç·å…¥é™¢æ‚£è€…æ•°',
            hover_name='è¨ºç™‚ç§‘å',
            title='ç·Šæ€¥å…¥é™¢ç‡ vs æ­»äº¡ç‡ï¼ˆãƒãƒ–ãƒ«ã‚µã‚¤ã‚ºï¼šç·å…¥é™¢æ‚£è€…æ•°ï¼‰',
            labels={'ç·Šæ€¥å…¥é™¢ç‡': 'ç·Šæ€¥å…¥é™¢ç‡ï¼ˆ%ï¼‰', 'æ­»äº¡ç‡': 'æ­»äº¡ç‡ï¼ˆ%ï¼‰'}
        )
        fig_rates.update_layout(height=400)
        st.plotly_chart(fig_rates, use_container_width=True)
        
    except Exception as e:
        logger.error(f"è¨ºç™‚ç§‘åˆ¥ã‚°ãƒ©ãƒ•ä½œæˆã‚¨ãƒ©ãƒ¼: {e}")
        st.error(f"è¨ºç™‚ç§‘åˆ¥ã‚°ãƒ©ãƒ•ä½œæˆä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")

# ===============================================================================
# å‡ºåŠ›ãƒ»äºˆæ¸¬ã‚»ã‚¯ã‚·ãƒ§ãƒ³ï¼ˆçµ±ä¸€ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼å¯¾å¿œç‰ˆï¼‰
# ===============================================================================

def create_individual_pdf_section(df_filtered):
    """å€‹åˆ¥PDFå‡ºåŠ›ã‚»ã‚¯ã‚·ãƒ§ãƒ³ï¼ˆçµ±ä¸€ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼å¯¾å¿œç‰ˆï¼‰"""
    st.subheader("ğŸ“„ å€‹åˆ¥PDFå‡ºåŠ›")
    
    target_data = st.session_state.get('target_data')
    
    try:
        # PDFå‡ºåŠ›è¨­å®š
        col1, col2 = st.columns(2)
        
        with col1:
            output_type = st.selectbox(
                "å‡ºåŠ›å˜ä½",
                ["å…¨ä½“", "è¨ºç™‚ç§‘åˆ¥", "ç—…æ£Ÿåˆ¥"],
                key="pdf_output_type"
            )
        
        with col2:
            pdf_orientation = st.selectbox(
                "PDFå‘ã",
                ["ç¸¦å‘ã", "æ¨ªå‘ã"],
                key="pdf_orientation"
            )
        
        # å¯¾è±¡é¸æŠ
        target_items = []
        if output_type == "è¨ºç™‚ç§‘åˆ¥":
            available_depts = sorted(df_filtered['è¨ºç™‚ç§‘å'].unique())
            target_items = st.multiselect(
                "å‡ºåŠ›å¯¾è±¡è¨ºç™‚ç§‘",
                available_depts,
                default=available_depts[:3] if len(available_depts) >= 3 else available_depts,
                key="pdf_target_depts"
            )
        elif output_type == "ç—…æ£Ÿåˆ¥":
            available_wards = sorted(df_filtered['ç—…æ£Ÿã‚³ãƒ¼ãƒ‰'].unique())
            target_items = st.multiselect(
                "å‡ºåŠ›å¯¾è±¡ç—…æ£Ÿ",
                available_wards,
                default=available_wards[:3] if len(available_wards) >= 3 else available_wards,
                key="pdf_target_wards"
            )
        
        # ã‚°ãƒ©ãƒ•è¡¨ç¤ºæœŸé–“
        graph_days = st.slider(
            "ã‚°ãƒ©ãƒ•è¡¨ç¤ºæœŸé–“ï¼ˆæ—¥æ•°ï¼‰",
            min_value=30,
            max_value=365,
            value=90,
            step=30,
            key="pdf_graph_days"
        )
        
        # PDFç”Ÿæˆãƒœã‚¿ãƒ³
        if st.button("PDFç”Ÿæˆ", key="generate_individual_pdf"):
            if output_type != "å…¨ä½“" and not target_items:
                st.warning("å‡ºåŠ›å¯¾è±¡ã‚’é¸æŠã—ã¦ãã ã•ã„ã€‚")
            else:
                generate_individual_pdfs(
                    df_filtered, target_data, output_type, target_items, 
                    pdf_orientation, graph_days
                )
    
    except Exception as e:
        logger.error(f"å€‹åˆ¥PDFå‡ºåŠ›è¨­å®šã‚¨ãƒ©ãƒ¼: {e}")
        st.error(f"å€‹åˆ¥PDFå‡ºåŠ›è¨­å®šä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")

def create_bulk_pdf_section(df_filtered):
    """ä¸€æ‹¬PDFå‡ºåŠ›ã‚»ã‚¯ã‚·ãƒ§ãƒ³ï¼ˆçµ±ä¸€ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼å¯¾å¿œç‰ˆï¼‰"""
    st.subheader("ğŸ“š ä¸€æ‹¬PDFå‡ºåŠ›")
    
    filter_summary = get_unified_filter_summary()
    st.info(f"ğŸ“š ä¸€æ‹¬PDFå¯¾è±¡: {filter_summary}")
    st.info("ä¸€æ‹¬PDFå‡ºåŠ›æ©Ÿèƒ½ã«ã‚ˆã‚Šã€ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼æ¡ä»¶ã«è©²å½“ã™ã‚‹å…¨è¨ºç™‚ç§‘ã¾ãŸã¯å…¨ç—…æ£Ÿã®PDFãƒ¬ãƒãƒ¼ãƒˆã‚’ä¸€åº¦ã«ç”Ÿæˆã§ãã¾ã™ã€‚")

def create_prediction_analysis_section(df_filtered):
    """äºˆæ¸¬åˆ†æã‚»ã‚¯ã‚·ãƒ§ãƒ³ï¼ˆçµ±ä¸€ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼å¯¾å¿œç‰ˆï¼‰"""
    st.subheader("ğŸ”® äºˆæ¸¬åˆ†æ")
    
    if display_forecast_analysis_tab:
        try:
            # ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ã§äºˆæ¸¬åˆ†æã‚’å®Ÿè¡Œ
            original_df = st.session_state.get('df')
            st.session_state['df'] = df_filtered
            
            display_forecast_analysis_tab()
            
            st.session_state['df'] = original_df
        except Exception as e:
            logger.error(f"äºˆæ¸¬åˆ†æã‚¨ãƒ©ãƒ¼: {e}")
            st.error(f"äºˆæ¸¬åˆ†æã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
            st.info("è©³ç´°ãªã‚¨ãƒ©ãƒ¼æƒ…å ±ã¯ãƒ­ã‚°ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
        finally:
            if 'original_df' in locals():
                st.session_state['df'] = original_df
    else:
        st.warning("äºˆæ¸¬åˆ†ææ©Ÿèƒ½ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚forecast_analysis_tab.pyã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
        create_fallback_prediction_analysis(df_filtered)

# ===============================================================================
# PDFç”Ÿæˆé–¢æ•°ï¼ˆã‚·ãƒ³ãƒ—ãƒ«ç‰ˆï¼‰
# ===============================================================================

def generate_individual_pdfs(df, target_data, output_type, target_items, orientation, graph_days):
    """å€‹åˆ¥PDFç”Ÿæˆå‡¦ç†ï¼ˆã‚·ãƒ³ãƒ—ãƒ«ç‰ˆï¼‰"""
    try:
        if not create_pdf:
            st.error("PDFç”Ÿæˆæ©Ÿèƒ½ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚")
            return
        
        filter_summary = get_unified_filter_summary()
        st.info(f"PDFç”Ÿæˆå¯¾è±¡: {filter_summary}")
        st.info("PDFç”Ÿæˆæ©Ÿèƒ½ã¯å®Ÿè£…ä¸­ã§ã™ã€‚")
        
    except Exception as e:
        logger.error(f"PDFç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")
        st.error(f"PDFç”Ÿæˆä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")

# ===============================================================================
# ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯é–¢æ•°ç¾¤ï¼ˆçµ±ä¸€ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼å¯¾å¿œç‰ˆï¼‰
# ===============================================================================

def create_fallback_los_analysis(df_filtered, filter_config):
    """å¹³å‡åœ¨é™¢æ—¥æ•°åˆ†æã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ç‰ˆï¼ˆçµ±ä¸€ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼å¯¾å¿œç‰ˆï¼‰"""
    st.info("ğŸ”§ å¹³å‡åœ¨é™¢æ—¥æ•°åˆ†æã®ä¸»è¦æ©Ÿèƒ½ãŒåˆ©ç”¨ã§ããªã„ãŸã‚ã€ç°¡æ˜“ç‰ˆã‚’è¡¨ç¤ºã—ã¾ã™ã€‚")
    
    try:
        if df_filtered.empty:
            st.warning("ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼æ¡ä»¶ã«ãƒãƒƒãƒã™ã‚‹ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
            return
        
        filter_summary = get_unified_filter_summary()
        st.info(f"åˆ†æå¯¾è±¡: {filter_summary}")
        
        available_columns = df_filtered.columns.tolist()
        column_mapping = {
            'åœ¨é™¢æ‚£è€…æ•°': ['å…¥é™¢æ‚£è€…æ•°ï¼ˆåœ¨é™¢ï¼‰', 'åœ¨é™¢æ‚£è€…æ•°', 'ç¾åœ¨æ‚£è€…æ•°'],
            'å…¥é™¢æ‚£è€…æ•°': ['ç·å…¥é™¢æ‚£è€…æ•°', 'å…¥é™¢æ‚£è€…æ•°', 'æ–°è¦å…¥é™¢æ‚£è€…æ•°'],
            'é€€é™¢æ‚£è€…æ•°': ['ç·é€€é™¢æ‚£è€…æ•°', 'é€€é™¢æ‚£è€…æ•°', 'é€€é™¢è€…æ•°']
        }
        actual_columns = {}
        missing_required = False
        for standard_name, possible_names in column_mapping.items():
            found_column = next((pn for pn in possible_names if pn in available_columns), None)
            if found_column:
                actual_columns[standard_name] = found_column
            elif standard_name in ['åœ¨é™¢æ‚£è€…æ•°', 'å…¥é™¢æ‚£è€…æ•°', 'é€€é™¢æ‚£è€…æ•°']: # ã“ã‚Œã‚‰ã¯å¿…é ˆ
                missing_required = True
                st.error(f"å¹³å‡åœ¨é™¢æ—¥æ•°è¨ˆç®—ã«å¿…è¦ãªä¸»è¦åˆ— '{standard_name}' (ã¾ãŸã¯ãã®ã‚¨ã‚¤ãƒªã‚¢ã‚¹) ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
                logger.error(f"ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯LOS: å¿…é ˆåˆ— '{standard_name}' ãŒæ¬ è½ã€‚")
        
        if missing_required:
            return

        total_patient_days = df_filtered[actual_columns['åœ¨é™¢æ‚£è€…æ•°']].sum() if 'åœ¨é™¢æ‚£è€…æ•°' in actual_columns else 0
        total_admissions = df_filtered[actual_columns['å…¥é™¢æ‚£è€…æ•°']].sum() if 'å…¥é™¢æ‚£è€…æ•°' in actual_columns else 0
        total_discharges = df_filtered[actual_columns['é€€é™¢æ‚£è€…æ•°']].sum() if 'é€€é™¢æ‚£è€…æ•°' in actual_columns else 0
        
        if (total_admissions + total_discharges) > 0:
            alos = total_patient_days / ((total_admissions + total_discharges) / 2)
            st.metric("æœŸé–“å…¨ä½“ã®å¹³å‡åœ¨é™¢æ—¥æ•°", f"{alos:.2f}æ—¥")
        else:
            st.metric("æœŸé–“å…¨ä½“ã®å¹³å‡åœ¨é™¢æ—¥æ•°", "è¨ˆç®—ä¸å¯ (å…¥é€€é™¢ãªã—)")
        
        # æ—¥åˆ¥ãƒˆãƒ¬ãƒ³ãƒ‰ (ã‚ˆã‚Šå®‰å…¨ã«)
        if 'æ—¥ä»˜' in df_filtered.columns and 'åœ¨é™¢æ‚£è€…æ•°' in actual_columns:
            daily_agg_dict = {actual_columns['åœ¨é™¢æ‚£è€…æ•°']: 'sum'}
            if 'å…¥é™¢æ‚£è€…æ•°' in actual_columns:
                daily_agg_dict[actual_columns['å…¥é™¢æ‚£è€…æ•°']] = 'sum'
            if 'é€€é™¢æ‚£è€…æ•°' in actual_columns:
                daily_agg_dict[actual_columns['é€€é™¢æ‚£è€…æ•°']] = 'sum'

            daily_alos = df_filtered.groupby('æ—¥ä»˜', observed=True).agg(daily_agg_dict).reset_index()
            
            # Rename to standard names for calculation
            rename_map = {v: k for k, v in actual_columns.items() if k in ['åœ¨é™¢æ‚£è€…æ•°', 'å…¥é™¢æ‚£è€…æ•°', 'é€€é™¢æ‚£è€…æ•°']}
            daily_alos = daily_alos.rename(columns=rename_map)

            if 'åœ¨é™¢æ‚£è€…æ•°' in daily_alos.columns and 'å…¥é™¢æ‚£è€…æ•°' in daily_alos.columns and 'é€€é™¢æ‚£è€…æ•°' in daily_alos.columns:
                daily_alos['æ—¥åˆ¥å¹³å‡åœ¨é™¢æ—¥æ•°'] = daily_alos.apply(
                    lambda row: row['åœ¨é™¢æ‚£è€…æ•°'] / ((row['å…¥é™¢æ‚£è€…æ•°'] + row['é€€é™¢æ‚£è€…æ•°']) / 2)
                    if (row['å…¥é™¢æ‚£è€…æ•°'] + row['é€€é™¢æ‚£è€…æ•°']) > 0 else np.nan, # np.nan for plotting
                    axis=1
                )
                
                fig = px.line(
                    daily_alos.dropna(subset=['æ—¥åˆ¥å¹³å‡åœ¨é™¢æ—¥æ•°']), # Drop NaN for cleaner plot
                    x='æ—¥ä»˜',
                    y='æ—¥åˆ¥å¹³å‡åœ¨é™¢æ—¥æ•°',
                    title=f'æ—¥åˆ¥å¹³å‡åœ¨é™¢æ—¥æ•°æ¨ç§»ï¼ˆãƒ•ã‚£ãƒ«ã‚¿ãƒ¼é©ç”¨æ¸ˆã¿ï¼‰'
                )
                st.plotly_chart(fig, use_container_width=True)
            else:
                st.warning("æ—¥åˆ¥å¹³å‡åœ¨é™¢æ—¥æ•°ã®è¨ˆç®—ã«å¿…è¦ãªãƒ‡ãƒ¼ã‚¿ãŒä¸è¶³ã—ã¦ã„ã¾ã™ã€‚")
        else:
            st.warning("æ—¥åˆ¥ãƒˆãƒ¬ãƒ³ãƒ‰è¡¨ç¤ºã«å¿…è¦ãªæ—¥ä»˜ã¾ãŸã¯åœ¨é™¢æ‚£è€…æ•°ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
            
    except Exception as e:
        logger.error(f"ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ç‰ˆå¹³å‡åœ¨é™¢æ—¥æ•°åˆ†æã‚¨ãƒ©ãƒ¼: {e}", exc_info=True)
        st.error(f"ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ç‰ˆå¹³å‡åœ¨é™¢æ—¥æ•°åˆ†æã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚")

def create_fallback_dow_analysis(df_filtered, filter_config):
    """æ›œæ—¥åˆ¥åˆ†æã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ç‰ˆï¼ˆçµ±ä¸€ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼å¯¾å¿œç‰ˆï¼‰"""
    st.info("ç°¡æ˜“ç‰ˆã®æ›œæ—¥åˆ¥åˆ†æã‚’è¡¨ç¤ºã—ã¦ã„ã¾ã™ã€‚")
    
    try:
        if df_filtered.empty:
            st.warning("ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼æ¡ä»¶ã«ãƒãƒƒãƒã™ã‚‹ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
            return
        
        # åŸºæœ¬çµ±è¨ˆã®è¡¨ç¤º
        filter_summary = get_unified_filter_summary()
        st.info(f"åˆ†æå¯¾è±¡: {filter_summary}")
        
        # æ›œæ—¥ã®è¿½åŠ 
        df_copy = df_filtered.copy()
        df_copy['æ›œæ—¥'] = df_copy['æ—¥ä»˜'].dt.day_name()
        df_copy['æ›œæ—¥ç•ªå·'] = df_copy['æ—¥ä»˜'].dt.dayofweek
        
        # åˆ©ç”¨å¯èƒ½ãªåˆ—ã®ç¢ºèª
        numeric_columns = df_copy.select_dtypes(include=[np.number]).columns
        patient_columns = [col for col in numeric_columns if 'æ‚£è€…æ•°' in col]
        
        if not patient_columns:
            st.error("æ‚£è€…æ•°ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
            return
        
        # æ›œæ—¥åˆ¥é›†è¨ˆ
        agg_dict = {col: 'mean' for col in patient_columns}
        dow_summary = df_copy.groupby(['æ›œæ—¥', 'æ›œæ—¥ç•ªå·'], observed=True).agg(agg_dict).reset_index()
        dow_summary = dow_summary.sort_values('æ›œæ—¥ç•ªå·')
        
        # ã‚°ãƒ©ãƒ•è¡¨ç¤º
        fig = px.bar(
            dow_summary,
            x='æ›œæ—¥',
            y=patient_columns[:3],  # æœ€å¤§3ã¤ã¾ã§è¡¨ç¤º
            title=f'æ›œæ—¥åˆ¥å¹³å‡æ‚£è€…æ•°ï¼ˆãƒ•ã‚£ãƒ«ã‚¿ãƒ¼é©ç”¨æ¸ˆã¿ï¼‰',
            barmode='group'
        )
        st.plotly_chart(fig, use_container_width=True)
        
        # ãƒ†ãƒ¼ãƒ–ãƒ«è¡¨ç¤º
        st.dataframe(dow_summary, use_container_width=True)
        
    except Exception as e:
        logger.error(f"ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ç‰ˆæ›œæ—¥åˆ¥åˆ†æã‚¨ãƒ©ãƒ¼: {e}")
        st.error(f"ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ç‰ˆæ›œæ—¥åˆ¥åˆ†æã§ã‚¨ãƒ©ãƒ¼: {e}")

def create_fallback_individual_analysis(df_filtered, filter_config):
    """å€‹åˆ¥åˆ†æã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ç‰ˆï¼ˆçµ±ä¸€ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼å¯¾å¿œç‰ˆï¼‰"""
    st.info("å€‹åˆ¥åˆ†ææ©Ÿèƒ½ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚")
    filter_summary = get_unified_filter_summary()
    st.info(f"åˆ†æå¯¾è±¡: {filter_summary}")
    st.write("individual_analysis_tab.pyãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")

def create_fallback_prediction_analysis(df_filtered):
    """äºˆæ¸¬åˆ†æã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ç‰ˆï¼ˆçµ±ä¸€ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼å¯¾å¿œç‰ˆï¼‰"""
    st.info("äºˆæ¸¬åˆ†ææ©Ÿèƒ½ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚")
    filter_summary = get_unified_filter_summary()
    st.info(f"åˆ†æå¯¾è±¡: {filter_summary}")
    st.write("forecast_analysis_tab.pyãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")