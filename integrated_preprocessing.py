import pandas as pd
import numpy as np
import streamlit as st
import jpholiday
import gc
import time
import hashlib
from io import BytesIO
import os
import tempfile
from concurrent.futures import ThreadPoolExecutor
import concurrent.futures
import logging

# ãƒ­ã‚®ãƒ³ã‚°è¨­å®š
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

def fix_problematic_data(series, column_name="unknown"):
    """
    å®Œå…¨ã«ãƒã‚¤ãƒ•ãƒ³ãªã©ã®å•é¡Œã‚’è§£æ±ºã™ã‚‹é–¢æ•°
    """
    print(f"ğŸ”§ ãƒ‡ãƒ¼ã‚¿ä¿®æ­£é–‹å§‹: {column_name}")
    
    try:
        # ã‚¹ãƒ†ãƒƒãƒ—1: å…¨ã¦æ–‡å­—åˆ—ã«å¤‰æ›
        series_str = series.astype(str)
        
        # ã‚¹ãƒ†ãƒƒãƒ—2: å•é¡Œã®ã‚ã‚‹å€¤ã‚’å®Œå…¨ã«é™¤å»
        replacements = {
            '-': '0',
            'ï¼': '0',
            ' ': '0',
            'ã€€': '0',
            'ãªã—': '0',
            'NA': '0',
            'N/A': '0',
            'NULL': '0',
            'null': '0',
            'nan': '0',
            'NaN': '0',
            'NaT': '0',
            'None': '0',
            '': '0'
        }
        
        for old, new in replacements.items():
            series_str = series_str.replace(old, new)
        
        # ã‚¹ãƒ†ãƒƒãƒ—3: æ•°å€¤å¤‰æ›
        series_numeric = pd.to_numeric(series_str, errors='coerce')
        
        # ã‚¹ãƒ†ãƒƒãƒ—4: ã¾ã NaNãŒã‚ã‚Œã°0ã§åŸ‹ã‚ã‚‹
        series_filled = series_numeric.fillna(0.0)
        
        # ã‚¹ãƒ†ãƒƒãƒ—5: å¼·åˆ¶çš„ã«float64å‹ã«çµ±ä¸€
        series_final = series_filled.astype('float64')
        
        print(f"âœ… ãƒ‡ãƒ¼ã‚¿ä¿®æ­£å®Œäº†: {column_name} â†’ {series_final.dtype}")
        return series_final
        
    except Exception as e:
        print(f"âŒ ãƒ‡ãƒ¼ã‚¿ä¿®æ­£ã‚¨ãƒ©ãƒ¼ {column_name}: {e}")
        # å®Œå…¨ã«ã‚¨ãƒ©ãƒ¼ã®å ´åˆã¯0ã®é…åˆ—ã‚’è¿”ã™
        return pd.Series([0.0] * len(series), dtype='float64', index=series.index)

def emergency_dataframe_fix(df):
    """ç·Šæ€¥DataFrameã‚¨ãƒ©ãƒ¼ä¿®æ­£"""
    if df is None or df.empty:
        return df
    
    print("ğŸš¨ ç·Šæ€¥DataFrameä¿®æ­£é–‹å§‹")
    df_fixed = df.copy()
    
    # ã™ã¹ã¦ã®åˆ—ã‚’ãƒã‚§ãƒƒã‚¯
    for col in df_fixed.columns:
        if col == 'æ—¥ä»˜':
            # æ—¥ä»˜åˆ—ã¯ç‰¹åˆ¥å‡¦ç†
            if not pd.api.types.is_datetime64_any_dtype(df_fixed[col]):
                df_fixed[col] = pd.to_datetime(df_fixed[col], errors='coerce')
        elif df_fixed[col].dtype == 'object':
            # ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆå‹ã®åˆ—ã‚’ãƒã‚§ãƒƒã‚¯
            numeric_keywords = ['æ•°', 'ç‡', 'é¡', 'å††', 'æ—¥', 'åœ¨é™¢', 'å…¥é™¢', 'é€€é™¢', 'æ­»äº¡']
            
            if any(keyword in str(col) for keyword in numeric_keywords):
                # æ•°å€¤åˆ—ã¨ã—ã¦å‡¦ç†
                df_fixed[col] = fix_problematic_data(df_fixed[col], col)
            else:
                # æ–‡å­—åˆ—åˆ—ã¨ã—ã¦å‡¦ç†
                df_fixed[col] = df_fixed[col].astype(str).fillna('')
    
    print("âœ… ç·Šæ€¥DataFrameä¿®æ­£å®Œäº†")
    return df_fixed

def efficient_duplicate_check(df_raw):
    """é‡è¤‡ãƒã‚§ãƒƒã‚¯é–¢æ•°"""
    start_time = time.time()
    
    # ç©ºã®ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ãƒã‚§ãƒƒã‚¯
    if df_raw is None or df_raw.empty:
        logger.info("é‡è¤‡ãƒã‚§ãƒƒã‚¯: ç©ºã®ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ãŒæ¸¡ã•ã‚Œã¾ã—ãŸ")
        return df_raw
    
    initial_rows = len(df_raw)
    
    # ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡æœ€é©åŒ–ã®ãŸã‚ã®å‹å¤‰æ›
    for col in df_raw.select_dtypes(include=['object']).columns:
        try:
            # ã‚«ãƒ¼ãƒ‡ã‚£ãƒŠãƒªãƒ†ã‚£ï¼ˆãƒ¦ãƒ‹ãƒ¼ã‚¯å€¤ã®æ¯”ç‡ï¼‰ãŒä½ã„åˆ—ã®ã¿ã‚«ãƒ†ã‚´ãƒªå‹ã«å¤‰æ›
            if df_raw[col].nunique() / len(df_raw) < 0.5:
                df_raw[col] = df_raw[col].astype('category')
                logger.debug(f"åˆ— '{col}' ã‚’ã‚«ãƒ†ã‚´ãƒªå‹ã«å¤‰æ›")
        except Exception as e:
            logger.warning(f"åˆ— '{col}' ã®å‹å¤‰æ›ã‚¨ãƒ©ãƒ¼: {e}")
    
    try:
        # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹è¨ˆæ¸¬é–‹å§‹
        mem_before = df_raw.memory_usage(deep=True).sum() / (1024 * 1024)
        
        # é‡è¤‡é™¤å»å®Ÿè¡Œï¼ˆinplace=Falseã§ã‚³ãƒ”ãƒ¼ã‚’ä½œæˆï¼‰
        df_processed = df_raw.drop_duplicates()
        
        # é‡è¤‡é™¤å»å¾Œã®ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡
        mem_after = df_processed.memory_usage(deep=True).sum() / (1024 * 1024)
        
        # çµæœã®é›†è¨ˆ
        rows_dropped = initial_rows - len(df_processed)
        
        # å…ƒã®ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã¨ãã®é–¢é€£ãƒªã‚½ãƒ¼ã‚¹ã‚’è§£æ”¾
        del df_raw
        gc.collect()
        
        # çµ‚äº†æ™‚é–“ã¨ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æƒ…å ±
        end_time = time.time()
        processing_time = end_time - start_time
        
        # çµæœã‚’ãƒ­ã‚°ã«å‡ºåŠ›
        logger.info(f"é‡è¤‡ãƒã‚§ãƒƒã‚¯çµæœ: åˆæœŸè¡Œæ•°={initial_rows:,}, å‰Šé™¤è¡Œæ•°={rows_dropped:,}, "
                   f"æœ€çµ‚è¡Œæ•°={len(df_processed):,}, å‡¦ç†æ™‚é–“={processing_time:.2f}ç§’, "
                   f"ãƒ¡ãƒ¢ãƒªå‰Šæ¸›={mem_before-mem_after:.2f}MB")
        
        # ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã«ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æƒ…å ±ã‚’è¨˜éŒ²
        if 'performance_metrics' not in st.session_state:
            st.session_state.performance_metrics = {}
        st.session_state.performance_metrics['duplicate_check_time'] = processing_time
        st.session_state.performance_metrics['duplicate_rows_removed'] = rows_dropped
        
        return df_processed
    
    except Exception as e:
        import traceback
        error_detail = traceback.format_exc()
        logger.error(f"é‡è¤‡ãƒã‚§ãƒƒã‚¯å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}\n{error_detail}")
        
        # ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ãŸå ´åˆã¯å…ƒã®ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’è¿”ã™
        return df_raw

def integrated_preprocess_data(df: pd.DataFrame, target_data_df: pd.DataFrame = None):
    """ä¿®æ­£ç‰ˆã®å‰å‡¦ç†é–¢æ•°"""
    print("ğŸš€ integrated_preprocess_data é–‹å§‹")
    
    start_time = time.time()
    validation_results = {
        "is_valid": True,
        "warnings": [],
        "errors": [],
        "info": []
    }

    major_departments_list = []
    # target_data_df ã‚’ä½¿ã£ã¦ major_departments_list ã‚’ç”Ÿæˆã™ã‚‹ãƒ­ã‚¸ãƒƒã‚¯
    if target_data_df is not None and not target_data_df.empty and 'éƒ¨é–€ã‚³ãƒ¼ãƒ‰' in target_data_df.columns:
        potential_major_depts = target_data_df['éƒ¨é–€ã‚³ãƒ¼ãƒ‰'].astype(str).unique()
        if 'éƒ¨é–€å' in target_data_df.columns:
            potential_major_depts_from_name = target_data_df['éƒ¨é–€å'].astype(str).unique()
            potential_major_depts = np.union1d(potential_major_depts, potential_major_depts_from_name)

        if 'è¨ºç™‚ç§‘å' in df.columns:
            actual_depts_in_df = df['è¨ºç™‚ç§‘å'].astype(str).unique()
            major_departments_list = [dept for dept in actual_depts_in_df if dept in potential_major_depts]
        
        if not major_departments_list and len(potential_major_depts) > 0:
            major_departments_list = list(potential_major_depts)
            validation_results["warnings"].append(
                "ç›®æ¨™è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã«è¨˜è¼‰ã®è¨ºç™‚ç§‘ãŒã€å®Ÿç¸¾ãƒ‡ãƒ¼ã‚¿ã®è¨ºç™‚ç§‘åã¨ç›´æ¥ä¸€è‡´ã—ã¾ã›ã‚“ã§ã—ãŸã€‚"
                "ç›®æ¨™è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã®ã€Œéƒ¨é–€ã‚³ãƒ¼ãƒ‰ã€ã¾ãŸã¯ã€Œéƒ¨é–€åã€ã‚’ä¸»è¦è¨ºç™‚ç§‘ã¨ã—ã¦æ‰±ã„ã¾ã™ã€‚"
            )
        if not major_departments_list:
            validation_results["warnings"].append("ç›®æ¨™è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ä¸»è¦è¨ºç™‚ç§‘ãƒªã‚¹ãƒˆã‚’ç‰¹å®šã§ãã¾ã›ã‚“ã§ã—ãŸã€‚")
    else:
        validation_results["warnings"].append("ç›®æ¨™è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ãŒæä¾›ã•ã‚Œãªã‹ã£ãŸã‹ã€'éƒ¨é–€ã‚³ãƒ¼ãƒ‰'åˆ—ãŒã‚ã‚Šã¾ã›ã‚“ã€‚å…¨ã¦ã®è¨ºç™‚ç§‘ã‚’ã€Œãã®ä»–ã€ã¨ã—ã¦æ‰±ã„ã¾ã™ã€‚")

    try:
        if df is None or df.empty:
            validation_results["is_valid"] = False
            validation_results["errors"].append("å…¥åŠ›ãƒ‡ãƒ¼ã‚¿ãŒç©ºã§ã™ã€‚")
            return None, validation_results

        # å¿…è¦ãªåˆ—ã®ç¢ºèª
        expected_cols = ["ç—…æ£Ÿã‚³ãƒ¼ãƒ‰", "è¨ºç™‚ç§‘å", "æ—¥ä»˜", "åœ¨é™¢æ‚£è€…æ•°",
                         "å…¥é™¢æ‚£è€…æ•°", "ç·Šæ€¥å…¥é™¢æ‚£è€…æ•°", "é€€é™¢æ‚£è€…æ•°", "æ­»äº¡æ‚£è€…æ•°"]

        # åˆ©ç”¨å¯èƒ½ãªåˆ—ã®ã¿ã‚’å¯¾è±¡ã¨ã™ã‚‹
        available_cols = [col for col in df.columns if col in expected_cols]
        df_processed = df[available_cols].copy()

        # ğŸš¨ æœ€åˆã«ç·Šæ€¥ä¿®æ­£ã‚’é©ç”¨
        print("ğŸš¨ å…¥åŠ›ãƒ‡ãƒ¼ã‚¿ã«ç·Šæ€¥ä¿®æ­£ã‚’é©ç”¨")
        df_processed = emergency_dataframe_fix(df_processed)

        # å¿…é ˆåˆ—ã®ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°
        initial_rows = len(df_processed)
        if 'ç—…æ£Ÿã‚³ãƒ¼ãƒ‰' in df_processed.columns:
            df_processed.dropna(subset=['ç—…æ£Ÿã‚³ãƒ¼ãƒ‰'], inplace=True)
            rows_dropped_due_to_ward_nan = initial_rows - len(df_processed)
            if rows_dropped_due_to_ward_nan > 0:
                validation_results["warnings"].append(
                    f"ã€Œç—…æ£Ÿã‚³ãƒ¼ãƒ‰ã€ãŒæ¬ æã—ã¦ã„ã‚‹è¡ŒãŒ {rows_dropped_due_to_ward_nan} ä»¶ã‚ã‚Šã¾ã—ãŸã€‚ã“ã‚Œã‚‰ã®è¡Œã¯é™¤å¤–ã•ã‚Œã¾ã—ãŸã€‚"
                )
        
        # æ—¥ä»˜åˆ—ã®å‡¦ç†
        if 'æ—¥ä»˜' not in df_processed.columns:
            validation_results["is_valid"] = False
            validation_results["errors"].append("å¿…é ˆåˆ—ã€Œæ—¥ä»˜ã€ãŒå­˜åœ¨ã—ã¾ã›ã‚“ã€‚")
            return None, validation_results
            
        if not pd.api.types.is_datetime64_any_dtype(df_processed['æ—¥ä»˜']):
            df_processed['æ—¥ä»˜'] = pd.to_datetime(df_processed['æ—¥ä»˜'], errors='coerce')
        
        initial_rows = len(df_processed)
        df_processed.dropna(subset=['æ—¥ä»˜'], inplace=True)
        rows_dropped_due_to_date_nan = initial_rows - len(df_processed)
        if rows_dropped_due_to_date_nan > 0:
             validation_results["warnings"].append(
                f"ç„¡åŠ¹ãªæ—¥ä»˜ã¾ãŸã¯æ—¥ä»˜ãŒæ¬ æã—ã¦ã„ã‚‹è¡ŒãŒ {rows_dropped_due_to_date_nan} ä»¶ã‚ã‚Šã¾ã—ãŸã€‚ã“ã‚Œã‚‰ã®è¡Œã¯é™¤å¤–ã•ã‚Œã¾ã—ãŸã€‚"
            )

        if df_processed.empty:
            validation_results["is_valid"] = False
            validation_results["errors"].append("å¿…é ˆã®ã€Œç—…æ£Ÿã‚³ãƒ¼ãƒ‰ã€ã¾ãŸã¯ã€Œæ—¥ä»˜ã€ã®å‡¦ç†å¾Œã«ãƒ‡ãƒ¼ã‚¿ãŒç©ºã«ãªã‚Šã¾ã—ãŸã€‚")
            return None, validation_results
            
        # ç—…æ£Ÿã‚³ãƒ¼ãƒ‰ã‚’æ–‡å­—åˆ—å‹ã«å¤‰æ›
        if 'ç—…æ£Ÿã‚³ãƒ¼ãƒ‰' in df_processed.columns:
            df_processed["ç—…æ£Ÿã‚³ãƒ¼ãƒ‰"] = df_processed["ç—…æ£Ÿã‚³ãƒ¼ãƒ‰"].astype(str)
    
        # è¨ºç™‚ç§‘åã®å‡¦ç†
        if 'è¨ºç™‚ç§‘å' in df_processed.columns:
            if pd.api.types.is_categorical_dtype(df_processed['è¨ºç™‚ç§‘å']):
                df_processed['è¨ºç™‚ç§‘å'] = df_processed['è¨ºç™‚ç§‘å'].astype(str).fillna("ç©ºç™½è¨ºç™‚ç§‘")
            else:
                df_processed['è¨ºç™‚ç§‘å'] = df_processed['è¨ºç™‚ç§‘å'].fillna("ç©ºç™½è¨ºç™‚ç§‘").astype(str)
            
            # è¨ºç™‚ç§‘åã®é›†ç´„
            df_processed['è¨ºç™‚ç§‘å'] = df_processed['è¨ºç™‚ç§‘å'].apply(
                lambda x: x if x in major_departments_list else 'ãã®ä»–'
            )
            validation_results["info"].append(
                f"è¨ºç™‚ç§‘åã‚’ä¸»è¦è¨ºç™‚ç§‘ï¼ˆ{len(major_departments_list)}ä»¶ï¼‰ã¨ã€Œãã®ä»–ã€ã«é›†ç´„ã—ã¾ã—ãŸã€‚"
            )
        else:
            validation_results["warnings"].append("ã€Œè¨ºç™‚ç§‘åã€åˆ—ãŒå­˜åœ¨ã—ãªã„ãŸã‚ã€è¨ºç™‚ç§‘é›†ç´„ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã—ãŸã€‚")
    
        # æ•°å€¤åˆ—ã®ç¢ºå®Ÿãªä¿®æ­£
        numeric_cols_to_process = [
            "åœ¨é™¢æ‚£è€…æ•°", "å…¥é™¢æ‚£è€…æ•°", "ç·Šæ€¥å…¥é™¢æ‚£è€…æ•°", "é€€é™¢æ‚£è€…æ•°", "æ­»äº¡æ‚£è€…æ•°"
        ]
        
        for col in numeric_cols_to_process:
            if col in df_processed.columns:
                df_processed[col] = fix_problematic_data(df_processed[col], col)
                validation_results["info"].append(f"æ•°å€¤åˆ—'{col}'ã‚’ä¿®æ­£ã—ã¾ã—ãŸã€‚")
            else:
                # å­˜åœ¨ã—ãªã„å ´åˆã¯0ã§åŸ‹ã‚ãŸåˆ—ã‚’ä½œæˆ
                df_processed[col] = 0.0
                validation_results["warnings"].append(f"æ•°å€¤åˆ—'{col}'ãŒå­˜åœ¨ã—ãªã‹ã£ãŸãŸã‚ã€0ã§åŸ‹ã‚ãŸåˆ—ã‚’ä½œæˆã—ã¾ã—ãŸã€‚")

        # åˆ—åçµ±ä¸€å‡¦ç†
        if "åœ¨é™¢æ‚£è€…æ•°" in df_processed.columns:
            if "å…¥é™¢æ‚£è€…æ•°ï¼ˆåœ¨é™¢ï¼‰" not in df_processed.columns:
                df_processed["å…¥é™¢æ‚£è€…æ•°ï¼ˆåœ¨é™¢ï¼‰"] = df_processed["åœ¨é™¢æ‚£è€…æ•°"].copy()
                validation_results["info"].append("ã€Œåœ¨é™¢æ‚£è€…æ•°ã€åˆ—ã‚’ã€Œå…¥é™¢æ‚£è€…æ•°ï¼ˆåœ¨é™¢ï¼‰ã€åˆ—ã«ã‚³ãƒ”ãƒ¼ã—ã¾ã—ãŸã€‚")
        elif "å…¥é™¢æ‚£è€…æ•°ï¼ˆåœ¨é™¢ï¼‰" not in df_processed.columns:
            df_processed["å…¥é™¢æ‚£è€…æ•°ï¼ˆåœ¨é™¢ï¼‰"] = 0.0
            df_processed["åœ¨é™¢æ‚£è€…æ•°"] = 0.0

        # å…¥é™¢æ‚£è€…æ•°ï¼ˆåœ¨é™¢ï¼‰ã®å¼·åˆ¶ä¿®æ­£
        if "å…¥é™¢æ‚£è€…æ•°ï¼ˆåœ¨é™¢ï¼‰" in df_processed.columns:
            df_processed["å…¥é™¢æ‚£è€…æ•°ï¼ˆåœ¨é™¢ï¼‰"] = fix_problematic_data(
                df_processed["å…¥é™¢æ‚£è€…æ•°ï¼ˆåœ¨é™¢ï¼‰"], "å…¥é™¢æ‚£è€…æ•°ï¼ˆåœ¨é™¢ï¼‰"
            )

        # æ´¾ç”ŸæŒ‡æ¨™ã®è¨ˆç®—
        if "å…¥é™¢æ‚£è€…æ•°" in df_processed.columns and "ç·Šæ€¥å…¥é™¢æ‚£è€…æ•°" in df_processed.columns:
            df_processed["ç·å…¥é™¢æ‚£è€…æ•°"] = df_processed["å…¥é™¢æ‚£è€…æ•°"] + df_processed["ç·Šæ€¥å…¥é™¢æ‚£è€…æ•°"]
        else:
            df_processed["ç·å…¥é™¢æ‚£è€…æ•°"] = 0.0

        if "é€€é™¢æ‚£è€…æ•°" in df_processed.columns and "æ­»äº¡æ‚£è€…æ•°" in df_processed.columns:
            df_processed["ç·é€€é™¢æ‚£è€…æ•°"] = df_processed["é€€é™¢æ‚£è€…æ•°"] + df_processed["æ­»äº¡æ‚£è€…æ•°"]
        else:
            df_processed["ç·é€€é™¢æ‚£è€…æ•°"] = 0.0

        if "ç·å…¥é™¢æ‚£è€…æ•°" in df_processed.columns:
            df_processed["æ–°å…¥é™¢æ‚£è€…æ•°"] = df_processed["ç·å…¥é™¢æ‚£è€…æ•°"]
        else:
            df_processed["æ–°å…¥é™¢æ‚£è€…æ•°"] = 0.0

        # å¹³æ—¥/ä¼‘æ—¥ãƒ•ãƒ©ã‚°ã®è¿½åŠ 
        if 'æ—¥ä»˜' in df_processed.columns:
            df_processed = add_weekday_flag(df_processed)

        # æœ€çµ‚çš„ãªä¿®æ­£ã‚’å†åº¦é©ç”¨
        print("ğŸš¨ æœ€çµ‚ä¿®æ­£ã‚’é©ç”¨")
        df_processed = emergency_dataframe_fix(df_processed)

        gc.collect()
        end_time = time.time()
        validation_results["info"].append(f"ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†æ™‚é–“: {end_time - start_time:.2f}ç§’")
        validation_results["info"].append(f"å‡¦ç†å¾Œã®ãƒ¬ã‚³ãƒ¼ãƒ‰æ•°: {len(df_processed)}")
        
        if not df_processed.empty:
            validation_results["info"].append(f"ãƒ‡ãƒ¼ã‚¿æœŸé–“: {df_processed['æ—¥ä»˜'].min().strftime('%Y/%m/%d')} - {df_processed['æ—¥ä»˜'].max().strftime('%Y/%m/%d')}")

        if df_processed.empty:
            validation_results["is_valid"] = False
            validation_results["errors"].append("å‰å‡¦ç†ã®çµæœã€æœ‰åŠ¹ãªãƒ‡ãƒ¼ã‚¿ãŒæ®‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
            return None, validation_results

        print("âœ… integrated_preprocess_data å®Œäº†")
        return df_processed, validation_results

    except Exception as e:
        import traceback
        error_detail = traceback.format_exc()
        validation_results["is_valid"] = False
        validation_results["errors"].append(f"ãƒ‡ãƒ¼ã‚¿ã®å‰å‡¦ç†ä¸­ã«äºˆæœŸã›ã¬ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}")
        validation_results["errors"].append(f"è©³ç´°: {error_detail}")
        print(f"âŒ å‰å‡¦ç†ã‚¨ãƒ©ãƒ¼: {error_detail}")
        return None, validation_results

def add_weekday_flag(df):
    """å¹³æ—¥/ä¼‘æ—¥ã®åˆ¤å®šãƒ•ãƒ©ã‚°ã‚’è¿½åŠ ã™ã‚‹"""
    def is_holiday(date):
        return (
            date.weekday() >= 5 or  # åœŸæ—¥
            jpholiday.is_holiday(date) or  # ç¥æ—¥
            (date.month == 12 and date.day >= 29) or  # å¹´æœ«
            (date.month == 1 and date.day <= 3)  # å¹´å§‹
        )
    
    df["å¹³æ—¥åˆ¤å®š"] = df["æ—¥ä»˜"].apply(lambda x: "ä¼‘æ—¥" if is_holiday(x) else "å¹³æ—¥")
    return df

def calculate_file_hash(file_content_bytes):
    """ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒãƒƒã‚·ãƒ¥å€¤ã‚’è¨ˆç®—"""
    try:
        max_bytes = 10 * 1024 * 1024
        if len(file_content_bytes) > max_bytes:
            head = file_content_bytes[:5 * 1024 * 1024]
            tail = file_content_bytes[-5 * 1024 * 1024:]
            combined = head + tail
            return hashlib.md5(combined).hexdigest()
        else:
            return hashlib.md5(file_content_bytes).hexdigest()
    except Exception as e:
        print(f"ãƒ•ã‚¡ã‚¤ãƒ«ãƒãƒƒã‚·ãƒ¥è¨ˆç®—ã‚¨ãƒ©ãƒ¼: {str(e)}")
        file_size = len(file_content_bytes)
        timestamp = int(time.time())
        return f"size_{file_size}_time_{timestamp}"

@st.cache_data(ttl=3600, show_spinner=False)
def read_excel_cached(file_content_bytes, sheet_name=0, usecols=None, dtype=None):
    """Excelãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿"""
    temp_path = None
    try:
        with tempfile.NamedTemporaryFile(delete=False, suffix='.xlsx') as temp_file:
            temp_file.write(file_content_bytes)
            temp_path = temp_file.name

        df = pd.read_excel(
            temp_path, 
            sheet_name=sheet_name, 
            engine='openpyxl', 
            usecols=usecols, 
            dtype=dtype
        )
        
        if df.empty:
            print(f"è­¦å‘Š: èª­ã¿è¾¼ã¾ã‚ŒãŸExcelãƒ•ã‚¡ã‚¤ãƒ«ãŒç©ºã§ã™: sheet_name={sheet_name}")
            return None
            
        return df
    except Exception as e:
        print(f"Excelèª­è¾¼ã‚¨ãƒ©ãƒ¼: {str(e)}")
        import traceback
        print(traceback.format_exc())
        return None
    finally:
        if temp_path and os.path.exists(temp_path):
            try:
                os.unlink(temp_path)
            except Exception as e:
                print(f"ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«å‰Šé™¤ã‚¨ãƒ©ãƒ¼: {str(e)}")

def load_files(base_file, new_files, usecols_excel=None, dtype_excel=None):
    """è¤‡æ•°ã®Excelãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚€"""
    start_time = time.time()
    df_list = []
    files_to_process = []

    if base_file:
        files_to_process.append(base_file)
        print(f"åŸºæœ¬ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‡¦ç†ãƒªã‚¹ãƒˆã«è¿½åŠ : {base_file.name}")
    
    if new_files:
        files_to_process.extend(new_files)
        print(f"è¿½åŠ ãƒ•ã‚¡ã‚¤ãƒ«{len(new_files)}ä»¶ã‚’å‡¦ç†ãƒªã‚¹ãƒˆã«è¿½åŠ ")

    if not files_to_process:
        print("å‡¦ç†å¯¾è±¡ãƒ•ã‚¡ã‚¤ãƒ«ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
        return pd.DataFrame()

    file_contents = []
    for file_obj in files_to_process:
        try:
            file_obj.seek(0)
            file_content = file_obj.read()
            file_contents.append((file_obj.name, file_content))
            file_obj.seek(0)
            file_size = len(file_content) / (1024 * 1024)
            print(f"ãƒ•ã‚¡ã‚¤ãƒ«èª­è¾¼: {file_obj.name} ({file_size:.2f} MB)")
        except Exception as e:
            print(f"ãƒ•ã‚¡ã‚¤ãƒ«èª­è¾¼ã‚¨ãƒ©ãƒ¼ ({file_obj.name}): {str(e)}")

    max_workers = min(4, len(file_contents)) if file_contents else 1
    print(f"ä¸¦åˆ—å‡¦ç†ãƒ¯ãƒ¼ã‚«ãƒ¼æ•°: {max_workers}")
    
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        futures = {
            executor.submit(read_excel_cached, content, 0, usecols_excel, dtype_excel): name
            for name, content in file_contents
        }

        successful_files = 0
        for future in concurrent.futures.as_completed(futures):
            file_name = futures[future]
            try:
                df = future.result()
                if df is not None and not df.empty:
                    df_list.append(df)
                    rows, cols = df.shape
                    print(f"ãƒ•ã‚¡ã‚¤ãƒ« '{file_name}' ã®èª­è¾¼æˆåŠŸ: {rows}è¡Œ Ã— {cols}åˆ—")
                    successful_files += 1
                else:
                    print(f"ãƒ•ã‚¡ã‚¤ãƒ« '{file_name}' ã®èª­è¾¼çµæœãŒç©ºã§ã™")
            except Exception as e:
                print(f"ãƒ•ã‚¡ã‚¤ãƒ« '{file_name}' ã®å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼: {str(e)}")
                import traceback
                print(traceback.format_exc())

    if not df_list:
        print("èª­ã¿è¾¼ã¿å¯èƒ½ãªExcelãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
        return pd.DataFrame()

    try:
        df_raw = pd.concat(df_list, ignore_index=True)
        df_raw = efficient_duplicate_check(df_raw)
        
        end_time = time.time()
        rows, cols = df_raw.shape
        print(f"ãƒ‡ãƒ¼ã‚¿èª­è¾¼å®Œäº†: {successful_files}/{len(file_contents)}ãƒ•ã‚¡ã‚¤ãƒ«æˆåŠŸ, {rows}è¡Œ Ã— {cols}åˆ—, å‡¦ç†æ™‚é–“: {end_time - start_time:.2f}ç§’")
        
        return df_raw
    except Exception as e:
        print(f"ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ çµåˆã‚¨ãƒ©ãƒ¼: {str(e)}")
        return pd.DataFrame()