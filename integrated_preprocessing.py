import pandas as pd
import numpy as np
import streamlit as st
import jpholiday
import gc
import time
import hashlib
from io import BytesIO
import os
import tempfile
from concurrent.futures import ThreadPoolExecutor
import concurrent.futures
import logging

# ãƒ­ã‚®ãƒ³ã‚°è¨­å®š
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

def efficient_duplicate_check(df_raw):
    """
    ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã®é‡è¤‡ã‚’åŠ¹ç‡çš„ã«ãƒã‚§ãƒƒã‚¯ã—ã¦é™¤å»ã™ã‚‹é–¢æ•°ï¼ˆä¿®æ­£ç‰ˆï¼‰
    
    Args:
        df_raw (pd.DataFrame): é‡è¤‡ãƒã‚§ãƒƒã‚¯å¯¾è±¡ã®ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ 
        
    Returns:
        pd.DataFrame: é‡è¤‡ãŒé™¤å»ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ 
    """
    start_time = time.time()
    
    # ç©ºã®ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ãƒã‚§ãƒƒã‚¯
    if df_raw is None or df_raw.empty:
        logger.info("é‡è¤‡ãƒã‚§ãƒƒã‚¯: ç©ºã®ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ãŒæ¸¡ã•ã‚Œã¾ã—ãŸ")
        return df_raw
    
    initial_rows = len(df_raw)
    
    # ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡æœ€é©åŒ–ã®ãŸã‚ã®å‹å¤‰æ›
    for col in df_raw.select_dtypes(include=['object']).columns:
        try:
            if df_raw[col].nunique() / len(df_raw) < 0.5:
                df_raw[col] = df_raw[col].astype('category')
                logger.debug(f"åˆ— '{col}' ã‚’ã‚«ãƒ†ã‚´ãƒªå‹ã«å¤‰æ›")
        except Exception as e:
            logger.warning(f"åˆ— '{col}' ã®å‹å¤‰æ›ã‚¨ãƒ©ãƒ¼: {e}")
    
    try:
        # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹è¨ˆæ¸¬é–‹å§‹
        mem_before = df_raw.memory_usage(deep=True).sum() / (1024 * 1024)
        
        # âœ… ä¿®æ­£ï¼šç‰¹å®šã®åˆ—ã®ã¿ã§é‡è¤‡é™¤å»
        # é‡è¦ãªè­˜åˆ¥åˆ—ã®ã¿ã§é‡è¤‡ãƒã‚§ãƒƒã‚¯
        key_columns = []
        if 'æ—¥ä»˜' in df_raw.columns:
            key_columns.append('æ—¥ä»˜')
        if 'ç—…æ£Ÿã‚³ãƒ¼ãƒ‰' in df_raw.columns:
            key_columns.append('ç—…æ£Ÿã‚³ãƒ¼ãƒ‰')
        if 'è¨ºç™‚ç§‘å' in df_raw.columns:
            key_columns.append('è¨ºç™‚ç§‘å')
        
        if key_columns:
            # è­˜åˆ¥å¯èƒ½ãªåˆ—ãŒã‚ã‚‹å ´åˆã¯ãã‚Œã‚‰ã®åˆ—ã®ã¿ã§é‡è¤‡é™¤å»
            df_processed = df_raw.drop_duplicates(subset=key_columns)
            logger.info(f"é‡è¤‡é™¤å»: ã‚­ãƒ¼åˆ— {key_columns} ã§å®Ÿè¡Œ")
        else:
            # ã‚­ãƒ¼åˆ—ãŒãªã„å ´åˆã¯å…¨åˆ—ã§é‡è¤‡é™¤å»ï¼ˆå¾“æ¥é€šã‚Šï¼‰
            df_processed = df_raw.drop_duplicates()
            logger.warning("é‡è¤‡é™¤å»: ã‚­ãƒ¼åˆ—ãŒè¦‹ã¤ã‹ã‚‰ãªã„ãŸã‚å…¨åˆ—ã§å®Ÿè¡Œ")
        
        # é‡è¤‡é™¤å»å¾Œã®ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡
        mem_after = df_processed.memory_usage(deep=True).sum() / (1024 * 1024)
        
        # çµæœã®é›†è¨ˆ
        rows_dropped = initial_rows - len(df_processed)
        
        # âœ… è¿½åŠ ï¼šå¤§é‡å‰Šé™¤ã®è­¦å‘Š
        if rows_dropped > 0:
            drop_rate = (rows_dropped / initial_rows) * 100
            if drop_rate > 30:  # 30%ä»¥ä¸Šå‰Šé™¤ã•ã‚ŒãŸå ´åˆ
                logger.error(f"ğŸš¨ è­¦å‘Š: é‡è¤‡é™¤å»ã§{rows_dropped:,}ä»¶ï¼ˆ{drop_rate:.1f}%ï¼‰å‰Šé™¤ã•ã‚Œã¾ã—ãŸã€‚è¨­å®šã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
            elif drop_rate > 10:  # 10%ä»¥ä¸Šå‰Šé™¤ã•ã‚ŒãŸå ´åˆ
                logger.warning(f"âš ï¸ æ³¨æ„: é‡è¤‡é™¤å»ã§{rows_dropped:,}ä»¶ï¼ˆ{drop_rate:.1f}%ï¼‰å‰Šé™¤ã•ã‚Œã¾ã—ãŸã€‚")
        
        # å…ƒã®ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã¨ãã®é–¢é€£ãƒªã‚½ãƒ¼ã‚¹ã‚’è§£æ”¾
        del df_raw
        gc.collect()
        
        # çµ‚äº†æ™‚é–“ã¨ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æƒ…å ±
        end_time = time.time()
        processing_time = end_time - start_time
        
        # çµæœã‚’ãƒ­ã‚°ã«å‡ºåŠ›
        logger.info(f"é‡è¤‡ãƒã‚§ãƒƒã‚¯çµæœ: åˆæœŸè¡Œæ•°={initial_rows:,}, å‰Šé™¤è¡Œæ•°={rows_dropped:,}, "
                   f"æœ€çµ‚è¡Œæ•°={len(df_processed):,}, å‡¦ç†æ™‚é–“={processing_time:.2f}ç§’, "
                   f"ãƒ¡ãƒ¢ãƒªå‰Šæ¸›={mem_before-mem_after:.2f}MB")
        
        # ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã«ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æƒ…å ±ã‚’è¨˜éŒ²
        if 'performance_metrics' not in st.session_state:
            st.session_state.performance_metrics = {}
        st.session_state.performance_metrics['duplicate_check_time'] = processing_time
        st.session_state.performance_metrics['duplicate_rows_removed'] = rows_dropped
        
        return df_processed
    
    except Exception as e:
        import traceback
        error_detail = traceback.format_exc()
        logger.error(f"é‡è¤‡ãƒã‚§ãƒƒã‚¯å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}\n{error_detail}")
        
        # ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ãŸå ´åˆã¯å…ƒã®ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’è¿”ã™
        return df_raw

@st.cache_data(ttl=3600, show_spinner=False)
def integrated_preprocess_data(df: pd.DataFrame, target_data_df: pd.DataFrame = None):
    start_time = time.time()
    validation_results = {
        "is_valid": True,
        "warnings": [],
        "errors": [],
        "info": []
    }

    # ãƒ‡ãƒ¼ã‚¿ä»¶æ•°ã®ãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚°
    initial_record_count = len(df) if df is not None and not df.empty else 0
    validation_results["info"].append(f"åˆæœŸãƒ‡ãƒ¼ã‚¿ä»¶æ•°: {initial_record_count:,}ä»¶")

    try:
        if df is None or df.empty:
            validation_results["is_valid"] = False
            validation_results["errors"].append("å…¥åŠ›ãƒ‡ãƒ¼ã‚¿ãŒç©ºã§ã™ã€‚")
            return None, validation_results

        # å¿…è¦ãªåˆ—ã®ç¢ºèª
        expected_cols = ["ç—…æ£Ÿã‚³ãƒ¼ãƒ‰", "è¨ºç™‚ç§‘å", "æ—¥ä»˜", "åœ¨é™¢æ‚£è€…æ•°",
                         "å…¥é™¢æ‚£è€…æ•°", "ç·Šæ€¥å…¥é™¢æ‚£è€…æ•°", "é€€é™¢æ‚£è€…æ•°", "æ­»äº¡æ‚£è€…æ•°"]

        # åˆ©ç”¨å¯èƒ½ãªåˆ—ã®ã¿ã‚’å¯¾è±¡ã¨ã™ã‚‹
        available_cols = [col for col in df.columns if col in expected_cols]
        df_processed = df[available_cols].copy()

        # --- 1. åŸºæœ¬çš„ãªãƒ‡ãƒ¼ã‚¿ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚° ---
        # ã€Œç—…æ£Ÿã‚³ãƒ¼ãƒ‰ã€ãŒæ¬ æã—ã¦ã„ã‚‹è¡Œã®ã¿ã‚’é™¤å¤–
        before_ward_filter = len(df_processed)
        df_processed.dropna(subset=['ç—…æ£Ÿã‚³ãƒ¼ãƒ‰'], inplace=True)
        after_ward_filter = len(df_processed)
        rows_dropped_due_to_ward_nan = before_ward_filter - after_ward_filter
        if rows_dropped_due_to_ward_nan > 0:
            validation_results["warnings"].append(
                f"ã€Œç—…æ£Ÿã‚³ãƒ¼ãƒ‰ã€ãŒæ¬ æã—ã¦ã„ã‚‹è¡ŒãŒ {rows_dropped_due_to_ward_nan} ä»¶ã‚ã‚Šã¾ã—ãŸã€‚ã“ã‚Œã‚‰ã®è¡Œã¯é™¤å¤–ã•ã‚Œã¾ã—ãŸã€‚"
            )
        
        # æ—¥ä»˜åˆ—ã®å‡¦ç†ã¨æ¬ æè¡Œã®é™¤å¤–
        if 'æ—¥ä»˜' not in df_processed.columns:
            validation_results["is_valid"] = False
            validation_results["errors"].append("å¿…é ˆåˆ—ã€Œæ—¥ä»˜ã€ãŒå­˜åœ¨ã—ã¾ã›ã‚“ã€‚")
            return None, validation_results
            
        df_processed['æ—¥ä»˜'] = pd.to_datetime(df_processed['æ—¥ä»˜'], errors='coerce')
        before_date_filter = len(df_processed)
        df_processed.dropna(subset=['æ—¥ä»˜'], inplace=True)
        after_date_filter = len(df_processed)
        rows_dropped_due_to_date_nan = before_date_filter - after_date_filter
        if rows_dropped_due_to_date_nan > 0:
             validation_results["warnings"].append(
                f"ç„¡åŠ¹ãªæ—¥ä»˜ã¾ãŸã¯æ—¥ä»˜ãŒæ¬ æã—ã¦ã„ã‚‹è¡ŒãŒ {rows_dropped_due_to_date_nan} ä»¶ã‚ã‚Šã¾ã—ãŸã€‚ã“ã‚Œã‚‰ã®è¡Œã¯é™¤å¤–ã•ã‚Œã¾ã—ãŸã€‚"
            )

        if df_processed.empty:
            validation_results["is_valid"] = False
            validation_results["errors"].append("å¿…é ˆã®ã€Œç—…æ£Ÿã‚³ãƒ¼ãƒ‰ã€ã¾ãŸã¯ã€Œæ—¥ä»˜ã€ã®å‡¦ç†å¾Œã«ãƒ‡ãƒ¼ã‚¿ãŒç©ºã«ãªã‚Šã¾ã—ãŸã€‚")
            return None, validation_results
            
        # ç—…æ£Ÿã‚³ãƒ¼ãƒ‰ã‚’æ–‡å­—åˆ—å‹ã«å¤‰æ›
        df_processed["ç—…æ£Ÿã‚³ãƒ¼ãƒ‰"] = df_processed["ç—…æ£Ÿã‚³ãƒ¼ãƒ‰"].astype(str)
    
        # è¨ºç™‚ç§‘åã®æ¬ æå€¤å‡¦ç†ï¼ˆã€Œãã®ä»–ã€ã¸ã®é›†ç´„ã¯å¾Œã§å®Ÿæ–½ï¼‰
        if 'è¨ºç™‚ç§‘å' in df_processed.columns:
            if pd.api.types.is_categorical_dtype(df_processed['è¨ºç™‚ç§‘å']):
                df_processed['è¨ºç™‚ç§‘å'] = df_processed['è¨ºç™‚ç§‘å'].astype(str).fillna("ç©ºç™½è¨ºç™‚ç§‘")
            else:
                df_processed['è¨ºç™‚ç§‘å'] = df_processed['è¨ºç™‚ç§‘å'].fillna("ç©ºç™½è¨ºç™‚ç§‘").astype(str)
            validation_results["info"].append("è¨ºç™‚ç§‘åã®æ¬ æå€¤ã‚’ã€Œç©ºç™½è¨ºç™‚ç§‘ã€ã§è£œå®Œã—ã¾ã—ãŸã€‚")
        else:
            validation_results["warnings"].append("ã€Œè¨ºç™‚ç§‘åã€åˆ—ãŒå­˜åœ¨ã—ãªã„ãŸã‚ã€è¨ºç™‚ç§‘å‡¦ç†ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã—ãŸã€‚")
        
        # --- 2. çœŸã®é‡è¤‡é™¤å»ï¼ˆå…¨åˆ—ãŒå®Œå…¨ä¸€è‡´ã™ã‚‹è¡Œã®ã¿ï¼‰ ---
        before_true_dedup = len(df_processed)
        df_processed = df_processed.drop_duplicates()  # å…¨åˆ—ã§é‡è¤‡é™¤å»
        after_true_dedup = len(df_processed)
        true_duplicates_removed = before_true_dedup - after_true_dedup
        
        if true_duplicates_removed > 0:
            validation_results["info"].append(
                f"çœŸã®é‡è¤‡ãƒ‡ãƒ¼ã‚¿ï¼ˆå…¨åˆ—ä¸€è‡´ï¼‰ {true_duplicates_removed:,} è¡Œã‚’å‰Šé™¤ã—ã¾ã—ãŸ"
            )
        else:
            validation_results["info"].append("çœŸã®é‡è¤‡ãƒ‡ãƒ¼ã‚¿ã¯è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
    
        # --- 3. æ•°å€¤åˆ—ã®å‡¦ç†ã¨ãƒ‡ãƒ¼ã‚¿å‹å¤‰æ› ---
        numeric_cols_to_process = [
            "åœ¨é™¢æ‚£è€…æ•°", "å…¥é™¢æ‚£è€…æ•°", "ç·Šæ€¥å…¥é™¢æ‚£è€…æ•°", "é€€é™¢æ‚£è€…æ•°", "æ­»äº¡æ‚£è€…æ•°"
        ]
        
        for col in numeric_cols_to_process:
            if col in df_processed.columns:
                # éæ•°å€¤ãƒ‡ãƒ¼ã‚¿ã‚’NaNã«å¤‰æ›
                df_processed[col] = pd.to_numeric(df_processed[col], errors='coerce')
                    
                # æ•°å€¤åˆ—ã®NaNã‚’0ã§åŸ‹ã‚ã‚‹
                na_vals_before_fill = df_processed[col].isna().sum()
                if na_vals_before_fill > 0:
                    df_processed[col] = df_processed[col].fillna(0)
                    validation_results["info"].append(f"æ•°å€¤åˆ—'{col}'ã®æ¬ æå€¤ {na_vals_before_fill} ä»¶ã‚’0ã§è£œå®Œã—ã¾ã—ãŸã€‚")
            else:
                # æ•°å€¤åˆ—ãŒå­˜åœ¨ã—ãªã„å ´åˆã¯0ã§åŸ‹ã‚ãŸåˆ—ã‚’ä½œæˆ
                df_processed[col] = 0
                validation_results["warnings"].append(f"æ•°å€¤åˆ—'{col}'ãŒå­˜åœ¨ã—ãªã‹ã£ãŸãŸã‚ã€0ã§è£œå®Œã•ã‚ŒãŸåˆ—ã‚’ä½œæˆã—ã¾ã—ãŸã€‚")

        # --- 4. åŒã˜æ—¥ä»˜ãƒ»ç—…æ£Ÿãƒ»è¨ºç™‚ç§‘ã®ãƒ‡ãƒ¼ã‚¿ã‚’é›†è¨ˆ ---
        grouping_cols = ['æ—¥ä»˜', 'ç—…æ£Ÿã‚³ãƒ¼ãƒ‰', 'è¨ºç™‚ç§‘å']
        available_grouping_cols = [col for col in grouping_cols if col in df_processed.columns]
        
        if len(available_grouping_cols) >= 2:  # æœ€ä½2åˆ—å¿…è¦
            before_aggregation = len(df_processed)
            
            # æ•°å€¤åˆ—ã‚’åˆè¨ˆã§é›†è¨ˆ
            agg_dict = {}
            for col in numeric_cols_to_process:
                if col in df_processed.columns:
                    agg_dict[col] = 'sum'  # ãƒã‚¤ãƒŠã‚¹å€¤ã‚‚å«ã‚ã¦åˆè¨ˆ
            
            if agg_dict:
                df_processed = df_processed.groupby(available_grouping_cols, as_index=False).agg(agg_dict)
                after_aggregation = len(df_processed)
                aggregated_rows = before_aggregation - after_aggregation
                
                if aggregated_rows > 0:
                    validation_results["info"].append(
                        f"åŒã˜æ—¥ä»˜ãƒ»ç—…æ£Ÿãƒ»è¨ºç™‚ç§‘ã®ãƒ‡ãƒ¼ã‚¿ã‚’é›†è¨ˆ: {before_aggregation:,}è¡Œ â†’ {after_aggregation:,}è¡Œ"
                    )
                    validation_results["info"].append(
                        f"é›†è¨ˆã«ã‚ˆã‚Š {aggregated_rows:,} è¡ŒãŒçµ±åˆã•ã‚Œã¾ã—ãŸ"
                    )
                    validation_results["info"].append(f"é›†è¨ˆã‚­ãƒ¼: {available_grouping_cols}")
                else:
                    validation_results["info"].append("é›†è¨ˆå¯¾è±¡ã®ãƒ‡ãƒ¼ã‚¿ã¯è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
            else:
                validation_results["warnings"].append("é›†è¨ˆå¯èƒ½ãªæ•°å€¤åˆ—ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
        else:
            validation_results["warnings"].append("é›†è¨ˆã«å¿…è¦ãªã‚­ãƒ¼åˆ—ï¼ˆæ—¥ä»˜ãƒ»ç—…æ£Ÿãƒ»è¨ºç™‚ç§‘ï¼‰ãŒä¸è¶³ã—ã¦ã„ã¾ã™ã€‚")

        # --- 5. æ´¾ç”ŸæŒ‡æ¨™ã®è¨ˆç®— ---
        # åˆ—åã®çµ±ä¸€å‡¦ç†
        if "åœ¨é™¢æ‚£è€…æ•°" in df_processed.columns:
            df_processed["å…¥é™¢æ‚£è€…æ•°ï¼ˆåœ¨é™¢ï¼‰"] = df_processed["åœ¨é™¢æ‚£è€…æ•°"].copy()
            validation_results["info"].append("ã€Œåœ¨é™¢æ‚£è€…æ•°ã€åˆ—ã‚’ã€Œå…¥é™¢æ‚£è€…æ•°ï¼ˆåœ¨é™¢ï¼‰ã€åˆ—ã«ã‚³ãƒ”ãƒ¼ã—ã¾ã—ãŸã€‚")
        elif "å…¥é™¢æ‚£è€…æ•°ï¼ˆåœ¨é™¢ï¼‰" not in df_processed.columns:
            validation_results["errors"].append("ã€Œåœ¨é™¢æ‚£è€…æ•°ã€ã¾ãŸã¯ã€Œå…¥é™¢æ‚£è€…æ•°ï¼ˆåœ¨é™¢ï¼‰ã€åˆ—ã®ã„ãšã‚Œã‚‚å­˜åœ¨ã—ã¾ã›ã‚“ã€‚")
            df_processed["å…¥é™¢æ‚£è€…æ•°ï¼ˆåœ¨é™¢ï¼‰"] = 0
            df_processed["åœ¨é™¢æ‚£è€…æ•°"] = 0

        # ç·å…¥é™¢æ‚£è€…æ•°
        if "å…¥é™¢æ‚£è€…æ•°" in df_processed.columns and "ç·Šæ€¥å…¥é™¢æ‚£è€…æ•°" in df_processed.columns:
            df_processed["ç·å…¥é™¢æ‚£è€…æ•°"] = df_processed["å…¥é™¢æ‚£è€…æ•°"] + df_processed["ç·Šæ€¥å…¥é™¢æ‚£è€…æ•°"]
        else:
            validation_results["warnings"].append("ã€Œå…¥é™¢æ‚£è€…æ•°ã€ã¾ãŸã¯ã€Œç·Šæ€¥å…¥é™¢æ‚£è€…æ•°ã€åˆ—ãŒãªã„ãŸã‚ã€ã€Œç·å…¥é™¢æ‚£è€…æ•°ã€ã¯è¨ˆç®—ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚")
            df_processed["ç·å…¥é™¢æ‚£è€…æ•°"] = 0

        # ç·é€€é™¢æ‚£è€…æ•°
        if "é€€é™¢æ‚£è€…æ•°" in df_processed.columns and "æ­»äº¡æ‚£è€…æ•°" in df_processed.columns:
            df_processed["ç·é€€é™¢æ‚£è€…æ•°"] = df_processed["é€€é™¢æ‚£è€…æ•°"] + df_processed["æ­»äº¡æ‚£è€…æ•°"]
        else:
            validation_results["warnings"].append("ã€Œé€€é™¢æ‚£è€…æ•°ã€ã¾ãŸã¯ã€Œæ­»äº¡æ‚£è€…æ•°ã€åˆ—ãŒãªã„ãŸã‚ã€ã€Œç·é€€é™¢æ‚£è€…æ•°ã€ã¯è¨ˆç®—ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚")
            df_processed["ç·é€€é™¢æ‚£è€…æ•°"] = 0

        # æ–°å…¥é™¢æ‚£è€…æ•°
        if "ç·å…¥é™¢æ‚£è€…æ•°" in df_processed.columns:
            df_processed["æ–°å…¥é™¢æ‚£è€…æ•°"] = df_processed["ç·å…¥é™¢æ‚£è€…æ•°"]
        else:
            df_processed["æ–°å…¥é™¢æ‚£è€…æ•°"] = 0

        # --- 6. æœ€å¾Œã«è¨ºç™‚ç§‘åã®é›†ç´„ï¼ˆç›®æ¨™ãƒ‡ãƒ¼ã‚¿ã¨ã®ãƒãƒƒãƒ”ãƒ³ã‚°ï¼‰ ---
        major_departments_list = []
        
        # target_data_df ã‚’ä½¿ã£ã¦ major_departments_list ã‚’ç”Ÿæˆ
        if target_data_df is not None and not target_data_df.empty and 'éƒ¨é–€ã‚³ãƒ¼ãƒ‰' in target_data_df.columns:
            potential_major_depts = target_data_df['éƒ¨é–€ã‚³ãƒ¼ãƒ‰'].astype(str).unique()
            if 'éƒ¨é–€å' in target_data_df.columns:
                potential_major_depts_from_name = target_data_df['éƒ¨é–€å'].astype(str).unique()
                potential_major_depts = np.union1d(potential_major_depts, potential_major_depts_from_name)

            if 'è¨ºç™‚ç§‘å' in df_processed.columns:
                actual_depts_in_df = df_processed['è¨ºç™‚ç§‘å'].astype(str).unique()
                major_departments_list = [dept for dept in actual_depts_in_df if dept in potential_major_depts]
            
            if not major_departments_list and len(potential_major_depts) > 0:
                major_departments_list = list(potential_major_depts)
                validation_results["warnings"].append(
                    "ç›®æ¨™è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã«è¨˜è¼‰ã®è¨ºç™‚ç§‘ãŒã€å®Ÿç¸¾ãƒ‡ãƒ¼ã‚¿ã®è¨ºç™‚ç§‘åã¨ç›´æ¥ä¸€è‡´ã—ã¾ã›ã‚“ã§ã—ãŸã€‚"
                    "ç›®æ¨™è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã®ã€Œéƒ¨é–€ã‚³ãƒ¼ãƒ‰ã€ã¾ãŸã¯ã€Œéƒ¨é–€åã€ã‚’ä¸»è¦è¨ºç™‚ç§‘ã¨ã—ã¦æ‰±ã„ã¾ã™ã€‚"
                )
            if not major_departments_list:
                validation_results["warnings"].append("ç›®æ¨™è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ä¸»è¦è¨ºç™‚ç§‘ãƒªã‚¹ãƒˆã‚’ç‰¹å®šã§ãã¾ã›ã‚“ã§ã—ãŸã€‚")
        else:
            validation_results["warnings"].append("ç›®æ¨™è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ãŒæä¾›ã•ã‚Œãªã‹ã£ãŸã‹ã€'éƒ¨é–€ã‚³ãƒ¼ãƒ‰'åˆ—ãŒã‚ã‚Šã¾ã›ã‚“ã€‚å…¨ã¦ã®è¨ºç™‚ç§‘ã‚’ã€Œãã®ä»–ã€ã¨ã—ã¦æ‰±ã„ã¾ã™ã€‚")

        # è¨ºç™‚ç§‘åã®æœ€çµ‚é›†ç´„
        if 'è¨ºç™‚ç§‘å' in df_processed.columns:
            before_dept_aggregation = df_processed['è¨ºç™‚ç§‘å'].nunique()
            df_processed['è¨ºç™‚ç§‘å'] = df_processed['è¨ºç™‚ç§‘å'].apply(
                lambda x: x if x in major_departments_list else 'ãã®ä»–'
            )
            after_dept_aggregation = df_processed['è¨ºç™‚ç§‘å'].nunique()
            
            validation_results["info"].append(
                f"è¨ºç™‚ç§‘åã‚’é›†ç´„: {before_dept_aggregation}ç¨®é¡ â†’ {after_dept_aggregation}ç¨®é¡"
            )
            validation_results["info"].append(
                f"ä¸»è¦è¨ºç™‚ç§‘ï¼ˆ{len(major_departments_list)}ä»¶ï¼‰ã¨ã€Œãã®ä»–ã€ã«é›†ç´„ã—ã¾ã—ãŸã€‚"
            )

        # --- 7. æœ€çµ‚çš„ãªé›†è¨ˆï¼ˆè¨ºç™‚ç§‘é›†ç´„å¾Œï¼‰ ---
        # è¨ºç™‚ç§‘é›†ç´„å¾Œã«å†åº¦åŒã˜ã‚­ãƒ¼ã§é›†è¨ˆãŒå¿…è¦ãªå ´åˆ
        if len(available_grouping_cols) >= 2:
            before_final_agg = len(df_processed)
            
            agg_dict_final = {}
            for col in numeric_cols_to_process + ["å…¥é™¢æ‚£è€…æ•°ï¼ˆåœ¨é™¢ï¼‰", "ç·å…¥é™¢æ‚£è€…æ•°", "ç·é€€é™¢æ‚£è€…æ•°", "æ–°å…¥é™¢æ‚£è€…æ•°"]:
                if col in df_processed.columns:
                    agg_dict_final[col] = 'sum'
            
            if agg_dict_final:
                df_processed = df_processed.groupby(available_grouping_cols, as_index=False).agg(agg_dict_final)
                after_final_agg = len(df_processed)
                
                if before_final_agg != after_final_agg:
                    final_aggregated = before_final_agg - after_final_agg
                    validation_results["info"].append(
                        f"è¨ºç™‚ç§‘é›†ç´„å¾Œã®æœ€çµ‚é›†è¨ˆ: {before_final_agg:,}è¡Œ â†’ {after_final_agg:,}è¡Œ"
                    )
                    validation_results["info"].append(
                        f"æœ€çµ‚é›†è¨ˆã«ã‚ˆã‚Š {final_aggregated:,} è¡ŒãŒçµ±åˆã•ã‚Œã¾ã—ãŸ"
                    )

        # --- 8. å¹³æ—¥/ä¼‘æ—¥ãƒ•ãƒ©ã‚°ã®è¿½åŠ  ---
        if 'æ—¥ä»˜' in df_processed.columns:
            df_processed = add_weekday_flag(df_processed)
        else:
            validation_results["errors"].append("ã€Œæ—¥ä»˜ã€åˆ—ãŒãªã„ãŸã‚ã€å¹³æ—¥/ä¼‘æ—¥ãƒ•ãƒ©ã‚°ã‚’è¿½åŠ ã§ãã¾ã›ã‚“ã€‚")
            
        gc.collect()
        end_time = time.time()
        
        # æœ€çµ‚çš„ãªãƒ‡ãƒ¼ã‚¿ä»¶æ•°ã®ç¢ºèª
        final_record_count = len(df_processed)
        total_loss = initial_record_count - final_record_count
        loss_rate = (total_loss / initial_record_count) * 100 if initial_record_count > 0 else 0
        
        validation_results["info"].append(f"ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†æ™‚é–“: {end_time - start_time:.2f}ç§’")
        validation_results["info"].append(f"å‡¦ç†å¾Œã®ãƒ¬ã‚³ãƒ¼ãƒ‰æ•°: {final_record_count:,}")
        validation_results["info"].append(f"ç·ãƒ‡ãƒ¼ã‚¿å¤‰åŒ–: {total_loss:,}ä»¶ï¼ˆ{loss_rate:.1f}%ï¼‰")
        
        # ãƒ‡ãƒ¼ã‚¿å¤‰åŒ–ã®èª¬æ˜
        if loss_rate > 0:
            validation_results["info"].append("ãƒ‡ãƒ¼ã‚¿æ¸›å°‘ã¯é‡è¤‡é™¤å»ãƒ»é›†è¨ˆå‡¦ç†ã«ã‚ˆã‚‹ã‚‚ã®ã§ã™ã€‚")
        
        if not df_processed.empty:
            validation_results["info"].append(f"ãƒ‡ãƒ¼ã‚¿æœŸé–“: {df_processed['æ—¥ä»˜'].min().strftime('%Y/%m/%d')} - {df_processed['æ—¥ä»˜'].max().strftime('%Y/%m/%d')}")
        
        if df_processed.empty:
            validation_results["is_valid"] = False
            validation_results["errors"].append("å‰å‡¦ç†ã®çµæœã€æœ‰åŠ¹ãªãƒ‡ãƒ¼ã‚¿ãŒæ®‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
            return None, validation_results

        return df_processed, validation_results

    except Exception as e:
        import traceback
        error_detail = traceback.format_exc()
        validation_results["is_valid"] = False
        validation_results["errors"].append(f"ãƒ‡ãƒ¼ã‚¿ã®å‰å‡¦ç†ä¸­ã«äºˆæœŸã›ã¬ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}")
        validation_results["errors"].append(f"è©³ç´°: {error_detail}")
        print(f"å‰å‡¦ç†ã‚¨ãƒ©ãƒ¼: {error_detail}")
        return None, validation_results

def add_weekday_flag(df):
    """
    å¹³æ—¥/ä¼‘æ—¥ã®åˆ¤å®šãƒ•ãƒ©ã‚°ã‚’è¿½åŠ ã™ã‚‹
    
    Parameters:
    -----------
    df : pd.DataFrame
        ãƒ•ãƒ©ã‚°ã‚’è¿½åŠ ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ 
    
    Returns:
    --------
    pd.DataFrame
        ãƒ•ãƒ©ã‚°ãŒè¿½åŠ ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ 
    """
    def is_holiday(date):
        return (
            date.weekday() >= 5 or  # åœŸæ—¥
            jpholiday.is_holiday(date) or  # ç¥æ—¥
            (date.month == 12 and date.day >= 29) or  # å¹´æœ«
            (date.month == 1 and date.day <= 3)  # å¹´å§‹
        )
    
    # å¹³æ—¥/ä¼‘æ—¥ãƒ•ãƒ©ã‚°ã‚’è¿½åŠ 
    df["å¹³æ—¥åˆ¤å®š"] = df["æ—¥ä»˜"].apply(lambda x: "ä¼‘æ—¥" if is_holiday(x) else "å¹³æ—¥")
    
    return df  # ã“ã®è¡ŒãŒã‚¤ãƒ³ãƒ‡ãƒ³ãƒˆã•ã‚Œã¦ã„ã‚‹ã“ã¨ã‚’ç¢ºèª
    
def calculate_file_hash(file_content_bytes):
    """
    ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒãƒƒã‚·ãƒ¥å€¤ã‚’è¨ˆç®—ã—ã¦ä¸€æ„ã®è­˜åˆ¥å­ã‚’ä½œæˆ
    
    Parameters:
    -----------
    file_content_bytes: bytes
        ãƒ•ã‚¡ã‚¤ãƒ«ã®å†…å®¹ã‚’ãƒã‚¤ãƒˆåˆ—ã§è¡¨ã—ãŸã‚‚ã®
        
    Returns:
    --------
    str
        MD5ãƒãƒƒã‚·ãƒ¥å€¤ã®16é€²æ•°æ–‡å­—åˆ—
    """
    try:
        # ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºãŒå¤§ãã™ãã‚‹å ´åˆã€å…ˆé ­éƒ¨åˆ†ã®ã¿ã‚’ä½¿ç”¨ã—ã¦ãƒãƒƒã‚·ãƒ¥è¨ˆç®—
        max_bytes = 10 * 1024 * 1024  # 10MBä¸Šé™
        if len(file_content_bytes) > max_bytes:
            # å…ˆé ­5MBã¨æœ«å°¾5MBã‚’çµ„ã¿åˆã‚ã›ã¦ãƒãƒƒã‚·ãƒ¥è¨ˆç®—
            head = file_content_bytes[:5 * 1024 * 1024]  # å…ˆé ­5MB
            tail = file_content_bytes[-5 * 1024 * 1024:]  # æœ«å°¾5MB
            combined = head + tail
            return hashlib.md5(combined).hexdigest()
        else:
            # é€šå¸¸ã®ãƒãƒƒã‚·ãƒ¥è¨ˆç®—
            return hashlib.md5(file_content_bytes).hexdigest()
    except Exception as e:
        print(f"ãƒ•ã‚¡ã‚¤ãƒ«ãƒãƒƒã‚·ãƒ¥è¨ˆç®—ã‚¨ãƒ©ãƒ¼: {str(e)}")
        # ã‚¨ãƒ©ãƒ¼æ™‚ã«ã¯ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºã¨ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã®çµ„ã¿åˆã‚ã›ã‚’è¿”ã™
        file_size = len(file_content_bytes)
        timestamp = int(time.time())
        return f"size_{file_size}_time_{timestamp}"


@st.cache_data(ttl=3600, show_spinner=False)
def read_excel_cached(file_content_bytes, sheet_name=0, usecols=None, dtype=None):
    """
    ãƒ•ã‚¡ã‚¤ãƒ«å†…å®¹ã«åŸºã¥ã„ãŸã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ä½¿ç”¨ã—ã¦Excelã‚’èª­ã¿è¾¼ã‚€
    ä¾‹å¤–å‡¦ç†ã‚’å¼·åŒ–
    
    Parameters:
    -----------
    file_content_bytes: bytes
        Excelãƒ•ã‚¡ã‚¤ãƒ«ã®å†…å®¹ã‚’ãƒã‚¤ãƒˆåˆ—ã§è¡¨ã—ãŸã‚‚ã®
    sheet_name: int or str, default 0
        èª­ã¿è¾¼ã‚€ã‚·ãƒ¼ãƒˆåã¾ãŸã¯ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹
    usecols: list or None, default None
        èª­ã¿è¾¼ã‚€åˆ—ã®ãƒªã‚¹ãƒˆ
    dtype: dict or None, default None
        åˆ—ã®ãƒ‡ãƒ¼ã‚¿å‹ã‚’æŒ‡å®šã™ã‚‹è¾æ›¸
        
    Returns:
    --------
    pd.DataFrame
        èª­ã¿è¾¼ã¾ã‚ŒãŸExcelãƒ‡ãƒ¼ã‚¿
    """
    temp_path = None
    try:
        # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆ
        with tempfile.NamedTemporaryFile(delete=False, suffix='.xlsx') as temp_file:
            temp_file.write(file_content_bytes)
            temp_path = temp_file.name

        # èª­ã¿è¾¼ã¿æ™‚ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ãƒ­ã‚°å‡ºåŠ›ï¼ˆãƒ‡ãƒãƒƒã‚°ç”¨ï¼‰
        print(f"Excelèª­è¾¼: usecols={usecols}, dtype={dtype}")
        
        # Excelãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿
        df = pd.read_excel(
            temp_path, 
            sheet_name=sheet_name, 
            engine='openpyxl', 
            usecols=usecols, 
            dtype=dtype
        )
        
        # åŸºæœ¬çš„ãªæ¤œè¨¼
        if df.empty:
            print(f"è­¦å‘Š: èª­ã¿è¾¼ã¾ã‚ŒãŸExcelãƒ•ã‚¡ã‚¤ãƒ«ãŒç©ºã§ã™: sheet_name={sheet_name}")
            return None
            
        return df
    except Exception as e:
        # è©³ç´°ãªã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’ãƒ­ã‚°ã«å‡ºåŠ›
        print(f"Excelèª­è¾¼ã‚¨ãƒ©ãƒ¼: {str(e)}")
        import traceback
        print(traceback.format_exc())
        return None
    finally:
        # ç¢ºå®Ÿã«ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤
        if temp_path and os.path.exists(temp_path):
            try:
                os.unlink(temp_path)
            except Exception as e:
                print(f"ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«å‰Šé™¤ã‚¨ãƒ©ãƒ¼: {str(e)}")


def load_files(base_file, new_files, usecols_excel=None, dtype_excel=None):
    """
    è¤‡æ•°ã®Excelãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¸¦åˆ—å‡¦ç†ã§èª­ã¿è¾¼ã‚€ï¼ˆä¿®æ­£ç‰ˆï¼‰
    """
    start_time = time.time()
    
    # ãƒ•ã‚¡ã‚¤ãƒ«ãƒªã‚¹ãƒˆã®æº–å‚™
    df_list = []
    files_to_process = []

    if base_file:
        files_to_process.append(base_file)
        print(f"åŸºæœ¬ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‡¦ç†ãƒªã‚¹ãƒˆã«è¿½åŠ : {base_file.name}")
    
    if new_files:
        files_to_process.extend(new_files)
        print(f"è¿½åŠ ãƒ•ã‚¡ã‚¤ãƒ«{len(new_files)}ä»¶ã‚’å‡¦ç†ãƒªã‚¹ãƒˆã«è¿½åŠ ")

    if not files_to_process:
        print("å‡¦ç†å¯¾è±¡ãƒ•ã‚¡ã‚¤ãƒ«ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
        return pd.DataFrame()

    # ãƒ•ã‚¡ã‚¤ãƒ«å†…å®¹ã‚’ãƒ¡ãƒ¢ãƒªã«èª­ã¿è¾¼ã‚€
    file_contents = []
    for file_obj in files_to_process:
        try:
            file_obj.seek(0)
            file_content = file_obj.read()
            file_contents.append((file_obj.name, file_content))
            file_obj.seek(0)
            file_size = len(file_content) / (1024 * 1024)
            print(f"ãƒ•ã‚¡ã‚¤ãƒ«èª­è¾¼: {file_obj.name} ({file_size:.2f} MB)")
        except Exception as e:
            print(f"ãƒ•ã‚¡ã‚¤ãƒ«èª­è¾¼ã‚¨ãƒ©ãƒ¼ ({file_obj.name}): {str(e)}")

    # ä¸¦åˆ—å‡¦ç†ã®è¨­å®š
    max_workers = min(4, len(file_contents)) if file_contents else 1
    print(f"ä¸¦åˆ—å‡¦ç†ãƒ¯ãƒ¼ã‚«ãƒ¼æ•°: {max_workers}")
    
    # ä¸¦åˆ—å‡¦ç†ã§Excelãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚€
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        futures = {
            executor.submit(read_excel_cached, content, 0, usecols_excel, dtype_excel): name
            for name, content in file_contents
        }

        successful_files = 0
        for future in concurrent.futures.as_completed(futures):
            file_name = futures[future]
            try:
                df = future.result()
                if df is not None and not df.empty:
                    df_list.append(df)
                    rows, cols = df.shape
                    print(f"ãƒ•ã‚¡ã‚¤ãƒ« '{file_name}' ã®èª­è¾¼æˆåŠŸ: {rows}è¡Œ Ã— {cols}åˆ—")
                    successful_files += 1
                else:
                    print(f"ãƒ•ã‚¡ã‚¤ãƒ« '{file_name}' ã®èª­è¾¼çµæœãŒç©ºã§ã™")
            except Exception as e:
                print(f"ãƒ•ã‚¡ã‚¤ãƒ« '{file_name}' ã®å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼: {str(e)}")
                import traceback
                print(traceback.format_exc())

    if not df_list:
        print("èª­ã¿è¾¼ã¿å¯èƒ½ãªExcelãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
        return pd.DataFrame()

    # ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã®çµåˆ
    try:
        df_raw = pd.concat(df_list, ignore_index=True)
        
        # âœ… ä¿®æ­£ï¼šçµåˆå¾Œã®é‡è¤‡é™¤å»ã‚’å®‰å…¨ã«å®Ÿè¡Œ
        before_dedup = len(df_raw)
        df_raw = efficient_duplicate_check(df_raw)  # ä¿®æ­£æ¸ˆã¿ã®é–¢æ•°ã‚’ä½¿ç”¨
        after_dedup = len(df_raw)
        
        dedup_removed = before_dedup - after_dedup
        print(f"é‡è¤‡é™¤å»: {dedup_removed:,}ä»¶å‰Šé™¤")
        
        # çµæœã®å‡ºåŠ›
        end_time = time.time()
        rows, cols = df_raw.shape
        print(f"ãƒ‡ãƒ¼ã‚¿èª­è¾¼å®Œäº†: {successful_files}/{len(file_contents)}ãƒ•ã‚¡ã‚¤ãƒ«æˆåŠŸ, {rows}è¡Œ Ã— {cols}åˆ—, å‡¦ç†æ™‚é–“: {end_time - start_time:.2f}ç§’")
        
        return df_raw
    except Exception as e:
        print(f"ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ çµåˆã‚¨ãƒ©ãƒ¼: {str(e)}")
        return pd.DataFrame()